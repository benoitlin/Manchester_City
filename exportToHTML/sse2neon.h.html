<html>
<head>
<title>sse2neon.h</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #ab51ba;}
.s6 { color: #0f9795;}
.s7 { color: #4646f1;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
sse2neon.h</font>
</center></td></tr></table>
<pre><span class="s0">#ifndef </span><span class="s1">SSE2NEON_H</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_H</span>

<span class="s2">// This header file provides a simple API translation layer</span>
<span class="s2">// between SSE intrinsics to their corresponding Arm/Aarch64 NEON versions</span>
<span class="s2">//</span>
<span class="s2">// This header file does not yet translate all of the SSE intrinsics.</span>
<span class="s2">//</span>
<span class="s2">// Contributors to this work are:</span>
<span class="s2">//   John W. Ratcliff &lt;jratcliffscarab@gmail.com&gt;</span>
<span class="s2">//   Brandon Rowlett &lt;browlett@nvidia.com&gt;</span>
<span class="s2">//   Ken Fast &lt;kfast@gdeb.com&gt;</span>
<span class="s2">//   Eric van Beurden &lt;evanbeurden@nvidia.com&gt;</span>
<span class="s2">//   Alexander Potylitsin &lt;apotylitsin@nvidia.com&gt;</span>
<span class="s2">//   Hasindu Gamaarachchi &lt;hasindu2008@gmail.com&gt;</span>
<span class="s2">//   Jim Huang &lt;jserv@biilabs.io&gt;</span>
<span class="s2">//   Mark Cheng &lt;marktwtn@biilabs.io&gt;</span>
<span class="s2">//   Malcolm James MacLeod &lt;malcolm@gulden.com&gt;</span>
<span class="s2">//   Devin Hussey (easyaspi314) &lt;husseydevin@gmail.com&gt;</span>
<span class="s2">//   Sebastian Pop &lt;spop@amazon.com&gt;</span>
<span class="s2">//   Developer Ecosystem Engineering &lt;DeveloperEcosystemEngineering@apple.com&gt;</span>
<span class="s2">//   Danila Kutenin &lt;danilak@google.com&gt;</span>
<span class="s2">//   Fran√ßois Turban (JishinMaster) &lt;francois.turban@gmail.com&gt;</span>
<span class="s2">//   Pei-Hsuan Hung &lt;afcidk@gmail.com&gt;</span>
<span class="s2">//   Yang-Hao Yuan &lt;yanghau@biilabs.io&gt;</span>

<span class="s2">/* 
 * sse2neon is freely redistributable under the MIT License. 
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a copy 
 * of this software and associated documentation files (the &quot;Software&quot;), to deal 
 * in the Software without restriction, including without limitation the rights 
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell 
 * copies of the Software, and to permit persons to whom the Software is 
 * furnished to do so, subject to the following conditions: 
 * 
 * The above copyright notice and this permission notice shall be included in 
 * all copies or substantial portions of the Software. 
 * 
 * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR 
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE 
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER 
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, 
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE 
 * SOFTWARE. 
 */</span>

<span class="s2">/* Tunable configurations */</span>

<span class="s2">/* Enable precise implementation of _mm_min_ps and _mm_max_ps 
 * This would slow down the computation a bit, but gives consistent result with 
 * x86 SSE2. (e.g. would solve a hole or NaN pixel in the rendering result) 
 */</span>
<span class="s0">#ifndef </span><span class="s1">SSE2NEON_PRECISE_MINMAX</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_PRECISE_MINMAX (</span><span class="s3">0</span><span class="s1">)</span>
<span class="s0">#endif</span>

<span class="s0">#if </span><span class="s1">defined(__GNUC__) || defined(__clang__)</span>
<span class="s1">#pragma push_macro(</span><span class="s4">&quot;FORCE_INLINE&quot;</span><span class="s1">)</span>
<span class="s1">#pragma push_macro(</span><span class="s4">&quot;ALIGN_STRUCT&quot;</span><span class="s1">)</span>
<span class="s0">#define </span><span class="s1">FORCE_INLINE </span><span class="s0">static </span><span class="s5">inline </span><span class="s1">__attribute__((always_inline))</span>
<span class="s0">#define </span><span class="s1">ALIGN_STRUCT(x) __attribute__((aligned(x)))</span>
<span class="s0">#else</span>
<span class="s0">#error </span><span class="s4">&quot;Macro name collisions may happen with unsupported compiler.&quot;</span>
<span class="s0">#ifdef </span><span class="s1">FORCE_INLINE</span>
<span class="s0">#undef </span><span class="s1">FORCE_INLINE</span>
<span class="s0">#endif</span>
<span class="s0">#define </span><span class="s1">FORCE_INLINE </span><span class="s0">static </span><span class="s5">inline</span>
<span class="s0">#ifndef </span><span class="s1">ALIGN_STRUCT</span>
<span class="s0">#define </span><span class="s1">ALIGN_STRUCT(x) __declspec(align(x))</span>
<span class="s0">#endif</span>
<span class="s0">#endif</span>

<span class="s0">#include </span><span class="s1">&lt;stdint.h&gt;</span>
<span class="s0">#include </span><span class="s1">&lt;stdlib.h&gt;</span>

<span class="s2">// These cause the build to fail on raspberry pi with 'unsupported target'</span>
<span class="s2">// and don't seem to do anything particularly useful</span>
<span class="s2">///* Architecture-specific build options */</span>
<span class="s2">///* FIXME: #pragma GCC push_options is only available on GCC */</span>
<span class="s2">//#if defined(__GNUC__)</span>
<span class="s2">//#if defined(__arm__) &amp;&amp; __ARM_ARCH == 7</span>
<span class="s2">///* According to ARM C Language Extensions Architecture specification,</span>
<span class="s2">// * __ARM_NEON is defined to a value indicating the Advanced SIMD (NEON)</span>
<span class="s2">// * architecture supported.</span>
<span class="s2">// */</span>
<span class="s2">//#if !defined(__ARM_NEON) || !defined(__ARM_NEON__)</span>
<span class="s2">//#error &quot;You must enable NEON instructions (e.g. -mfpu=neon) to use SSE2NEON.&quot;</span>
<span class="s2">//#endif</span>
<span class="s2">//#pragma GCC push_options</span>
<span class="s2">//#pragma GCC target(&quot;fpu=neon&quot;)</span>
<span class="s2">//#elif defined(__aarch64__)</span>
<span class="s2">//#pragma GCC push_options</span>
<span class="s2">//#pragma GCC target(&quot;+simd&quot;)</span>
<span class="s2">//#else</span>
<span class="s2">//#error &quot;Unsupported target. Must be either ARMv7-A+NEON or ARMv8-A.&quot;</span>
<span class="s2">//#endif</span>
<span class="s2">//#endif</span>

<span class="s0">#include </span><span class="s1">&lt;arm_neon.h&gt;</span>

<span class="s2">/* Rounding functions require either Aarch64 instructions or libm failback */</span>
<span class="s0">#if </span><span class="s1">!defined(__aarch64__)</span>
<span class="s0">#include </span><span class="s1">&lt;math.h&gt;</span>
<span class="s0">#endif</span>

<span class="s2">/* &quot;__has_builtin&quot; can be used to query support for built-in functions 
 * provided by gcc/clang and other compilers that support it. 
 */</span>
<span class="s0">#ifndef </span><span class="s1">__has_builtin </span><span class="s2">/* GCC prior to 10 or non-clang compilers */</span>
<span class="s2">/* Compatibility with gcc &lt;= 9 */</span>
<span class="s0">#if </span><span class="s1">__GNUC__ &lt;= </span><span class="s3">9</span>
<span class="s0">#define </span><span class="s1">__has_builtin(x) HAS##x</span>
<span class="s0">#define </span><span class="s1">HAS__builtin_popcount </span><span class="s3">1</span>
<span class="s0">#define </span><span class="s1">HAS__builtin_popcountll </span><span class="s3">1</span>
<span class="s0">#else</span>
<span class="s0">#define </span><span class="s1">__has_builtin(x) </span><span class="s3">0</span>
<span class="s0">#endif</span>
<span class="s0">#endif</span>

<span class="s2">/** 
 * MACRO for shuffle parameter for _mm_shuffle_ps(). 
 * Argument fp3 is a digit[0123] that represents the fp from argument &quot;b&quot; 
 * of mm_shuffle_ps that will be placed in fp3 of result. fp2 is the same 
 * for fp2 in result. fp1 is a digit[0123] that represents the fp from 
 * argument &quot;a&quot; of mm_shuffle_ps that will be places in fp1 of result. 
 * fp0 is the same for fp0 of result. 
 */</span>
<span class="s0">#define </span><span class="s1">_MM_SHUFFLE(fp3, fp2, fp1, fp0) \</span>
    <span class="s1">(((fp3) &lt;&lt; </span><span class="s3">6</span><span class="s1">) | ((fp2) &lt;&lt; </span><span class="s3">4</span><span class="s1">) | ((fp1) &lt;&lt; </span><span class="s3">2</span><span class="s1">) | ((fp0)))</span>

<span class="s2">/* Rounding mode macros. */</span>
<span class="s0">#define </span><span class="s1">_MM_FROUND_TO_NEAREST_INT </span><span class="s3">0x00</span>
<span class="s0">#define </span><span class="s1">_MM_FROUND_TO_NEG_INF </span><span class="s3">0x01</span>
<span class="s0">#define </span><span class="s1">_MM_FROUND_TO_POS_INF </span><span class="s3">0x02</span>
<span class="s0">#define </span><span class="s1">_MM_FROUND_TO_ZERO </span><span class="s3">0x03</span>
<span class="s0">#define </span><span class="s1">_MM_FROUND_CUR_DIRECTION </span><span class="s3">0x04</span>
<span class="s0">#define </span><span class="s1">_MM_FROUND_NO_EXC </span><span class="s3">0x08</span>

<span class="s2">/* indicate immediate constant argument in a given range */</span>
<span class="s0">#define </span><span class="s1">__constrange(a, b) </span><span class="s0">const</span>

<span class="s2">/* A few intrinsics accept traditional data types like ints or floats, but 
 * most operate on data types that are specific to SSE. 
 * If a vector type ends in d, it contains doubles, and if it does not have 
 * a suffix, it contains floats. An integer vector type can contain any type 
 * of integer, from chars to shorts to unsigned long longs. 
 */</span>
<span class="s0">typedef </span><span class="s1">int64x1_t __m64;</span>
<span class="s0">typedef </span><span class="s1">float32x4_t __m128; </span><span class="s2">/* 128-bit vector containing 4 floats */</span>
<span class="s2">// On ARM 32-bit architecture, the float64x2_t is not supported.</span>
<span class="s2">// The data type __m128d should be represented in a different way for related</span>
<span class="s2">// intrinsic conversion.</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
<span class="s0">typedef </span><span class="s1">float64x2_t __m128d; </span><span class="s2">/* 128-bit vector containing 2 doubles */</span>
<span class="s0">#else</span>
<span class="s0">typedef </span><span class="s1">float32x4_t __m128d;</span>
<span class="s0">#endif</span>
<span class="s0">typedef </span><span class="s1">int64x2_t __m128i; </span><span class="s2">/* 128-bit vector containing integers */</span>

<span class="s2">/* type-safe casting between types */</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_m128_f16(x) vreinterpretq_f32_f16(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128_f32(x) (x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128_f64(x) vreinterpretq_f32_f64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_m128_u8(x) vreinterpretq_f32_u8(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128_u16(x) vreinterpretq_f32_u16(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128_u32(x) vreinterpretq_f32_u32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128_u64(x) vreinterpretq_f32_u64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_m128_s8(x) vreinterpretq_f32_s8(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128_s16(x) vreinterpretq_f32_s16(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128_s32(x) vreinterpretq_f32_s32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128_s64(x) vreinterpretq_f32_s64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_f16_m128(x) vreinterpretq_f16_f32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_f32_m128(x) (x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_f64_m128(x) vreinterpretq_f64_f32(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_u8_m128(x) vreinterpretq_u8_f32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_u16_m128(x) vreinterpretq_u16_f32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_u32_m128(x) vreinterpretq_u32_f32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_u64_m128(x) vreinterpretq_u64_f32(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_s8_m128(x) vreinterpretq_s8_f32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_s16_m128(x) vreinterpretq_s16_f32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_s32_m128(x) vreinterpretq_s32_f32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_s64_m128(x) vreinterpretq_s64_f32(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_m128i_s8(x) vreinterpretq_s64_s8(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128i_s16(x) vreinterpretq_s64_s16(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128i_s32(x) vreinterpretq_s64_s32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128i_s64(x) (x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_m128i_u8(x) vreinterpretq_s64_u8(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128i_u16(x) vreinterpretq_s64_u16(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128i_u32(x) vreinterpretq_s64_u32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128i_u64(x) vreinterpretq_s64_u64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_s8_m128i(x) vreinterpretq_s8_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_s16_m128i(x) vreinterpretq_s16_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_s32_m128i(x) vreinterpretq_s32_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_s64_m128i(x) (x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_u8_m128i(x) vreinterpretq_u8_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_u16_m128i(x) vreinterpretq_u16_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_u32_m128i(x) vreinterpretq_u32_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_u64_m128i(x) vreinterpretq_u64_s64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpret_m64_s8(x) vreinterpret_s64_s8(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_m64_s16(x) vreinterpret_s64_s16(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_m64_s32(x) vreinterpret_s64_s32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_m64_s64(x) (x)</span>

<span class="s0">#define </span><span class="s1">vreinterpret_m64_u8(x) vreinterpret_s64_u8(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_m64_u16(x) vreinterpret_s64_u16(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_m64_u32(x) vreinterpret_s64_u32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_m64_u64(x) vreinterpret_s64_u64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpret_m64_f16(x) vreinterpret_s64_f16(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_m64_f32(x) vreinterpret_s64_f32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_m64_f64(x) vreinterpret_s64_f64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpret_u8_m64(x) vreinterpret_u8_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_u16_m64(x) vreinterpret_u16_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_u32_m64(x) vreinterpret_u32_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_u64_m64(x) vreinterpret_u64_s64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpret_s8_m64(x) vreinterpret_s8_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_s16_m64(x) vreinterpret_s16_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_s32_m64(x) vreinterpret_s32_s64(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpret_s64_m64(x) (x)</span>

<span class="s0">#define </span><span class="s1">vreinterpret_f32_m64(x) vreinterpret_f32_s64(x)</span>

<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128d_s32(x) vreinterpretq_f64_s32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128d_s64(x) vreinterpretq_f64_s64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_m128d_f64(x) (x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_s64_m128d(x) vreinterpretq_s64_f64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_f64_m128d(x) (x)</span>
<span class="s0">#else</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128d_s32(x) vreinterpretq_f32_s32(x)</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_m128d_s64(x) vreinterpretq_f32_s64(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_m128d_f32(x) (x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_s64_m128d(x) vreinterpretq_s64_f32(x)</span>

<span class="s0">#define </span><span class="s1">vreinterpretq_f32_m128d(x) (x)</span>
<span class="s0">#endif</span>

<span class="s2">// A struct is defined in this header file called 'SIMDVec' which can be used</span>
<span class="s2">// by applications which attempt to access the contents of an _m128 struct</span>
<span class="s2">// directly.  It is important to note that accessing the __m128 struct directly</span>
<span class="s2">// is bad coding practice by Microsoft: @see:</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/ayeb3ayc.aspx</span>
<span class="s2">//</span>
<span class="s2">// However, some legacy source code may try to access the contents of an __m128</span>
<span class="s2">// struct directly so the developer can use the SIMDVec as an alias for it.  Any</span>
<span class="s2">// casting must be done manually by the developer, as you cannot cast or</span>
<span class="s2">// otherwise alias the base NEON data type for intrinsic operations.</span>
<span class="s2">//</span>
<span class="s2">// union intended to allow direct access to an __m128 variable using the names</span>
<span class="s2">// that the MSVC compiler provides.  This union should really only be used when</span>
<span class="s2">// trying to access the members of the vector as integer values.  GCC/clang</span>
<span class="s2">// allow native access to the float members through a simple array access</span>
<span class="s2">// operator (in C since 4.6, in C++ since 4.8).</span>
<span class="s2">//</span>
<span class="s2">// Ideally direct accesses to SIMD vectors should not be used since it can cause</span>
<span class="s2">// a performance hit.  If it really is needed however, the original __m128</span>
<span class="s2">// variable can be aliased with a pointer to this union and used to access</span>
<span class="s2">// individual components.  The use of this union should be hidden behind a macro</span>
<span class="s2">// that is used throughout the codebase to access the members instead of always</span>
<span class="s2">// declaring this type of variable.</span>
<span class="s0">typedef union </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) SIMDVec {</span>
    <span class="s0">float </span><span class="s1">m128_f32[</span><span class="s3">4</span><span class="s1">];     </span><span class="s2">// as floats - DON'T USE. Added for convenience.</span>
    <span class="s1">int8_t m128_i8[</span><span class="s3">16</span><span class="s1">];    </span><span class="s2">// as signed 8-bit integers.</span>
    <span class="s1">int16_t m128_i16[</span><span class="s3">8</span><span class="s1">];   </span><span class="s2">// as signed 16-bit integers.</span>
    <span class="s1">int32_t m128_i32[</span><span class="s3">4</span><span class="s1">];   </span><span class="s2">// as signed 32-bit integers.</span>
    <span class="s1">int64_t m128_i64[</span><span class="s3">2</span><span class="s1">];   </span><span class="s2">// as signed 64-bit integers.</span>
    <span class="s1">uint8_t m128_u8[</span><span class="s3">16</span><span class="s1">];   </span><span class="s2">// as unsigned 8-bit integers.</span>
    <span class="s1">uint16_t m128_u16[</span><span class="s3">8</span><span class="s1">];  </span><span class="s2">// as unsigned 16-bit integers.</span>
    <span class="s1">uint32_t m128_u32[</span><span class="s3">4</span><span class="s1">];  </span><span class="s2">// as unsigned 32-bit integers.</span>
    <span class="s1">uint64_t m128_u64[</span><span class="s3">2</span><span class="s1">];  </span><span class="s2">// as unsigned 64-bit integers.</span>
<span class="s1">} SIMDVec;</span>

<span class="s2">// casting using SIMDVec</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_nth_u64_m128i(x, n) (((SIMDVec *) &amp;x)</span><span class="s6">-&gt;</span><span class="s1">m128_u64[n])</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_nth_u32_m128i(x, n) (((SIMDVec *) &amp;x)</span><span class="s6">-&gt;</span><span class="s1">m128_u32[n])</span>
<span class="s0">#define </span><span class="s1">vreinterpretq_nth_u8_m128i(x, n) (((SIMDVec *) &amp;x)</span><span class="s6">-&gt;</span><span class="s1">m128_u8[n])</span>

<span class="s2">/* Backwards compatibility for compilers with lack of specific type support */</span>

<span class="s2">// Older gcc does not define vld1q_u8_x4 type</span>
<span class="s0">#if </span><span class="s1">defined(__GNUC__) &amp;&amp; !defined(__clang__)</span>
<span class="s0">#if </span><span class="s1">__GNUC__ &lt;= </span><span class="s3">9</span>
<span class="s1">FORCE_INLINE uint8x16x4_t vld1q_u8_x4(</span><span class="s0">const </span><span class="s1">uint8_t *p)</span>
<span class="s1">{</span>
    <span class="s1">uint8x16x4_t ret;</span>
    <span class="s1">ret.val[</span><span class="s3">0</span><span class="s1">] = vld1q_u8(p + </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">ret.val[</span><span class="s3">1</span><span class="s1">] = vld1q_u8(p + </span><span class="s3">16</span><span class="s1">);</span>
    <span class="s1">ret.val[</span><span class="s3">2</span><span class="s1">] = vld1q_u8(p + </span><span class="s3">32</span><span class="s1">);</span>
    <span class="s1">ret.val[</span><span class="s3">3</span><span class="s1">] = vld1q_u8(p + </span><span class="s3">48</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">ret;</span>
<span class="s1">}</span>
<span class="s0">#endif</span>
<span class="s0">#endif</span>

<span class="s2">/* Function Naming Conventions 
 * The naming convention of SSE intrinsics is straightforward. A generic SSE 
 * intrinsic function is given as follows: 
 *   _mm_&lt;name&gt;_&lt;data_type&gt; 
 * 
 * The parts of this format are given as follows: 
 * 1. &lt;name&gt; describes the operation performed by the intrinsic 
 * 2. &lt;data_type&gt; identifies the data type of the function's primary arguments 
 * 
 * This last part, &lt;data_type&gt;, is a little complicated. It identifies the 
 * content of the input values, and can be set to any of the following values: 
 * + ps - vectors contain floats (ps stands for packed single-precision) 
 * + pd - vectors cantain doubles (pd stands for packed double-precision) 
 * + epi8/epi16/epi32/epi64 - vectors contain 8-bit/16-bit/32-bit/64-bit 
 *                            signed integers 
 * + epu8/epu16/epu32/epu64 - vectors contain 8-bit/16-bit/32-bit/64-bit 
 *                            unsigned integers 
 * + si128 - unspecified 128-bit vector or 256-bit vector 
 * + m128/m128i/m128d - identifies input vector types when they are different 
 *                      than the type of the returned vector 
 * 
 * For example, _mm_setzero_ps. The _mm implies that the function returns 
 * a 128-bit vector. The _ps at the end implies that the argument vectors 
 * contain floats. 
 * 
 * A complete example: Byte Shuffle - pshufb (_mm_shuffle_epi8) 
 *   // Set packed 16-bit integers. 128 bits, 8 short, per 16 bits 
 *   __m128i v_in = _mm_setr_epi16(1, 2, 3, 4, 5, 6, 7, 8); 
 *   // Set packed 8-bit integers 
 *   // 128 bits, 16 chars, per 8 bits 
 *   __m128i v_perm = _mm_setr_epi8(1, 0,  2,  3, 8, 9, 10, 11, 
 *                                  4, 5, 12, 13, 6, 7, 14, 15); 
 *   // Shuffle packed 8-bit integers 
 *   __m128i v_out = _mm_shuffle_epi8(v_in, v_perm); // pshufb 
 * 
 * Data (Number, Binary, Byte Index): 
    +------+------+-------------+------+------+-------------+ 
    |      1      |      2      |      3      |      4      | Number 
    +------+------+------+------+------+------+------+------+ 
    | 0000 | 0001 | 0000 | 0010 | 0000 | 0011 | 0000 | 0100 | Binary 
    +------+------+------+------+------+------+------+------+ 
    |    0 |    1 |    2 |    3 |    4 |    5 |    6 |    7 | Index 
    +------+------+------+------+------+------+------+------+ 
 
    +------+------+------+------+------+------+------+------+ 
    |      5      |      6      |      7      |      8      | Number 
    +------+------+------+------+------+------+------+------+ 
    | 0000 | 0101 | 0000 | 0110 | 0000 | 0111 | 0000 | 1000 | Binary 
    +------+------+------+------+------+------+------+------+ 
    |    8 |    9 |   10 |   11 |   12 |   13 |   14 |   15 | Index 
    +------+------+------+------+------+------+------+------+ 
 * Index (Byte Index): 
    +------+------+------+------+------+------+------+------+ 
    |    1 |    0 |    2 |    3 |    8 |    9 |   10 |   11 | 
    +------+------+------+------+------+------+------+------+ 
 
    +------+------+------+------+------+------+------+------+ 
    |    4 |    5 |   12 |   13 |    6 |    7 |   14 |   15 | 
    +------+------+------+------+------+------+------+------+ 
 * Result: 
    +------+------+------+------+------+------+------+------+ 
    |    1 |    0 |    2 |    3 |    8 |    9 |   10 |   11 | Index 
    +------+------+------+------+------+------+------+------+ 
    | 0001 | 0000 | 0000 | 0010 | 0000 | 0101 | 0000 | 0110 | Binary 
    +------+------+------+------+------+------+------+------+ 
    |     256     |      2      |      5      |      6      | Number 
    +------+------+------+------+------+------+------+------+ 
 
    +------+------+------+------+------+------+------+------+ 
    |    4 |    5 |   12 |   13 |    6 |    7 |   14 |   15 | Index 
    +------+------+------+------+------+------+------+------+ 
    | 0000 | 0011 | 0000 | 0111 | 0000 | 0100 | 0000 | 1000 | Binary 
    +------+------+------+------+------+------+------+------+ 
    |      3      |      7      |      4      |      8      | Number 
    +------+------+------+------+------+------+-------------+ 
 */</span>

<span class="s2">/* Set/get methods */</span>

<span class="s2">/* Constants for use with _mm_prefetch.  */</span>
<span class="s0">enum </span><span class="s1">_mm_hint {</span>
    <span class="s1">_MM_HINT_NTA = </span><span class="s3">0</span><span class="s1">,  </span><span class="s2">/* load data to L1 and L2 cache, mark it as NTA */</span>
    <span class="s1">_MM_HINT_T0 = </span><span class="s3">1</span><span class="s1">,   </span><span class="s2">/* load data to L1 and L2 cache */</span>
    <span class="s1">_MM_HINT_T1 = </span><span class="s3">2</span><span class="s1">,   </span><span class="s2">/* load data to L2 cache only */</span>
    <span class="s1">_MM_HINT_T2 = </span><span class="s3">3</span><span class="s1">,   </span><span class="s2">/* load data to L2 cache only, mark it as NTA */</span>
    <span class="s1">_MM_HINT_ENTA = </span><span class="s3">4</span><span class="s1">, </span><span class="s2">/* exclusive version of _MM_HINT_NTA */</span>
    <span class="s1">_MM_HINT_ET0 = </span><span class="s3">5</span><span class="s1">,  </span><span class="s2">/* exclusive version of _MM_HINT_T0 */</span>
    <span class="s1">_MM_HINT_ET1 = </span><span class="s3">6</span><span class="s1">,  </span><span class="s2">/* exclusive version of _MM_HINT_T1 */</span>
    <span class="s1">_MM_HINT_ET2 = </span><span class="s3">7   </span><span class="s2">/* exclusive version of _MM_HINT_T2 */</span>
<span class="s1">};</span>

<span class="s2">// Loads one cache line of data from address p to a location closer to the</span>
<span class="s2">// processor. https://msdn.microsoft.com/en-us/library/84szxsww(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_prefetch(</span><span class="s0">const void </span><span class="s1">*p, </span><span class="s0">int </span><span class="s1">i)</span>
<span class="s1">{</span>
    <span class="s1">(</span><span class="s0">void</span><span class="s1">) i;</span>
    <span class="s1">__builtin_prefetch(p);</span>
<span class="s1">}</span>

<span class="s2">// Copy the lower single-precision (32-bit) floating-point element of a to dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := a[31:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtss_f32</span>
<span class="s1">FORCE_INLINE </span><span class="s0">float </span><span class="s1">_mm_cvtss_f32(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s1">}</span>

<span class="s2">// Convert the lower single-precision (32-bit) floating-point element in a to a</span>
<span class="s2">// 32-bit integer, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_FP32_To_Int32(a[31:0])</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtss_si32</span>
<span class="s0">#define </span><span class="s1">_mm_cvtss_si32(a) _mm_cvt_ss2si(a)</span>

<span class="s2">// Convert the lower single-precision (32-bit) floating-point element in a to a</span>
<span class="s2">// 64-bit integer, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := Convert_FP32_To_Int64(a[31:0])</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtss_si64</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_cvtss_si64(__m128 a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_s64(</span>
        <span class="s1">vreinterpretq_s64_s32(vcvtnq_s32_f32(vreinterpretq_f32_m128(a))), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s0">#else</span>
    <span class="s1">float32_t data = vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32_t diff = data - floor(data);</span>
    <span class="s0">if </span><span class="s1">(diff &gt; </span><span class="s3">0.5</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">(int64_t) ceil(data);</span>
    <span class="s0">if </span><span class="s1">(diff == </span><span class="s3">0.5</span><span class="s1">) {</span>
        <span class="s1">int64_t f = (int64_t) floor(data);</span>
        <span class="s1">int64_t c = (int64_t) ceil(data);</span>
        <span class="s0">return </span><span class="s1">c &amp; </span><span class="s3">1 </span><span class="s1">? f : c;</span>
    <span class="s1">}</span>
    <span class="s0">return </span><span class="s1">(int64_t) floor(data);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Convert packed single-precision (32-bit) floating-point elements in a to</span>
<span class="s2">// packed 32-bit integers with truncation, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//      i := 32*j</span>
<span class="s2">//      dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtt_ps2pi</span>
<span class="s1">FORCE_INLINE __m64 _mm_cvtt_ps2pi(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s32(</span>
        <span class="s1">vget_low_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a))));</span>
<span class="s1">}</span>

<span class="s2">// Convert the lower single-precision (32-bit) floating-point element in a to a</span>
<span class="s2">// 32-bit integer with truncation, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtt_ss2si</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_cvtt_ss2si(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a)), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s1">}</span>

<span class="s2">// Convert packed single-precision (32-bit) floating-point elements in a to</span>
<span class="s2">// packed 32-bit integers with truncation, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//      i := 32*j</span>
<span class="s2">//      dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvttps_pi32</span>
<span class="s0">#define </span><span class="s1">_mm_cvttps_pi32(a) _mm_cvtt_ps2pi(a)</span>

<span class="s2">// Convert the lower single-precision (32-bit) floating-point element in a to a</span>
<span class="s2">// 32-bit integer with truncation, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvttss_si32</span>
<span class="s0">#define </span><span class="s1">_mm_cvttss_si32(a) _mm_cvtt_ss2si(a)</span>

<span class="s2">// Convert the lower single-precision (32-bit) floating-point element in a to a</span>
<span class="s2">// 64-bit integer with truncation, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvttss_si64</span>
<span class="s1">FORCE_INLINE int64_t _mm_cvttss_si64(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_s64(</span>
        <span class="s1">vmovl_s32(vget_low_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a)))), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s1">}</span>

<span class="s2">// Sets the 128-bit value to zero</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/ys7dw0kh(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_setzero_si128(</span><span class="s0">void</span><span class="s1">)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vdupq_n_s32(</span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Clears the four single-precision, floating-point values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/tk1t2tbz(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_setzero_ps(</span><span class="s0">void</span><span class="s1">)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vdupq_n_f32(</span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Return vector of type __m128d with all elements set to zero.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_setzero_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_setzero_pd(</span><span class="s0">void</span><span class="s1">)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(vdupq_n_f64(</span><span class="s3">0</span><span class="s1">));</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f32(vdupq_n_f32(</span><span class="s3">0</span><span class="s1">));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Sets the four single-precision, floating-point values to w.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := r1 := r2 := r3 := w</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/2x1se8ha(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_set1_ps(</span><span class="s0">float </span><span class="s1">_w)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vdupq_n_f32(_w));</span>
<span class="s1">}</span>

<span class="s2">// Sets the four single-precision, floating-point values to w.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/2x1se8ha(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_set_ps1(</span><span class="s0">float </span><span class="s1">_w)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vdupq_n_f32(_w));</span>
<span class="s1">}</span>

<span class="s2">// Sets the four single-precision, floating-point values to the four inputs.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/afh0zf75(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_set_ps(</span><span class="s0">float </span><span class="s1">w, </span><span class="s0">float </span><span class="s1">z, </span><span class="s0">float </span><span class="s1">y, </span><span class="s0">float </span><span class="s1">x)</span>
<span class="s1">{</span>
    <span class="s0">float </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {x, y, z, w};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vld1q_f32(data));</span>
<span class="s1">}</span>

<span class="s2">// Copy single-precision (32-bit) floating-point element a to the lower element</span>
<span class="s2">// of dst, and zero the upper 3 elements.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_ss</span>
<span class="s1">FORCE_INLINE __m128 _mm_set_ss(</span><span class="s0">float </span><span class="s1">a)</span>
<span class="s1">{</span>
    <span class="s0">float </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {a, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vld1q_f32(data));</span>
<span class="s1">}</span>

<span class="s2">// Sets the four single-precision, floating-point values to the four inputs in</span>
<span class="s2">// reverse order.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/d2172ct3(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_setr_ps(</span><span class="s0">float </span><span class="s1">w, </span><span class="s0">float </span><span class="s1">z, </span><span class="s0">float </span><span class="s1">y, </span><span class="s0">float </span><span class="s1">x)</span>
<span class="s1">{</span>
    <span class="s0">float </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {w, z, y, x};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vld1q_f32(data));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 8 signed 16-bit integer values in reverse order.</span>
<span class="s2">//</span>
<span class="s2">// Return Value</span>
<span class="s2">//   r0 := w0</span>
<span class="s2">//   r1 := w1</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := w7</span>
<span class="s1">FORCE_INLINE __m128i _mm_setr_epi16(</span><span class="s0">short </span><span class="s1">w0,</span>
                                    <span class="s0">short </span><span class="s1">w1,</span>
                                    <span class="s0">short </span><span class="s1">w2,</span>
                                    <span class="s0">short </span><span class="s1">w3,</span>
                                    <span class="s0">short </span><span class="s1">w4,</span>
                                    <span class="s0">short </span><span class="s1">w5,</span>
                                    <span class="s0">short </span><span class="s1">w6,</span>
                                    <span class="s0">short </span><span class="s1">w7)</span>
<span class="s1">{</span>
    <span class="s1">int16_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">8</span><span class="s1">] = {w0, w1, w2, w3, w4, w5, w6, w7};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vld1q_s16((int16_t *) data));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 4 signed 32-bit integer values in reverse order</span>
<span class="s2">// https://technet.microsoft.com/en-us/library/security/27yb3ee5(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_setr_epi32(</span><span class="s0">int </span><span class="s1">i3, </span><span class="s0">int </span><span class="s1">i2, </span><span class="s0">int </span><span class="s1">i1, </span><span class="s0">int </span><span class="s1">i0)</span>
<span class="s1">{</span>
    <span class="s1">int32_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {i3, i2, i1, i0};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vld1q_s32(data));</span>
<span class="s1">}</span>

<span class="s2">// Set packed 64-bit integers in dst with the supplied values in reverse order.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_setr_epi64</span>
<span class="s1">FORCE_INLINE __m128i _mm_setr_epi64(__m64 e1, __m64 e0)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vcombine_s64(e1, e0));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 16 signed 8-bit integer values to b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := b</span>
<span class="s2">//   r1 := b</span>
<span class="s2">//   ...</span>
<span class="s2">//   r15 := b</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/6e14xhyf(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_set1_epi8(</span><span class="s0">signed char </span><span class="s1">w)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(vdupq_n_s8(w));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 8 signed 16-bit integer values to w.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := w</span>
<span class="s2">//   r1 := w</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := w</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/k0ya3x0e(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_set1_epi16(</span><span class="s0">short </span><span class="s1">w)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vdupq_n_s16(w));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 16 signed 8-bit integer values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/x0cx8zd3(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_set_epi8(</span><span class="s0">signed char </span><span class="s1">b15,</span>
                                  <span class="s0">signed char </span><span class="s1">b14,</span>
                                  <span class="s0">signed char </span><span class="s1">b13,</span>
                                  <span class="s0">signed char </span><span class="s1">b12,</span>
                                  <span class="s0">signed char </span><span class="s1">b11,</span>
                                  <span class="s0">signed char </span><span class="s1">b10,</span>
                                  <span class="s0">signed char </span><span class="s1">b9,</span>
                                  <span class="s0">signed char </span><span class="s1">b8,</span>
                                  <span class="s0">signed char </span><span class="s1">b7,</span>
                                  <span class="s0">signed char </span><span class="s1">b6,</span>
                                  <span class="s0">signed char </span><span class="s1">b5,</span>
                                  <span class="s0">signed char </span><span class="s1">b4,</span>
                                  <span class="s0">signed char </span><span class="s1">b3,</span>
                                  <span class="s0">signed char </span><span class="s1">b2,</span>
                                  <span class="s0">signed char </span><span class="s1">b1,</span>
                                  <span class="s0">signed char </span><span class="s1">b0)</span>
<span class="s1">{</span>
    <span class="s1">int8_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">)</span>
        <span class="s1">data[</span><span class="s3">16</span><span class="s1">] = {(int8_t) b0,  (int8_t) b1,  (int8_t) b2,  (int8_t) b3,</span>
                    <span class="s1">(int8_t) b4,  (int8_t) b5,  (int8_t) b6,  (int8_t) b7,</span>
                    <span class="s1">(int8_t) b8,  (int8_t) b9,  (int8_t) b10, (int8_t) b11,</span>
                    <span class="s1">(int8_t) b12, (int8_t) b13, (int8_t) b14, (int8_t) b15};</span>
    <span class="s0">return </span><span class="s1">(__m128i) vld1q_s8(data);</span>
<span class="s1">}</span>

<span class="s2">// Sets the 8 signed 16-bit integer values.</span>
<span class="s2">// https://msdn.microsoft.com/en-au/library/3e0fek84(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_set_epi16(</span><span class="s0">short </span><span class="s1">i7,</span>
                                   <span class="s0">short </span><span class="s1">i6,</span>
                                   <span class="s0">short </span><span class="s1">i5,</span>
                                   <span class="s0">short </span><span class="s1">i4,</span>
                                   <span class="s0">short </span><span class="s1">i3,</span>
                                   <span class="s0">short </span><span class="s1">i2,</span>
                                   <span class="s0">short </span><span class="s1">i1,</span>
                                   <span class="s0">short </span><span class="s1">i0)</span>
<span class="s1">{</span>
    <span class="s1">int16_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">8</span><span class="s1">] = {i0, i1, i2, i3, i4, i5, i6, i7};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vld1q_s16(data));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 16 signed 8-bit integer values in reverse order.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/2khb9c7k(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_setr_epi8(</span><span class="s0">signed char </span><span class="s1">b0,</span>
                                   <span class="s0">signed char </span><span class="s1">b1,</span>
                                   <span class="s0">signed char </span><span class="s1">b2,</span>
                                   <span class="s0">signed char </span><span class="s1">b3,</span>
                                   <span class="s0">signed char </span><span class="s1">b4,</span>
                                   <span class="s0">signed char </span><span class="s1">b5,</span>
                                   <span class="s0">signed char </span><span class="s1">b6,</span>
                                   <span class="s0">signed char </span><span class="s1">b7,</span>
                                   <span class="s0">signed char </span><span class="s1">b8,</span>
                                   <span class="s0">signed char </span><span class="s1">b9,</span>
                                   <span class="s0">signed char </span><span class="s1">b10,</span>
                                   <span class="s0">signed char </span><span class="s1">b11,</span>
                                   <span class="s0">signed char </span><span class="s1">b12,</span>
                                   <span class="s0">signed char </span><span class="s1">b13,</span>
                                   <span class="s0">signed char </span><span class="s1">b14,</span>
                                   <span class="s0">signed char </span><span class="s1">b15)</span>
<span class="s1">{</span>
    <span class="s1">int8_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">)</span>
        <span class="s1">data[</span><span class="s3">16</span><span class="s1">] = {(int8_t) b0,  (int8_t) b1,  (int8_t) b2,  (int8_t) b3,</span>
                    <span class="s1">(int8_t) b4,  (int8_t) b5,  (int8_t) b6,  (int8_t) b7,</span>
                    <span class="s1">(int8_t) b8,  (int8_t) b9,  (int8_t) b10, (int8_t) b11,</span>
                    <span class="s1">(int8_t) b12, (int8_t) b13, (int8_t) b14, (int8_t) b15};</span>
    <span class="s0">return </span><span class="s1">(__m128i) vld1q_s8(data);</span>
<span class="s1">}</span>

<span class="s2">// Sets the 4 signed 32-bit integer values to i.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := i</span>
<span class="s2">//   r1 := i</span>
<span class="s2">//   r2 := i</span>
<span class="s2">//   r3 := I</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/h4xscxat(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_set1_epi32(</span><span class="s0">int </span><span class="s1">_i)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vdupq_n_s32(_i));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 2 signed 64-bit integer values to i.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/whtfzhzk(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128i _mm_set1_epi64(__m64 _i)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vdupq_n_s64((int64_t) _i));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 2 signed 64-bit integer values to i.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set1_epi64x</span>
<span class="s1">FORCE_INLINE __m128i _mm_set1_epi64x(int64_t _i)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vdupq_n_s64(_i));</span>
<span class="s1">}</span>

<span class="s2">// Sets the 4 signed 32-bit integer values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/019beekt(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_set_epi32(</span><span class="s0">int </span><span class="s1">i3, </span><span class="s0">int </span><span class="s1">i2, </span><span class="s0">int </span><span class="s1">i1, </span><span class="s0">int </span><span class="s1">i0)</span>
<span class="s1">{</span>
    <span class="s1">int32_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {i0, i1, i2, i3};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vld1q_s32(data));</span>
<span class="s1">}</span>

<span class="s2">// Returns the __m128i structure with its two 64-bit integer values</span>
<span class="s2">// initialized to the values of the two 64-bit integers passed in.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/dk2sdw0h(v=vs.120).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_set_epi64x(int64_t i1, int64_t i2)</span>
<span class="s1">{</span>
    <span class="s1">int64_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">2</span><span class="s1">] = {i2, i1};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vld1q_s64(data));</span>
<span class="s1">}</span>

<span class="s2">// Returns the __m128i structure with its two 64-bit integer values</span>
<span class="s2">// initialized to the values of the two 64-bit integers passed in.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/dk2sdw0h(v=vs.120).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_set_epi64(__m64 i1, __m64 i2)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_set_epi64x((int64_t) i1, (int64_t) i2);</span>
<span class="s1">}</span>

<span class="s2">// Set packed double-precision (64-bit) floating-point elements in dst with the</span>
<span class="s2">// supplied values.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_set_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_set_pd(</span><span class="s0">double </span><span class="s1">e1, </span><span class="s0">double </span><span class="s1">e0)</span>
<span class="s1">{</span>
    <span class="s0">double </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">2</span><span class="s1">] = {e0, e1};</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(vld1q_f64((float64_t *) data));</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f32(vld1q_f32((float32_t *) data));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Stores four single-precision, floating-point values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/s3h4ay6y(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_store_ps(</span><span class="s0">float </span><span class="s1">*p, __m128 a)</span>
<span class="s1">{</span>
    <span class="s1">vst1q_f32(p, vreinterpretq_f32_m128(a));</span>
<span class="s1">}</span>

<span class="s2">// Stores four single-precision, floating-point values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/44e30x22(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_storeu_ps(</span><span class="s0">float </span><span class="s1">*p, __m128 a)</span>
<span class="s1">{</span>
    <span class="s1">vst1q_f32(p, vreinterpretq_f32_m128(a));</span>
<span class="s1">}</span>

<span class="s2">// Stores four 32-bit integer values as (as a __m128i value) at the address p.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/edk11s13(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_store_si128(__m128i *p, __m128i a)</span>
<span class="s1">{</span>
    <span class="s1">vst1q_s32((int32_t *) p, vreinterpretq_s32_m128i(a));</span>
<span class="s1">}</span>

<span class="s2">// Stores four 32-bit integer values as (as a __m128i value) at the address p.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/edk11s13(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_storeu_si128(__m128i *p, __m128i a)</span>
<span class="s1">{</span>
    <span class="s1">vst1q_s32((int32_t *) p, vreinterpretq_s32_m128i(a));</span>
<span class="s1">}</span>

<span class="s2">// Stores the lower single - precision, floating - point value.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/tzz10fbx(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_store_ss(</span><span class="s0">float </span><span class="s1">*p, __m128 a)</span>
<span class="s1">{</span>
    <span class="s1">vst1q_lane_f32(p, vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s1">}</span>

<span class="s2">// Store 128-bits (composed of 2 packed double-precision (64-bit) floating-point</span>
<span class="s2">// elements) from a into memory. mem_addr must be aligned on a 16-byte boundary</span>
<span class="s2">// or a general-protection exception may be generated.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_store_pd</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_store_pd(</span><span class="s0">double </span><span class="s1">*mem_addr, __m128d a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">vst1q_f64((float64_t *) mem_addr, vreinterpretq_f64_m128d(a));</span>
<span class="s0">#else</span>
    <span class="s1">vst1q_f32((float32_t *) mem_addr, vreinterpretq_f32_m128d(a));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Store 128-bits (composed of 2 packed double-precision (64-bit) floating-point</span>
<span class="s2">// elements) from a into memory. mem_addr does not need to be aligned on any</span>
<span class="s2">// particular boundary.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_storeu_pd</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_storeu_pd(</span><span class="s0">double </span><span class="s1">*mem_addr, __m128d a)</span>
<span class="s1">{</span>
    <span class="s1">_mm_store_pd(mem_addr, a);</span>
<span class="s1">}</span>

<span class="s2">// Reads the lower 64 bits of b and stores them into the lower 64 bits of a.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/hhwf428f%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_storel_epi64(__m128i *a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">uint64x1_t hi = vget_high_u64(vreinterpretq_u64_m128i(*a));</span>
    <span class="s1">uint64x1_t lo = vget_low_u64(vreinterpretq_u64_m128i(b));</span>
    <span class="s1">*a = vreinterpretq_m128i_u64(vcombine_u64(lo, hi));</span>
<span class="s1">}</span>

<span class="s2">// Stores the lower two single-precision floating point values of a to the</span>
<span class="s2">// address p.</span>
<span class="s2">//</span>
<span class="s2">//   *p0 := a0</span>
<span class="s2">//   *p1 := a1</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/h54t98ks(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_storel_pi(__m64 *p, __m128 a)</span>
<span class="s1">{</span>
    <span class="s1">*p = vreinterpret_m64_f32(vget_low_f32(a));</span>
<span class="s1">}</span>

<span class="s2">// Stores the upper two single-precision, floating-point values of a to the</span>
<span class="s2">// address p.</span>
<span class="s2">//</span>
<span class="s2">//   *p0 := a2</span>
<span class="s2">//   *p1 := a3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/a7525fs8(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_storeh_pi(__m64 *p, __m128 a)</span>
<span class="s1">{</span>
    <span class="s1">*p = vreinterpret_m64_f32(vget_high_f32(a));</span>
<span class="s1">}</span>

<span class="s2">// Loads a single single-precision, floating-point value, copying it into all</span>
<span class="s2">// four words</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/5cdkf716(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_load1_ps(</span><span class="s0">const float </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vld1q_dup_f32(p));</span>
<span class="s1">}</span>

<span class="s2">// Load a single-precision (32-bit) floating-point element from memory into all</span>
<span class="s2">// elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := MEM[mem_addr+31:mem_addr]</span>
<span class="s2">//   dst[63:32] := MEM[mem_addr+31:mem_addr]</span>
<span class="s2">//   dst[95:64] := MEM[mem_addr+31:mem_addr]</span>
<span class="s2">//   dst[127:96] := MEM[mem_addr+31:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_load_ps1</span>
<span class="s0">#define </span><span class="s1">_mm_load_ps1 _mm_load1_ps</span>

<span class="s2">// Sets the lower two single-precision, floating-point values with 64</span>
<span class="s2">// bits of data loaded from the address p; the upper two values are passed</span>
<span class="s2">// through from a.</span>
<span class="s2">//</span>
<span class="s2">// Return Value</span>
<span class="s2">//   r0 := *p0</span>
<span class="s2">//   r1 := *p1</span>
<span class="s2">//   r2 := a2</span>
<span class="s2">//   r3 := a3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/s57cyak2(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_loadl_pi(__m128 a, __m64 </span><span class="s0">const </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vcombine_f32(vld1_f32((</span><span class="s0">const </span><span class="s1">float32_t *) p), vget_high_f32(a)));</span>
<span class="s1">}</span>

<span class="s2">// Load 4 single-precision (32-bit) floating-point elements from memory into dst</span>
<span class="s2">// in reverse order. mem_addr must be aligned on a 16-byte boundary or a</span>
<span class="s2">// general-protection exception may be generated.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := MEM[mem_addr+127:mem_addr+96]</span>
<span class="s2">//   dst[63:32] := MEM[mem_addr+95:mem_addr+64]</span>
<span class="s2">//   dst[95:64] := MEM[mem_addr+63:mem_addr+32]</span>
<span class="s2">//   dst[127:96] := MEM[mem_addr+31:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadr_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_loadr_ps(</span><span class="s0">const float </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s1">float32x4_t v = vrev64q_f32(vld1q_f32(p));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vextq_f32(v, v, </span><span class="s3">2</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Sets the upper two single-precision, floating-point values with 64</span>
<span class="s2">// bits of data loaded from the address p; the lower two values are passed</span>
<span class="s2">// through from a.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0</span>
<span class="s2">//   r1 := a1</span>
<span class="s2">//   r2 := *p0</span>
<span class="s2">//   r3 := *p1</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/w92wta0x(v%3dvs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_loadh_pi(__m128 a, __m64 </span><span class="s0">const </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vcombine_f32(vget_low_f32(a), vld1_f32((</span><span class="s0">const </span><span class="s1">float32_t *) p)));</span>
<span class="s1">}</span>

<span class="s2">// Loads four single-precision, floating-point values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/zzd50xxt(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_load_ps(</span><span class="s0">const float </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vld1q_f32(p));</span>
<span class="s1">}</span>

<span class="s2">// Loads four single-precision, floating-point values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/x1b16s7z%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_loadu_ps(</span><span class="s0">const float </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s2">// for neon, alignment doesn't matter, so _mm_load_ps and _mm_loadu_ps are</span>
    <span class="s2">// equivalent for neon</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vld1q_f32(p));</span>
<span class="s1">}</span>

<span class="s2">// Load unaligned 16-bit integer from memory into the first element of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[15:0] := MEM[mem_addr+15:mem_addr]</span>
<span class="s2">//   dst[MAX:16] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si16</span>
<span class="s1">FORCE_INLINE __m128i _mm_loadu_si16(</span><span class="s0">const void </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vsetq_lane_s16(*(</span><span class="s0">const </span><span class="s1">int16_t *) p, vdupq_n_s16(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Load unaligned 64-bit integer from memory into the first element of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//   dst[MAX:64] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si64</span>
<span class="s1">FORCE_INLINE __m128i _mm_loadu_si64(</span><span class="s0">const void </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(</span>
        <span class="s1">vcombine_s64(vld1_s64((</span><span class="s0">const </span><span class="s1">int64_t *) p), vdup_n_s64(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s1">}</span>

<span class="s2">// Load a double-precision (64-bit) floating-point element from memory into the</span>
<span class="s2">// lower of dst, and zero the upper element. mem_addr does not need to be</span>
<span class="s2">// aligned on any particular boundary.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//   dst[127:64] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_load_sd</span>
<span class="s1">FORCE_INLINE __m128d _mm_load_sd(</span><span class="s0">const double </span><span class="s1">*p)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(vsetq_lane_f64(*p, vdupq_n_f64(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s0">#else</span>
    <span class="s0">const float </span><span class="s1">*fp = (</span><span class="s0">const float </span><span class="s1">*) p;</span>
    <span class="s0">float </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {fp[</span><span class="s3">0</span><span class="s1">], fp[</span><span class="s3">1</span><span class="s1">], </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f32(vld1q_f32(data));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Loads two double-precision from 16-byte aligned memory, floating-point</span>
<span class="s2">// values.</span>
<span class="s2">//</span>
<span class="s2">//   dst[127:0] := MEM[mem_addr+127:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_load_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_load_pd(</span><span class="s0">const double </span><span class="s1">*p)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(vld1q_f64(p));</span>
<span class="s0">#else</span>
    <span class="s0">const float </span><span class="s1">*fp = (</span><span class="s0">const float </span><span class="s1">*) p;</span>
    <span class="s0">float </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {fp[</span><span class="s3">0</span><span class="s1">], fp[</span><span class="s3">1</span><span class="s1">], fp[</span><span class="s3">2</span><span class="s1">], fp[</span><span class="s3">3</span><span class="s1">]};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f32(vld1q_f32(data));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Loads two double-precision from unaligned memory, floating-point values.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_loadu_pd(</span><span class="s0">const double </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_load_pd(p);</span>
<span class="s1">}</span>

<span class="s2">// Loads an single - precision, floating - point value into the low word and</span>
<span class="s2">// clears the upper three words.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/548bb9h4%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_load_ss(</span><span class="s0">const float </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vsetq_lane_f32(*p, vdupq_n_f32(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_loadl_epi64(__m128i </span><span class="s0">const </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s2">/* Load the lower 64 bits of the value pointed to by p into the 
     * lower 64 bits of the result, zeroing the upper 64 bits of the result. 
     */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vcombine_s32(vld1_s32((int32_t </span><span class="s0">const </span><span class="s1">*) p), vcreate_s32(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s1">}</span>

<span class="s2">// Load a double-precision (64-bit) floating-point element from memory into the</span>
<span class="s2">// lower element of dst, and copy the upper element from a to dst. mem_addr does</span>
<span class="s2">// not need to be aligned on any particular boundary.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//   dst[127:64] := a[127:64]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadl_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_loadl_pd(__m128d a, </span><span class="s0">const double </span><span class="s1">*p)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(</span>
        <span class="s1">vcombine_f64(vld1_f64(p), vget_high_f64(vreinterpretq_f64_m128d(a))));</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f32(</span>
        <span class="s1">vcombine_f32(vld1_f32((</span><span class="s0">const float </span><span class="s1">*) p),</span>
                     <span class="s1">vget_high_f32(vreinterpretq_f32_m128d(a))));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Load 2 double-precision (64-bit) floating-point elements from memory into dst</span>
<span class="s2">// in reverse order. mem_addr must be aligned on a 16-byte boundary or a</span>
<span class="s2">// general-protection exception may be generated.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := MEM[mem_addr+127:mem_addr+64]</span>
<span class="s2">//   dst[127:64] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadr_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_loadr_pd(</span><span class="s0">const double </span><span class="s1">*p)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">float64x2_t v = vld1q_f64(p);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(vextq_f64(v, v, </span><span class="s3">1</span><span class="s1">));</span>
<span class="s0">#else</span>
    <span class="s1">int64x2_t v = vld1q_s64((</span><span class="s0">const </span><span class="s1">int64_t *) p);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_s64(vextq_s64(v, v, </span><span class="s3">1</span><span class="s1">));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Sets the low word to the single-precision, floating-point value of b</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/35hdzazd(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_move_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vsetq_lane_f32(vgetq_lane_f32(vreinterpretq_f32_m128(b), </span><span class="s3">0</span><span class="s1">),</span>
                       <span class="s1">vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Copy the lower 64-bit integer in a to the lower element of dst, and zero the</span>
<span class="s2">// upper element.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0]</span>
<span class="s2">//   dst[127:64] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_move_epi64</span>
<span class="s1">FORCE_INLINE __m128i _mm_move_epi64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(</span>
        <span class="s1">vsetq_lane_s64(</span><span class="s3">0</span><span class="s1">, vreinterpretq_s64_m128i(a), </span><span class="s3">1</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Return vector of type __m128 with undefined elements.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_undefined_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_undefined_ps(</span><span class="s0">void</span><span class="s1">)</span>
<span class="s1">{</span>
    <span class="s1">__m128 a;</span>
    <span class="s0">return </span><span class="s1">a;</span>
<span class="s1">}</span>

<span class="s2">/* Logic/Binary operations */</span>

<span class="s2">// Computes the bitwise AND-NOT of the four single-precision, floating-point</span>
<span class="s2">// values of a and b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := ~a0 &amp; b0</span>
<span class="s2">//   r1 := ~a1 &amp; b1</span>
<span class="s2">//   r2 := ~a2 &amp; b2</span>
<span class="s2">//   r3 := ~a3 &amp; b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/68h7wd02(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_andnot_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_s32(</span>
        <span class="s1">vbicq_s32(vreinterpretq_s32_m128(b),</span>
                  <span class="s1">vreinterpretq_s32_m128(a)));  </span><span class="s2">// *NOTE* argument swap</span>
<span class="s1">}</span>

<span class="s2">// Compute the bitwise NOT of packed double-precision (64-bit) floating-point</span>
<span class="s2">// elements in a and then AND with b, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//       i := j*64</span>
<span class="s2">//       dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_andnot_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_andnot_pd(__m128d a, __m128d b)</span>
<span class="s1">{</span>
    <span class="s2">// *NOTE* argument swap</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_s64(</span>
        <span class="s1">vbicq_s64(vreinterpretq_s64_m128d(b), vreinterpretq_s64_m128d(a)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the bitwise AND of the 128-bit value in b and the bitwise NOT of the</span>
<span class="s2">// 128-bit value in a.</span>
<span class="s2">//</span>
<span class="s2">//   r := (~a) &amp; b</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/1beaceh8(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_andnot_si128(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vbicq_s32(vreinterpretq_s32_m128i(b),</span>
                  <span class="s1">vreinterpretq_s32_m128i(a)));  </span><span class="s2">// *NOTE* argument swap</span>
<span class="s1">}</span>

<span class="s2">// Computes the bitwise AND of the 128-bit value in a and the 128-bit value in</span>
<span class="s2">// b.</span>
<span class="s2">//</span>
<span class="s2">//   r := a &amp; b</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/6d1txsa8(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_and_si128(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vandq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the bitwise AND of the four single-precision, floating-point values</span>
<span class="s2">// of a and b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 &amp; b0</span>
<span class="s2">//   r1 := a1 &amp; b1</span>
<span class="s2">//   r2 := a2 &amp; b2</span>
<span class="s2">//   r3 := a3 &amp; b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/73ck1xc5(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_and_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_s32(</span>
        <span class="s1">vandq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compute the bitwise AND of packed double-precision (64-bit) floating-point</span>
<span class="s2">// elements in a and b, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//     i := j*64</span>
<span class="s2">//     dst[i+63:i] := a[i+63:i] AND b[i+63:i]</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_and_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_and_pd(__m128d a, __m128d b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_s64(</span>
        <span class="s1">vandq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the bitwise OR of the four single-precision, floating-point values</span>
<span class="s2">// of a and b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/7ctdsyy0(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_or_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_s32(</span>
        <span class="s1">vorrq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes bitwise EXOR (exclusive-or) of the four single-precision,</span>
<span class="s2">// floating-point values of a and b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/ss6k3wk8(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_xor_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_s32(</span>
        <span class="s1">veorq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compute the bitwise XOR of packed double-precision (64-bit) floating-point</span>
<span class="s2">// elements in a and b, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//      i := j*64</span>
<span class="s2">//      dst[i+63:i] := a[i+63:i] XOR b[i+63:i]</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_xor_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_xor_pd(__m128d a, __m128d b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_s64(</span>
        <span class="s1">veorq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the bitwise OR of the 128-bit value in a and the 128-bit value in b.</span>
<span class="s2">//</span>
<span class="s2">//   r := a | b</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/ew8ty0db(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_or_si128(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vorrq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the bitwise XOR of the 128-bit value in a and the 128-bit value in</span>
<span class="s2">// b.  https://msdn.microsoft.com/en-us/library/fzt08www(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_xor_si128(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">veorq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Duplicate odd-indexed single-precision (32-bit) floating-point elements</span>
<span class="s2">// from a, and store the results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_movehdup_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_movehdup_ps(__m128 a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_shufflevector)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(__builtin_shufflevector(</span>
        <span class="s1">vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), </span><span class="s3">1</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">3</span><span class="s1">));</span>
<span class="s0">#else</span>
    <span class="s1">float32_t a1 = vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s1">float32_t a3 = vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">3</span><span class="s1">);</span>
    <span class="s0">float </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {a1, a1, a3, a3};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vld1q_f32(data));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Duplicate even-indexed single-precision (32-bit) floating-point elements</span>
<span class="s2">// from a, and store the results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_moveldup_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_moveldup_ps(__m128 a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_shufflevector)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(__builtin_shufflevector(</span>
        <span class="s1">vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">2</span><span class="s1">));</span>
<span class="s0">#else</span>
    <span class="s1">float32_t a0 = vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32_t a2 = vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">2</span><span class="s1">);</span>
    <span class="s0">float </span><span class="s1">ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) data[</span><span class="s3">4</span><span class="s1">] = {a0, a0, a2, a2};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vld1q_f32(data));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Moves the upper two values of B into the lower two values of A.</span>
<span class="s2">//</span>
<span class="s2">//   r3 := a3</span>
<span class="s2">//   r2 := a2</span>
<span class="s2">//   r1 := b3</span>
<span class="s2">//   r0 := b2</span>
<span class="s1">FORCE_INLINE __m128 _mm_movehl_ps(__m128 __A, __m128 __B)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(__A));</span>
    <span class="s1">float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(__B));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(b32, a32));</span>
<span class="s1">}</span>

<span class="s2">// Moves the lower two values of B into the upper two values of A.</span>
<span class="s2">//</span>
<span class="s2">//   r3 := b1</span>
<span class="s2">//   r2 := b0</span>
<span class="s2">//   r1 := a1</span>
<span class="s2">//   r0 := a0</span>
<span class="s1">FORCE_INLINE __m128 _mm_movelh_ps(__m128 __A, __m128 __B)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(__A));</span>
    <span class="s1">float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(__B));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a10, b10));</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute value of packed signed 32-bit integers in a, and store</span>
<span class="s2">// the unsigned results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//     i := j*32</span>
<span class="s2">//     dst[i+31:i] := ABS(a[i+31:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_abs_epi32</span>
<span class="s1">FORCE_INLINE __m128i _mm_abs_epi32(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vabsq_s32(vreinterpretq_s32_m128i(a)));</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute value of packed signed 16-bit integers in a, and store</span>
<span class="s2">// the unsigned results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//     i := j*16</span>
<span class="s2">//     dst[i+15:i] := ABS(a[i+15:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_abs_epi16</span>
<span class="s1">FORCE_INLINE __m128i _mm_abs_epi16(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vabsq_s16(vreinterpretq_s16_m128i(a)));</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute value of packed signed 8-bit integers in a, and store</span>
<span class="s2">// the unsigned results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 15</span>
<span class="s2">//     i := j*8</span>
<span class="s2">//     dst[i+7:i] := ABS(a[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_abs_epi8</span>
<span class="s1">FORCE_INLINE __m128i _mm_abs_epi8(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(vabsq_s8(vreinterpretq_s8_m128i(a)));</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute value of packed signed 32-bit integers in a, and store</span>
<span class="s2">// the unsigned results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//     i := j*32</span>
<span class="s2">//     dst[i+31:i] := ABS(a[i+31:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_abs_pi32</span>
<span class="s1">FORCE_INLINE __m64 _mm_abs_pi32(__m64 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s32(vabs_s32(vreinterpret_s32_m64(a)));</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute value of packed signed 16-bit integers in a, and store</span>
<span class="s2">// the unsigned results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//     i := j*16</span>
<span class="s2">//     dst[i+15:i] := ABS(a[i+15:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_abs_pi16</span>
<span class="s1">FORCE_INLINE __m64 _mm_abs_pi16(__m64 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s16(vabs_s16(vreinterpret_s16_m64(a)));</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute value of packed signed 8-bit integers in a, and store</span>
<span class="s2">// the unsigned results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//     i := j*8</span>
<span class="s2">//     dst[i+7:i] := ABS(a[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_abs_pi8</span>
<span class="s1">FORCE_INLINE __m64 _mm_abs_pi8(__m64 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s8(vabs_s8(vreinterpret_s8_m64(a)));</span>
<span class="s1">}</span>

<span class="s2">// Takes the upper 64 bits of a and places it in the low end of the result</span>
<span class="s2">// Takes the lower 64 bits of b and places it into the high end of the result.</span>
<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_1032(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a32, b10));</span>
<span class="s1">}</span>

<span class="s2">// takes the lower two 32-bit values from a and swaps them and places in high</span>
<span class="s2">// end of result takes the higher two 32 bit values from b and swaps them and</span>
<span class="s2">// places in low end of result.</span>
<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_2301(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s1">float32x2_t b23 = vrev64_f32(vget_high_f32(vreinterpretq_f32_m128(b)));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a01, b23));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_0321(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a21 = vget_high_f32(</span>
        <span class="s1">vextq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), </span><span class="s3">3</span><span class="s1">));</span>
    <span class="s1">float32x2_t b03 = vget_low_f32(</span>
        <span class="s1">vextq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b), </span><span class="s3">3</span><span class="s1">));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a21, b03));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_2103(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a03 = vget_low_f32(</span>
        <span class="s1">vextq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), </span><span class="s3">3</span><span class="s1">));</span>
    <span class="s1">float32x2_t b21 = vget_high_f32(</span>
        <span class="s1">vextq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b), </span><span class="s3">3</span><span class="s1">));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a03, b21));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_1010(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a10, b10));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_1001(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s1">float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a01, b10));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_0101(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s1">float32x2_t b01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(b)));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a01, b01));</span>
<span class="s1">}</span>

<span class="s2">// keeps the low 64 bits of b in the low and puts the high 64 bits of a in the</span>
<span class="s2">// high</span>
<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_3210(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a10, b32));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_0011(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a11 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(a)), </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s1">float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a11, b00));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_0022(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a22 =</span>
        <span class="s1">vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a22, b00));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_2200(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(a)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32x2_t b22 =</span>
        <span class="s1">vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(b)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a00, b22));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_3202(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32_t a0 = vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32x2_t a22 =</span>
        <span class="s1">vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32x2_t a02 = vset_lane_f32(a0, a22, </span><span class="s3">1</span><span class="s1">); </span><span class="s2">/* TODO: use vzip ?*/</span>
    <span class="s1">float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a02, b32));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_1133(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a33 =</span>
        <span class="s1">vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s1">float32x2_t b11 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a33, b11));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_2010(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32_t b2 = vgetq_lane_f32(vreinterpretq_f32_m128(b), </span><span class="s3">2</span><span class="s1">);</span>
    <span class="s1">float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32x2_t b20 = vset_lane_f32(b2, b00, </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a10, b20));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_2001(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s1">float32_t b2 = vgetq_lane_f32(b, </span><span class="s3">2</span><span class="s1">);</span>
    <span class="s1">float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32x2_t b20 = vset_lane_f32(b2, b00, </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a01, b20));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128 _mm_shuffle_ps_2032(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32_t b2 = vgetq_lane_f32(b, </span><span class="s3">2</span><span class="s1">);</span>
    <span class="s1">float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32x2_t b20 = vset_lane_f32(b2, b00, </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(a32, b20));</span>
<span class="s1">}</span>

<span class="s2">// NEON does not support a general purpose permute intrinsic</span>
<span class="s2">// Selects four specific single-precision, floating-point values from a and b,</span>
<span class="s2">// based on the mask i.</span>
<span class="s2">//</span>
<span class="s2">// C equivalent:</span>
<span class="s2">//   __m128 _mm_shuffle_ps_default(__m128 a, __m128 b,</span>
<span class="s2">//                                 __constrange(0, 255) int imm) {</span>
<span class="s2">//       __m128 ret;</span>
<span class="s2">//       ret[0] = a[imm        &amp; 0x3];   ret[1] = a[(imm &gt;&gt; 2) &amp; 0x3];</span>
<span class="s2">//       ret[2] = b[(imm &gt;&gt; 4) &amp; 0x03];  ret[3] = b[(imm &gt;&gt; 6) &amp; 0x03];</span>
<span class="s2">//       return ret;</span>
<span class="s2">//   }</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/5f0858x0(v=vs.100).aspx</span>
<span class="s0">#define </span><span class="s1">_mm_shuffle_ps_default(a, b, imm)                                  \</span>
    <span class="s1">__extension__({                                                        \</span>
        <span class="s1">float32x4_t ret;                                                   \</span>
        <span class="s1">ret = vmovq_n_f32(                                                 \</span>
            <span class="s1">vgetq_lane_f32(vreinterpretq_f32_m128(a), (imm) &amp; (</span><span class="s3">0x3</span><span class="s1">)));     \</span>
        <span class="s1">ret = vsetq_lane_f32(                                              \</span>
            <span class="s1">vgetq_lane_f32(vreinterpretq_f32_m128(a), ((imm) &gt;&gt; </span><span class="s3">2</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), \</span>
            <span class="s1">ret, </span><span class="s3">1</span><span class="s1">);                                                       \</span>
        <span class="s1">ret = vsetq_lane_f32(                                              \</span>
            <span class="s1">vgetq_lane_f32(vreinterpretq_f32_m128(b), ((imm) &gt;&gt; </span><span class="s3">4</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), \</span>
            <span class="s1">ret, </span><span class="s3">2</span><span class="s1">);                                                       \</span>
        <span class="s1">ret = vsetq_lane_f32(                                              \</span>
            <span class="s1">vgetq_lane_f32(vreinterpretq_f32_m128(b), ((imm) &gt;&gt; </span><span class="s3">6</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), \</span>
            <span class="s1">ret, </span><span class="s3">3</span><span class="s1">);                                                       \</span>
        <span class="s1">vreinterpretq_m128_f32(ret);                                       \</span>
    <span class="s1">})</span>

<span class="s2">// FORCE_INLINE __m128 _mm_shuffle_ps(__m128 a, __m128 b, __constrange(0,255)</span>
<span class="s2">// int imm)</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_shufflevector)</span>
<span class="s0">#define </span><span class="s1">_mm_shuffle_ps(a, b, imm)                                \</span>
    <span class="s1">__extension__({                                              \</span>
        <span class="s1">float32x4_t _input1 = vreinterpretq_f32_m128(a);         \</span>
        <span class="s1">float32x4_t _input2 = vreinterpretq_f32_m128(b);         \</span>
        <span class="s1">float32x4_t _shuf = __builtin_shufflevector(             \</span>
            <span class="s1">_input1, _input2, (imm) &amp; (</span><span class="s3">0x3</span><span class="s1">), ((imm) &gt;&gt; </span><span class="s3">2</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">, \</span>
            <span class="s1">(((imm) &gt;&gt; </span><span class="s3">4</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">) + </span><span class="s3">4</span><span class="s1">, (((imm) &gt;&gt; </span><span class="s3">6</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">) + </span><span class="s3">4</span><span class="s1">); \</span>
        <span class="s1">vreinterpretq_m128_f32(_shuf);                           \</span>
    <span class="s1">})</span>
<span class="s0">#else  </span><span class="s2">// generic</span>
<span class="s0">#define </span><span class="s1">_mm_shuffle_ps(a, b, imm)                          \</span>
    <span class="s1">__extension__({                                        \</span>
        <span class="s1">__m128 ret;                                        \</span>
        <span class="s0">switch </span><span class="s1">(imm) {                                     \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_1032((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_2301((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_0321((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">3</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_2103((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_movelh_ps((a), (b));                 \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_1001((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_0101((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_3210((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_0011((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_0022((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_2200((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_3202((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_movehl_ps((b), (a));                 \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">1</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">3</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_1133((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_2010((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_2001((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                      \</span>
            <span class="s1">ret = _mm_shuffle_ps_2032((a), (b));           \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s0">default</span><span class="s1">:                                           \</span>
            <span class="s1">ret = _mm_shuffle_ps_default((a), (b), (imm)); \</span>
            <span class="s0">break</span><span class="s1">;                                         \</span>
        <span class="s1">}                                                  \</span>
        <span class="s1">ret;                                               \</span>
    <span class="s1">})</span>
<span class="s0">#endif</span>

<span class="s2">// Takes the upper 64 bits of a and places it in the low end of the result</span>
<span class="s2">// Takes the lower 64 bits of a and places it into the high end of the result.</span>
<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_1032(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a32 = vget_high_s32(vreinterpretq_s32_m128i(a));</span>
    <span class="s1">int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(a32, a10));</span>
<span class="s1">}</span>

<span class="s2">// takes the lower two 32-bit values from a and swaps them and places in low end</span>
<span class="s2">// of result takes the higher two 32 bit values from a and swaps them and places</span>
<span class="s2">// in high end of result.</span>
<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_2301(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));</span>
    <span class="s1">int32x2_t a23 = vrev64_s32(vget_high_s32(vreinterpretq_s32_m128i(a)));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(a01, a23));</span>
<span class="s1">}</span>

<span class="s2">// rotates the least significant 32 bits into the most signficant 32 bits, and</span>
<span class="s2">// shifts the rest down</span>
<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_0321(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vextq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(a), </span><span class="s3">1</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// rotates the most significant 32 bits into the least signficant 32 bits, and</span>
<span class="s2">// shifts the rest up</span>
<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_2103(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vextq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(a), </span><span class="s3">3</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// gets the lower 64 bits of a, and places it in the upper 64 bits</span>
<span class="s2">// gets the lower 64 bits of a and places it in the lower 64 bits</span>
<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_1010(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(a10, a10));</span>
<span class="s1">}</span>

<span class="s2">// gets the lower 64 bits of a, swaps the 0 and 1 elements, and places it in the</span>
<span class="s2">// lower 64 bits gets the lower 64 bits of a, and places it in the upper 64 bits</span>
<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_1001(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));</span>
    <span class="s1">int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(a01, a10));</span>
<span class="s1">}</span>

<span class="s2">// gets the lower 64 bits of a, swaps the 0 and 1 elements and places it in the</span>
<span class="s2">// upper 64 bits gets the lower 64 bits of a, swaps the 0 and 1 elements, and</span>
<span class="s2">// places it in the lower 64 bits</span>
<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_0101(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(a01, a01));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_2211(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a11 = vdup_lane_s32(vget_low_s32(vreinterpretq_s32_m128i(a)), </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s1">int32x2_t a22 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(a11, a22));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_0122(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a22 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(a22, a01));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi_3332(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a32 = vget_high_s32(vreinterpretq_s32_m128i(a));</span>
    <span class="s1">int32x2_t a33 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(a32, a33));</span>
<span class="s1">}</span>

<span class="s2">// Shuffle packed 8-bit integers in a according to shuffle control mask in the</span>
<span class="s2">// corresponding 8-bit element of b, and store the results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_shuffle_epi8</span>
<span class="s1">FORCE_INLINE __m128i _mm_shuffle_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">int8x16_t tbl = vreinterpretq_s8_m128i(a);   </span><span class="s2">// input a</span>
    <span class="s1">uint8x16_t idx = vreinterpretq_u8_m128i(b);  </span><span class="s2">// input b</span>
    <span class="s1">uint8x16_t idx_masked =</span>
        <span class="s1">vandq_u8(idx, vdupq_n_u8(</span><span class="s3">0x8F</span><span class="s1">));  </span><span class="s2">// avoid using meaningless bits</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(vqtbl1q_s8(tbl, idx_masked));</span>
<span class="s1">#elif defined(__GNUC__)</span>
    <span class="s1">int8x16_t ret;</span>
    <span class="s2">// %e and %f represent the even and odd D registers</span>
    <span class="s2">// respectively.</span>
    <span class="s1">__asm__ __volatile__(</span>
        <span class="s4">&quot;vtbl.8  %e[ret], {%e[tbl], %f[tbl]}, %e[idx]</span><span class="s7">\n</span><span class="s4">&quot;</span>
        <span class="s4">&quot;vtbl.8  %f[ret], {%e[tbl], %f[tbl]}, %f[idx]</span><span class="s7">\n</span><span class="s4">&quot;</span>
        <span class="s1">: [ret] </span><span class="s4">&quot;=&amp;w&quot;</span><span class="s1">(ret)</span>
        <span class="s1">: [tbl] </span><span class="s4">&quot;w&quot;</span><span class="s1">(tbl), [idx] </span><span class="s4">&quot;w&quot;</span><span class="s1">(idx_masked));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(ret);</span>
<span class="s0">#else</span>
    <span class="s2">// use this line if testing on aarch64</span>
    <span class="s1">int8x8x2_t a_split = {vget_low_s8(tbl), vget_high_s8(tbl)};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vcombine_s8(vtbl2_s8(a_split, vget_low_u8(idx_masked)),</span>
                    <span class="s1">vtbl2_s8(a_split, vget_high_u8(idx_masked))));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// C equivalent:</span>
<span class="s2">//   __m128i _mm_shuffle_epi32_default(__m128i a,</span>
<span class="s2">//                                     __constrange(0, 255) int imm) {</span>
<span class="s2">//       __m128i ret;</span>
<span class="s2">//       ret[0] = a[imm        &amp; 0x3];   ret[1] = a[(imm &gt;&gt; 2) &amp; 0x3];</span>
<span class="s2">//       ret[2] = a[(imm &gt;&gt; 4) &amp; 0x03];  ret[3] = a[(imm &gt;&gt; 6) &amp; 0x03];</span>
<span class="s2">//       return ret;</span>
<span class="s2">//   }</span>
<span class="s0">#define </span><span class="s1">_mm_shuffle_epi32_default(a, imm)                                   \</span>
    <span class="s1">__extension__({                                                         \</span>
        <span class="s1">int32x4_t ret;                                                      \</span>
        <span class="s1">ret = vmovq_n_s32(                                                  \</span>
            <span class="s1">vgetq_lane_s32(vreinterpretq_s32_m128i(a), (imm) &amp; (</span><span class="s3">0x3</span><span class="s1">)));     \</span>
        <span class="s1">ret = vsetq_lane_s32(                                               \</span>
            <span class="s1">vgetq_lane_s32(vreinterpretq_s32_m128i(a), ((imm) &gt;&gt; </span><span class="s3">2</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), \</span>
            <span class="s1">ret, </span><span class="s3">1</span><span class="s1">);                                                        \</span>
        <span class="s1">ret = vsetq_lane_s32(                                               \</span>
            <span class="s1">vgetq_lane_s32(vreinterpretq_s32_m128i(a), ((imm) &gt;&gt; </span><span class="s3">4</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), \</span>
            <span class="s1">ret, </span><span class="s3">2</span><span class="s1">);                                                        \</span>
        <span class="s1">ret = vsetq_lane_s32(                                               \</span>
            <span class="s1">vgetq_lane_s32(vreinterpretq_s32_m128i(a), ((imm) &gt;&gt; </span><span class="s3">6</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), \</span>
            <span class="s1">ret, </span><span class="s3">3</span><span class="s1">);                                                        \</span>
        <span class="s1">vreinterpretq_m128i_s32(ret);                                       \</span>
    <span class="s1">})</span>

<span class="s2">// FORCE_INLINE __m128i _mm_shuffle_epi32_splat(__m128i a, __constrange(0,255)</span>
<span class="s2">// int imm)</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
<span class="s0">#define </span><span class="s1">_mm_shuffle_epi32_splat(a, imm)                          \</span>
    <span class="s1">__extension__({                                              \</span>
        <span class="s1">vreinterpretq_m128i_s32(                                 \</span>
            <span class="s1">vdupq_laneq_s32(vreinterpretq_s32_m128i(a), (imm))); \</span>
    <span class="s1">})</span>
<span class="s0">#else</span>
<span class="s0">#define </span><span class="s1">_mm_shuffle_epi32_splat(a, imm)                                      \</span>
    <span class="s1">__extension__({                                                          \</span>
        <span class="s1">vreinterpretq_m128i_s32(                                             \</span>
            <span class="s1">vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_m128i(a), (imm)))); \</span>
    <span class="s1">})</span>
<span class="s0">#endif</span>

<span class="s2">// Shuffles the 4 signed or unsigned 32-bit integers in a as specified by imm.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/56f67xbk%28v=vs.90%29.aspx</span>
<span class="s2">// FORCE_INLINE __m128i _mm_shuffle_epi32(__m128i a,</span>
<span class="s2">//                                        __constrange(0,255) int imm)</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_shufflevector)</span>
<span class="s0">#define </span><span class="s1">_mm_shuffle_epi32(a, imm)                              \</span>
    <span class="s1">__extension__({                                            \</span>
        <span class="s1">int32x4_t _input = vreinterpretq_s32_m128i(a);         \</span>
        <span class="s1">int32x4_t _shuf = __builtin_shufflevector(             \</span>
            <span class="s1">_input, _input, (imm) &amp; (</span><span class="s3">0x3</span><span class="s1">), ((imm) &gt;&gt; </span><span class="s3">2</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">, \</span>
            <span class="s1">((imm) &gt;&gt; </span><span class="s3">4</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">, ((imm) &gt;&gt; </span><span class="s3">6</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">);           \</span>
        <span class="s1">vreinterpretq_m128i_s32(_shuf);                        \</span>
    <span class="s1">})</span>
<span class="s0">#else  </span><span class="s2">// generic</span>
<span class="s0">#define </span><span class="s1">_mm_shuffle_epi32(a, imm)                        \</span>
    <span class="s1">__extension__({                                      \</span>
        <span class="s1">__m128i ret;                                     \</span>
        <span class="s0">switch </span><span class="s1">(imm) {                                   \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_1032((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_2301((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_0321((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">3</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_2103((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_1010((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_1001((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_0101((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_2211((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_0122((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">3</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi_3332((a));             \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi32_splat((a), </span><span class="s3">0</span><span class="s1">);       \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">1</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">1</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi32_splat((a), </span><span class="s3">1</span><span class="s1">);       \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">2</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">2</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi32_splat((a), </span><span class="s3">2</span><span class="s1">);       \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">case </span><span class="s1">_MM_SHUFFLE(</span><span class="s3">3</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, </span><span class="s3">3</span><span class="s1">):                    \</span>
            <span class="s1">ret = _mm_shuffle_epi32_splat((a), </span><span class="s3">3</span><span class="s1">);       \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s0">default</span><span class="s1">:                                         \</span>
            <span class="s1">ret = _mm_shuffle_epi32_default((a), (imm)); \</span>
            <span class="s0">break</span><span class="s1">;                                       \</span>
        <span class="s1">}                                                \</span>
        <span class="s1">ret;                                             \</span>
    <span class="s1">})</span>
<span class="s0">#endif</span>

<span class="s2">// Shuffles the lower 4 signed or unsigned 16-bit integers in a as specified</span>
<span class="s2">// by imm.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/y41dkk37(v=vs.100)</span>
<span class="s2">// FORCE_INLINE __m128i _mm_shufflelo_epi16_function(__m128i a,</span>
<span class="s2">//                                                   __constrange(0,255) int</span>
<span class="s2">//                                                   imm)</span>
<span class="s0">#define </span><span class="s1">_mm_shufflelo_epi16_function(a, imm)                                  \</span>
    <span class="s1">__extension__({                                                           \</span>
        <span class="s1">int16x8_t ret = vreinterpretq_s16_m128i(a);                           \</span>
        <span class="s1">int16x4_t lowBits = vget_low_s16(ret);                                \</span>
        <span class="s1">ret = vsetq_lane_s16(vget_lane_s16(lowBits, (imm) &amp; (</span><span class="s3">0x3</span><span class="s1">)), ret, </span><span class="s3">0</span><span class="s1">);  \</span>
        <span class="s1">ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) &gt;&gt; </span><span class="s3">2</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), ret, \</span>
                             <span class="s3">1</span><span class="s1">);                                              \</span>
        <span class="s1">ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) &gt;&gt; </span><span class="s3">4</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), ret, \</span>
                             <span class="s3">2</span><span class="s1">);                                              \</span>
        <span class="s1">ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) &gt;&gt; </span><span class="s3">6</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), ret, \</span>
                             <span class="s3">3</span><span class="s1">);                                              \</span>
        <span class="s1">vreinterpretq_m128i_s16(ret);                                         \</span>
    <span class="s1">})</span>

<span class="s2">// FORCE_INLINE __m128i _mm_shufflelo_epi16(__m128i a,</span>
<span class="s2">//                                          __constrange(0,255) int imm)</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_shufflevector)</span>
<span class="s0">#define </span><span class="s1">_mm_shufflelo_epi16(a, imm)                                  \</span>
    <span class="s1">__extension__({                                                  \</span>
        <span class="s1">int16x8_t _input = vreinterpretq_s16_m128i(a);               \</span>
        <span class="s1">int16x8_t _shuf = __builtin_shufflevector(                   \</span>
            <span class="s1">_input, _input, ((imm) &amp; (</span><span class="s3">0x3</span><span class="s1">)), (((imm) &gt;&gt; </span><span class="s3">2</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">),   \</span>
            <span class="s1">(((imm) &gt;&gt; </span><span class="s3">4</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), (((imm) &gt;&gt; </span><span class="s3">6</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), </span><span class="s3">4</span><span class="s1">, </span><span class="s3">5</span><span class="s1">, </span><span class="s3">6</span><span class="s1">, </span><span class="s3">7</span><span class="s1">); \</span>
        <span class="s1">vreinterpretq_m128i_s16(_shuf);                              \</span>
    <span class="s1">})</span>
<span class="s0">#else  </span><span class="s2">// generic</span>
<span class="s0">#define </span><span class="s1">_mm_shufflelo_epi16(a, imm) _mm_shufflelo_epi16_function((a), (imm))</span>
<span class="s0">#endif</span>

<span class="s2">// Shuffles the upper 4 signed or unsigned 16-bit integers in a as specified</span>
<span class="s2">// by imm.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/13ywktbs(v=vs.100).aspx</span>
<span class="s2">// FORCE_INLINE __m128i _mm_shufflehi_epi16_function(__m128i a,</span>
<span class="s2">//                                                   __constrange(0,255) int</span>
<span class="s2">//                                                   imm)</span>
<span class="s0">#define </span><span class="s1">_mm_shufflehi_epi16_function(a, imm)                                   \</span>
    <span class="s1">__extension__({                                                            \</span>
        <span class="s1">int16x8_t ret = vreinterpretq_s16_m128i(a);                            \</span>
        <span class="s1">int16x4_t highBits = vget_high_s16(ret);                               \</span>
        <span class="s1">ret = vsetq_lane_s16(vget_lane_s16(highBits, (imm) &amp; (</span><span class="s3">0x3</span><span class="s1">)), ret, </span><span class="s3">4</span><span class="s1">);  \</span>
        <span class="s1">ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) &gt;&gt; </span><span class="s3">2</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), ret, \</span>
                             <span class="s3">5</span><span class="s1">);                                               \</span>
        <span class="s1">ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) &gt;&gt; </span><span class="s3">4</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), ret, \</span>
                             <span class="s3">6</span><span class="s1">);                                               \</span>
        <span class="s1">ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) &gt;&gt; </span><span class="s3">6</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">), ret, \</span>
                             <span class="s3">7</span><span class="s1">);                                               \</span>
        <span class="s1">vreinterpretq_m128i_s16(ret);                                          \</span>
    <span class="s1">})</span>

<span class="s2">// FORCE_INLINE __m128i _mm_shufflehi_epi16(__m128i a,</span>
<span class="s2">//                                          __constrange(0,255) int imm)</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_shufflevector)</span>
<span class="s0">#define </span><span class="s1">_mm_shufflehi_epi16(a, imm)                             \</span>
    <span class="s1">__extension__({                                             \</span>
        <span class="s1">int16x8_t _input = vreinterpretq_s16_m128i(a);          \</span>
        <span class="s1">int16x8_t _shuf = __builtin_shufflevector(              \</span>
            <span class="s1">_input, _input, </span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">3</span><span class="s1">, ((imm) &amp; (</span><span class="s3">0x3</span><span class="s1">)) + </span><span class="s3">4</span><span class="s1">,    \</span>
            <span class="s1">(((imm) &gt;&gt; </span><span class="s3">2</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">) + </span><span class="s3">4</span><span class="s1">, (((imm) &gt;&gt; </span><span class="s3">4</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">) + </span><span class="s3">4</span><span class="s1">, \</span>
            <span class="s1">(((imm) &gt;&gt; </span><span class="s3">6</span><span class="s1">) &amp; </span><span class="s3">0x3</span><span class="s1">) + </span><span class="s3">4</span><span class="s1">);                          \</span>
        <span class="s1">vreinterpretq_m128i_s16(_shuf);                         \</span>
    <span class="s1">})</span>
<span class="s0">#else  </span><span class="s2">// generic</span>
<span class="s0">#define </span><span class="s1">_mm_shufflehi_epi16(a, imm) _mm_shufflehi_epi16_function((a), (imm))</span>
<span class="s0">#endif</span>

<span class="s2">// Blend packed 16-bit integers from a and b using control mask imm8, and store</span>
<span class="s2">// the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//       i := j*16</span>
<span class="s2">//       IF imm8[j]</span>
<span class="s2">//           dst[i+15:i] := b[i+15:i]</span>
<span class="s2">//       ELSE</span>
<span class="s2">//           dst[i+15:i] := a[i+15:i]</span>
<span class="s2">//       FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">// FORCE_INLINE __m128i _mm_blend_epi16(__m128i a, __m128i b,</span>
<span class="s2">//                                      __constrange(0,255) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_blend_epi16(a, b, imm)                                        \</span>
    <span class="s1">__extension__({                                                       \</span>
        <span class="s0">const </span><span class="s1">uint16_t _mask[</span><span class="s3">8</span><span class="s1">] = {((imm) &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">0</span><span class="s1">)) ? </span><span class="s3">0xFFFF </span><span class="s1">: </span><span class="s3">0x0000</span><span class="s1">,  \</span>
                                   <span class="s1">((imm) &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">1</span><span class="s1">)) ? </span><span class="s3">0xFFFF </span><span class="s1">: </span><span class="s3">0x0000</span><span class="s1">,  \</span>
                                   <span class="s1">((imm) &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">2</span><span class="s1">)) ? </span><span class="s3">0xFFFF </span><span class="s1">: </span><span class="s3">0x0000</span><span class="s1">,  \</span>
                                   <span class="s1">((imm) &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">3</span><span class="s1">)) ? </span><span class="s3">0xFFFF </span><span class="s1">: </span><span class="s3">0x0000</span><span class="s1">,  \</span>
                                   <span class="s1">((imm) &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">4</span><span class="s1">)) ? </span><span class="s3">0xFFFF </span><span class="s1">: </span><span class="s3">0x0000</span><span class="s1">,  \</span>
                                   <span class="s1">((imm) &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">5</span><span class="s1">)) ? </span><span class="s3">0xFFFF </span><span class="s1">: </span><span class="s3">0x0000</span><span class="s1">,  \</span>
                                   <span class="s1">((imm) &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">6</span><span class="s1">)) ? </span><span class="s3">0xFFFF </span><span class="s1">: </span><span class="s3">0x0000</span><span class="s1">,  \</span>
                                   <span class="s1">((imm) &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">7</span><span class="s1">)) ? </span><span class="s3">0xFFFF </span><span class="s1">: </span><span class="s3">0x0000</span><span class="s1">}; \</span>
        <span class="s1">uint16x8_t _mask_vec = vld1q_u16(_mask);                          \</span>
        <span class="s1">uint16x8_t _a = vreinterpretq_u16_m128i(a);                       \</span>
        <span class="s1">uint16x8_t _b = vreinterpretq_u16_m128i(b);                       \</span>
        <span class="s1">vreinterpretq_m128i_u16(vbslq_u16(_mask_vec, _b, _a));            \</span>
    <span class="s1">})</span>

<span class="s2">// Blend packed 8-bit integers from a and b using mask, and store the results in</span>
<span class="s2">// dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 15</span>
<span class="s2">//       i := j*8</span>
<span class="s2">//       IF mask[i+7]</span>
<span class="s2">//           dst[i+7:i] := b[i+7:i]</span>
<span class="s2">//       ELSE</span>
<span class="s2">//           dst[i+7:i] := a[i+7:i]</span>
<span class="s2">//       FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s1">FORCE_INLINE __m128i _mm_blendv_epi8(__m128i _a, __m128i _b, __m128i _mask)</span>
<span class="s1">{</span>
    <span class="s2">// Use a signed shift right to create a mask with the sign bit</span>
    <span class="s1">uint8x16_t mask =</span>
        <span class="s1">vreinterpretq_u8_s8(vshrq_n_s8(vreinterpretq_s8_m128i(_mask), </span><span class="s3">7</span><span class="s1">));</span>
    <span class="s1">uint8x16_t a = vreinterpretq_u8_m128i(_a);</span>
    <span class="s1">uint8x16_t b = vreinterpretq_u8_m128i(_b);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(vbslq_u8(mask, b, a));</span>
<span class="s1">}</span>

<span class="s2">/* Shifts */</span>


<span class="s2">// Shift packed 16-bit integers in a right by imm while shifting in sign</span>
<span class="s2">// bits, and store the results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srai_epi16</span>
<span class="s1">FORCE_INLINE __m128i _mm_srai_epi16(__m128i a, </span><span class="s0">int </span><span class="s1">imm)</span>
<span class="s1">{</span>
    <span class="s0">const int </span><span class="s1">count = (imm &amp; ~</span><span class="s3">15</span><span class="s1">) ? </span><span class="s3">15 </span><span class="s1">: imm;</span>
    <span class="s0">return </span><span class="s1">(__m128i) vshlq_s16((int16x8_t) a, vdupq_n_s16(-count));</span>
<span class="s1">}</span>

<span class="s2">// Shifts the 8 signed or unsigned 16-bit integers in a left by count bits while</span>
<span class="s2">// shifting in zeros.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 &lt;&lt; count</span>
<span class="s2">//   r1 := a1 &lt;&lt; count</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := a7 &lt;&lt; count</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/es73bcsy(v=vs.90).aspx</span>
<span class="s0">#define </span><span class="s1">_mm_slli_epi16(a, imm)                                   \</span>
    <span class="s1">__extension__({                                              \</span>
        <span class="s1">__m128i ret;                                             \</span>
        <span class="s0">if </span><span class="s1">((imm) &lt;= </span><span class="s3">0</span><span class="s1">) {                                        \</span>
            <span class="s1">ret = a;                                             \</span>
        <span class="s1">} </span><span class="s0">else if </span><span class="s1">((imm) &gt; </span><span class="s3">15</span><span class="s1">) {                                 \</span>
            <span class="s1">ret = _mm_setzero_si128();                           \</span>
        <span class="s1">} </span><span class="s0">else </span><span class="s1">{                                                 \</span>
            <span class="s1">ret = vreinterpretq_m128i_s16(                       \</span>
                <span class="s1">vshlq_n_s16(vreinterpretq_s16_m128i(a), (imm))); \</span>
        <span class="s1">}                                                        \</span>
        <span class="s1">ret;                                                     \</span>
    <span class="s1">})</span>

<span class="s2">// Shifts the 4 signed or unsigned 32-bit integers in a left by count bits while</span>
<span class="s2">// shifting in zeros. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/z2k3bbtb%28v=vs.90%29.aspx</span>
<span class="s2">// FORCE_INLINE __m128i _mm_slli_epi32(__m128i a, __constrange(0,255) int imm)</span>
<span class="s1">FORCE_INLINE __m128i _mm_slli_epi32(__m128i a, </span><span class="s0">int </span><span class="s1">imm)</span>
<span class="s1">{</span>
    <span class="s0">if </span><span class="s1">(imm &lt;= </span><span class="s3">0</span><span class="s1">) </span><span class="s2">/* TODO: add constant range macro: [0, 255] */</span>
        <span class="s0">return </span><span class="s1">a;</span>
    <span class="s0">if </span><span class="s1">(imm &gt; </span><span class="s3">31</span><span class="s1">) </span><span class="s2">/* TODO: add unlikely macro */</span>
        <span class="s0">return </span><span class="s1">_mm_setzero_si128();</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vshlq_s32(vreinterpretq_s32_m128i(a), vdupq_n_s32(imm)));</span>
<span class="s1">}</span>

<span class="s2">// Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and</span>
<span class="s2">// store the results in dst.</span>
<span class="s1">FORCE_INLINE __m128i _mm_slli_epi64(__m128i a, </span><span class="s0">int </span><span class="s1">imm)</span>
<span class="s1">{</span>
    <span class="s0">if </span><span class="s1">(imm &lt;= </span><span class="s3">0</span><span class="s1">) </span><span class="s2">/* TODO: add constant range macro: [0, 255] */</span>
        <span class="s0">return </span><span class="s1">a;</span>
    <span class="s0">if </span><span class="s1">(imm &gt; </span><span class="s3">63</span><span class="s1">) </span><span class="s2">/* TODO: add unlikely macro */</span>
        <span class="s0">return </span><span class="s1">_mm_setzero_si128();</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(</span>
        <span class="s1">vshlq_s64(vreinterpretq_s64_m128i(a), vdupq_n_s64(imm)));</span>
<span class="s1">}</span>

<span class="s2">// Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and</span>
<span class="s2">// store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//     i := j*16</span>
<span class="s2">//     IF imm8[7:0] &gt; 15</span>
<span class="s2">//       dst[i+15:i] := 0</span>
<span class="s2">//     ELSE</span>
<span class="s2">//       dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; imm8[7:0])</span>
<span class="s2">//     FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srli_epi16</span>
<span class="s0">#define </span><span class="s1">_mm_srli_epi16(a, imm)                                             \</span>
    <span class="s1">__extension__({                                                        \</span>
        <span class="s1">__m128i ret;                                                       \</span>
        <span class="s0">if </span><span class="s1">((imm) == </span><span class="s3">0</span><span class="s1">) {                                                  \</span>
            <span class="s1">ret = a;                                                       \</span>
        <span class="s1">} </span><span class="s0">else if </span><span class="s1">(</span><span class="s3">0 </span><span class="s1">&lt; (imm) &amp;&amp; (imm) &lt; </span><span class="s3">16</span><span class="s1">) {                              \</span>
            <span class="s1">ret = vreinterpretq_m128i_u16(                                 \</span>
                <span class="s1">vshlq_u16(vreinterpretq_u16_m128i(a), vdupq_n_s16(-imm))); \</span>
        <span class="s1">} </span><span class="s0">else </span><span class="s1">{                                                           \</span>
            <span class="s1">ret = _mm_setzero_si128();                                     \</span>
        <span class="s1">}                                                                  \</span>
        <span class="s1">ret;                                                               \</span>
    <span class="s1">})</span>

<span class="s2">// Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and</span>
<span class="s2">// store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//     i := j*32</span>
<span class="s2">//     IF imm8[7:0] &gt; 31</span>
<span class="s2">//       dst[i+31:i] := 0</span>
<span class="s2">//     ELSE</span>
<span class="s2">//       dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; imm8[7:0])</span>
<span class="s2">//     FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srli_epi32</span>
<span class="s2">// FORCE_INLINE __m128i _mm_srli_epi32(__m128i a, __constrange(0,255) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_srli_epi32(a, imm)                                             \</span>
    <span class="s1">__extension__({                                                        \</span>
        <span class="s1">__m128i ret;                                                       \</span>
        <span class="s0">if </span><span class="s1">((imm) == </span><span class="s3">0</span><span class="s1">) {                                                  \</span>
            <span class="s1">ret = a;                                                       \</span>
        <span class="s1">} </span><span class="s0">else if </span><span class="s1">(</span><span class="s3">0 </span><span class="s1">&lt; (imm) &amp;&amp; (imm) &lt; </span><span class="s3">32</span><span class="s1">) {                              \</span>
            <span class="s1">ret = vreinterpretq_m128i_u32(                                 \</span>
                <span class="s1">vshlq_u32(vreinterpretq_u32_m128i(a), vdupq_n_s32(-imm))); \</span>
        <span class="s1">} </span><span class="s0">else </span><span class="s1">{                                                           \</span>
            <span class="s1">ret = _mm_setzero_si128();                                     \</span>
        <span class="s1">}                                                                  \</span>
        <span class="s1">ret;                                                               \</span>
    <span class="s1">})</span>

<span class="s2">// Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and</span>
<span class="s2">// store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//     i := j*64</span>
<span class="s2">//     IF imm8[7:0] &gt; 63</span>
<span class="s2">//       dst[i+63:i] := 0</span>
<span class="s2">//     ELSE</span>
<span class="s2">//       dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; imm8[7:0])</span>
<span class="s2">//     FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srli_epi64</span>
<span class="s0">#define </span><span class="s1">_mm_srli_epi64(a, imm)                                             \</span>
    <span class="s1">__extension__({                                                        \</span>
        <span class="s1">__m128i ret;                                                       \</span>
        <span class="s0">if </span><span class="s1">((imm) == </span><span class="s3">0</span><span class="s1">) {                                                  \</span>
            <span class="s1">ret = a;                                                       \</span>
        <span class="s1">} </span><span class="s0">else if </span><span class="s1">(</span><span class="s3">0 </span><span class="s1">&lt; (imm) &amp;&amp; (imm) &lt; </span><span class="s3">64</span><span class="s1">) {                              \</span>
            <span class="s1">ret = vreinterpretq_m128i_u64(                                 \</span>
                <span class="s1">vshlq_u64(vreinterpretq_u64_m128i(a), vdupq_n_s64(-imm))); \</span>
        <span class="s1">} </span><span class="s0">else </span><span class="s1">{                                                           \</span>
            <span class="s1">ret = _mm_setzero_si128();                                     \</span>
        <span class="s1">}                                                                  \</span>
        <span class="s1">ret;                                                               \</span>
    <span class="s1">})</span>

<span class="s2">// Shift packed 32-bit integers in a right by imm8 while shifting in sign bits,</span>
<span class="s2">// and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//     i := j*32</span>
<span class="s2">//     IF imm8[7:0] &gt; 31</span>
<span class="s2">//       dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)</span>
<span class="s2">//     ELSE</span>
<span class="s2">//       dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; imm8[7:0])</span>
<span class="s2">//     FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_srai_epi32</span>
<span class="s2">// FORCE_INLINE __m128i _mm_srai_epi32(__m128i a, __constrange(0,255) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_srai_epi32(a, imm)                                             \</span>
    <span class="s1">__extension__({                                                        \</span>
        <span class="s1">__m128i ret;                                                       \</span>
        <span class="s0">if </span><span class="s1">((imm) == </span><span class="s3">0</span><span class="s1">) {                                                  \</span>
            <span class="s1">ret = a;                                                       \</span>
        <span class="s1">} </span><span class="s0">else if </span><span class="s1">(</span><span class="s3">0 </span><span class="s1">&lt; (imm) &amp;&amp; (imm) &lt; </span><span class="s3">32</span><span class="s1">) {                              \</span>
            <span class="s1">ret = vreinterpretq_m128i_s32(                                 \</span>
                <span class="s1">vshlq_s32(vreinterpretq_s32_m128i(a), vdupq_n_s32(-imm))); \</span>
        <span class="s1">} </span><span class="s0">else </span><span class="s1">{                                                           \</span>
            <span class="s1">ret = vreinterpretq_m128i_s32(                                 \</span>
                <span class="s1">vshrq_n_s32(vreinterpretq_s32_m128i(a), </span><span class="s3">31</span><span class="s1">));              \</span>
        <span class="s1">}                                                                  \</span>
        <span class="s1">ret;                                                               \</span>
    <span class="s1">})</span>

<span class="s2">// Shifts the 128 - bit value in a right by imm bytes while shifting in</span>
<span class="s2">// zeros.imm must be an immediate.</span>
<span class="s2">//</span>
<span class="s2">//   r := srl(a, imm*8)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/305w28yz(v=vs.100).aspx</span>
<span class="s2">// FORCE_INLINE _mm_srli_si128(__m128i a, __constrange(0,255) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_srli_si128(a, imm)                                              \</span>
    <span class="s1">__extension__({                                                         \</span>
        <span class="s1">__m128i ret;                                                        \</span>
        <span class="s0">if </span><span class="s1">((imm) &lt;= </span><span class="s3">0</span><span class="s1">) {                                                   \</span>
            <span class="s1">ret = a;                                                        \</span>
        <span class="s1">} </span><span class="s0">else if </span><span class="s1">((imm) &gt; </span><span class="s3">15</span><span class="s1">) {                                            \</span>
            <span class="s1">ret = _mm_setzero_si128();                                      \</span>
        <span class="s1">} </span><span class="s0">else </span><span class="s1">{                                                            \</span>
            <span class="s1">ret = vreinterpretq_m128i_s8(                                   \</span>
                <span class="s1">vextq_s8(vreinterpretq_s8_m128i(a), vdupq_n_s8(</span><span class="s3">0</span><span class="s1">), (imm))); \</span>
        <span class="s1">}                                                                   \</span>
        <span class="s1">ret;                                                                \</span>
    <span class="s1">})</span>

<span class="s2">// Shifts the 128-bit value in a left by imm bytes while shifting in zeros. imm</span>
<span class="s2">// must be an immediate.</span>
<span class="s2">//</span>
<span class="s2">//   r := a &lt;&lt; (imm * 8)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/34d3k2kt(v=vs.100).aspx</span>
<span class="s2">// FORCE_INLINE __m128i _mm_slli_si128(__m128i a, __constrange(0,255) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_slli_si128(a, imm)                                          \</span>
    <span class="s1">__extension__({                                                     \</span>
        <span class="s1">__m128i ret;                                                    \</span>
        <span class="s0">if </span><span class="s1">((imm) &lt;= </span><span class="s3">0</span><span class="s1">) {                                               \</span>
            <span class="s1">ret = a;                                                    \</span>
        <span class="s1">} </span><span class="s0">else if </span><span class="s1">((imm) &gt; </span><span class="s3">15</span><span class="s1">) {                                        \</span>
            <span class="s1">ret = _mm_setzero_si128();                                  \</span>
        <span class="s1">} </span><span class="s0">else </span><span class="s1">{                                                        \</span>
            <span class="s1">ret = vreinterpretq_m128i_s8(vextq_s8(                      \</span>
                <span class="s1">vdupq_n_s8(</span><span class="s3">0</span><span class="s1">), vreinterpretq_s8_m128i(a), </span><span class="s3">16 </span><span class="s1">- (imm))); \</span>
        <span class="s1">}                                                               \</span>
        <span class="s1">ret;                                                            \</span>
    <span class="s1">})</span>

<span class="s2">// Shifts the 8 signed or unsigned 16-bit integers in a left by count bits while</span>
<span class="s2">// shifting in zeros.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 &lt;&lt; count</span>
<span class="s2">//   r1 := a1 &lt;&lt; count</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := a7 &lt;&lt; count</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/c79w388h(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_sll_epi16(__m128i a, __m128i count)</span>
<span class="s1">{</span>
    <span class="s1">uint64_t c = vreinterpretq_nth_u64_m128i(count, </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">if </span><span class="s1">(c &gt; </span><span class="s3">15</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">_mm_setzero_si128();</span>

    <span class="s1">int16x8_t vc = vdupq_n_s16((int16_t) c);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vshlq_s16(vreinterpretq_s16_m128i(a), vc));</span>
<span class="s1">}</span>

<span class="s2">// Shifts the 4 signed or unsigned 32-bit integers in a left by count bits while</span>
<span class="s2">// shifting in zeros.</span>
<span class="s2">//</span>
<span class="s2">// r0 := a0 &lt;&lt; count</span>
<span class="s2">// r1 := a1 &lt;&lt; count</span>
<span class="s2">// r2 := a2 &lt;&lt; count</span>
<span class="s2">// r3 := a3 &lt;&lt; count</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/6fe5a6s9(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_sll_epi32(__m128i a, __m128i count)</span>
<span class="s1">{</span>
    <span class="s1">uint64_t c = vreinterpretq_nth_u64_m128i(count, </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">if </span><span class="s1">(c &gt; </span><span class="s3">31</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">_mm_setzero_si128();</span>

    <span class="s1">int32x4_t vc = vdupq_n_s32((int32_t) c);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vshlq_s32(vreinterpretq_s32_m128i(a), vc));</span>
<span class="s1">}</span>

<span class="s2">// Shifts the 2 signed or unsigned 64-bit integers in a left by count bits while</span>
<span class="s2">// shifting in zeros.</span>
<span class="s2">//</span>
<span class="s2">// r0 := a0 &lt;&lt; count</span>
<span class="s2">// r1 := a1 &lt;&lt; count</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/6ta9dffd(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_sll_epi64(__m128i a, __m128i count)</span>
<span class="s1">{</span>
    <span class="s1">uint64_t c = vreinterpretq_nth_u64_m128i(count, </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">if </span><span class="s1">(c &gt; </span><span class="s3">63</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">_mm_setzero_si128();</span>

    <span class="s1">int64x2_t vc = vdupq_n_s64((int64_t) c);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vshlq_s64(vreinterpretq_s64_m128i(a), vc));</span>
<span class="s1">}</span>

<span class="s2">// Shifts the 8 signed or unsigned 16-bit integers in a right by count bits</span>
<span class="s2">// while shifting in zeros.</span>
<span class="s2">//</span>
<span class="s2">// r0 := srl(a0, count)</span>
<span class="s2">// r1 := srl(a1, count)</span>
<span class="s2">// ...</span>
<span class="s2">// r7 := srl(a7, count)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/wd5ax830(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_srl_epi16(__m128i a, __m128i count)</span>
<span class="s1">{</span>
    <span class="s1">uint64_t c = vreinterpretq_nth_u64_m128i(count, </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">if </span><span class="s1">(c &gt; </span><span class="s3">15</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">_mm_setzero_si128();</span>

    <span class="s1">int16x8_t vc = vdupq_n_s16(-(int16_t) c);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(vshlq_u16(vreinterpretq_u16_m128i(a), vc));</span>
<span class="s1">}</span>

<span class="s2">// Shifts the 4 signed or unsigned 32-bit integers in a right by count bits</span>
<span class="s2">// while shifting in zeros.</span>
<span class="s2">//</span>
<span class="s2">// r0 := srl(a0, count)</span>
<span class="s2">// r1 := srl(a1, count)</span>
<span class="s2">// r2 := srl(a2, count)</span>
<span class="s2">// r3 := srl(a3, count)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/a9cbttf4(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_srl_epi32(__m128i a, __m128i count)</span>
<span class="s1">{</span>
    <span class="s1">uint64_t c = vreinterpretq_nth_u64_m128i(count, </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">if </span><span class="s1">(c &gt; </span><span class="s3">31</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">_mm_setzero_si128();</span>

    <span class="s1">int32x4_t vc = vdupq_n_s32(-(int32_t) c);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(vshlq_u32(vreinterpretq_u32_m128i(a), vc));</span>
<span class="s1">}</span>

<span class="s2">// Shifts the 2 signed or unsigned 64-bit integers in a right by count bits</span>
<span class="s2">// while shifting in zeros.</span>
<span class="s2">//</span>
<span class="s2">// r0 := srl(a0, count)</span>
<span class="s2">// r1 := srl(a1, count)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/yf6cf9k8(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_srl_epi64(__m128i a, __m128i count)</span>
<span class="s1">{</span>
    <span class="s1">uint64_t c = vreinterpretq_nth_u64_m128i(count, </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">if </span><span class="s1">(c &gt; </span><span class="s3">63</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">_mm_setzero_si128();</span>

    <span class="s1">int64x2_t vc = vdupq_n_s64(-(int64_t) c);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(vshlq_u64(vreinterpretq_u64_m128i(a), vc));</span>
<span class="s1">}</span>

<span class="s2">// NEON does not provide a version of this function.</span>
<span class="s2">// Creates a 16-bit mask from the most significant bits of the 16 signed or</span>
<span class="s2">// unsigned 8-bit integers in a and zero extends the upper bits.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/s090c8fk(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_movemask_epi8(__m128i a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">uint8x16_t input = vreinterpretq_u8_m128i(a);</span>
    <span class="s0">const </span><span class="s1">int8_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">)</span>
        <span class="s1">xr[</span><span class="s3">16</span><span class="s1">] = {-</span><span class="s3">7</span><span class="s1">, -</span><span class="s3">6</span><span class="s1">, -</span><span class="s3">5</span><span class="s1">, -</span><span class="s3">4</span><span class="s1">, -</span><span class="s3">3</span><span class="s1">, -</span><span class="s3">2</span><span class="s1">, -</span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, -</span><span class="s3">7</span><span class="s1">, -</span><span class="s3">6</span><span class="s1">, -</span><span class="s3">5</span><span class="s1">, -</span><span class="s3">4</span><span class="s1">, -</span><span class="s3">3</span><span class="s1">, -</span><span class="s3">2</span><span class="s1">, -</span><span class="s3">1</span><span class="s1">, </span><span class="s3">0</span><span class="s1">};</span>
    <span class="s0">const </span><span class="s1">uint8x16_t mask_and = vdupq_n_u8(</span><span class="s3">0x80</span><span class="s1">);</span>
    <span class="s0">const </span><span class="s1">int8x16_t mask_shift = vld1q_s8(xr);</span>
    <span class="s0">const </span><span class="s1">uint8x16_t mask_result =</span>
        <span class="s1">vshlq_u8(vandq_u8(input, mask_and), mask_shift);</span>
    <span class="s1">uint8x8_t lo = vget_low_u8(mask_result);</span>
    <span class="s1">uint8x8_t hi = vget_high_u8(mask_result);</span>

    <span class="s0">return </span><span class="s1">vaddv_u8(lo) + (vaddv_u8(hi) &lt;&lt; </span><span class="s3">8</span><span class="s1">);</span>
<span class="s0">#else</span>
    <span class="s2">// Use increasingly wide shifts+adds to collect the sign bits</span>
    <span class="s2">// together.</span>
    <span class="s2">// Since the widening shifts would be rather confusing to follow in little</span>
    <span class="s2">// endian, everything will be illustrated in big endian order instead. This</span>
    <span class="s2">// has a different result - the bits would actually be reversed on a big</span>
    <span class="s2">// endian machine.</span>

    <span class="s2">// Starting input (only half the elements are shown):</span>
    <span class="s2">// 89 ff 1d c0 00 10 99 33</span>
    <span class="s1">uint8x16_t input = vreinterpretq_u8_m128i(a);</span>

    <span class="s2">// Shift out everything but the sign bits with an unsigned shift right.</span>
    <span class="s2">//</span>
    <span class="s2">// Bytes of the vector::</span>
    <span class="s2">// 89 ff 1d c0 00 10 99 33</span>
    <span class="s2">// \  \  \  \  \  \  \  \    high_bits = (uint16x4_t)(input &gt;&gt; 7)</span>
    <span class="s2">//  |  |  |  |  |  |  |  |</span>
    <span class="s2">// 01 01 00 01 00 00 01 00</span>
    <span class="s2">//</span>
    <span class="s2">// Bits of first important lane(s):</span>
    <span class="s2">// 10001001 (89)</span>
    <span class="s2">// \______</span>
    <span class="s2">//        |</span>
    <span class="s2">// 00000001 (01)</span>
    <span class="s1">uint16x8_t high_bits = vreinterpretq_u16_u8(vshrq_n_u8(input, </span><span class="s3">7</span><span class="s1">));</span>

    <span class="s2">// Merge the even lanes together with a 16-bit unsigned shift right + add.</span>
    <span class="s2">// 'xx' represents garbage data which will be ignored in the final result.</span>
    <span class="s2">// In the important bytes, the add functions like a binary OR.</span>
    <span class="s2">//</span>
    <span class="s2">// 01 01 00 01 00 00 01 00</span>
    <span class="s2">//  \_ |  \_ |  \_ |  \_ |   paired16 = (uint32x4_t)(input + (input &gt;&gt; 7))</span>
    <span class="s2">//    \|    \|    \|    \|</span>
    <span class="s2">// xx 03 xx 01 xx 00 xx 02</span>
    <span class="s2">//</span>
    <span class="s2">// 00000001 00000001 (01 01)</span>
    <span class="s2">//        \_______ |</span>
    <span class="s2">//                \|</span>
    <span class="s2">// xxxxxxxx xxxxxx11 (xx 03)</span>
    <span class="s1">uint32x4_t paired16 =</span>
        <span class="s1">vreinterpretq_u32_u16(vsraq_n_u16(high_bits, high_bits, </span><span class="s3">7</span><span class="s1">));</span>

    <span class="s2">// Repeat with a wider 32-bit shift + add.</span>
    <span class="s2">// xx 03 xx 01 xx 00 xx 02</span>
    <span class="s2">//     \____ |     \____ |  paired32 = (uint64x1_t)(paired16 + (paired16 &gt;&gt;</span>
    <span class="s2">//     14))</span>
    <span class="s2">//          \|          \|</span>
    <span class="s2">// xx xx xx 0d xx xx xx 02</span>
    <span class="s2">//</span>
    <span class="s2">// 00000011 00000001 (03 01)</span>
    <span class="s2">//        \\_____ ||</span>
    <span class="s2">//         '----.\||</span>
    <span class="s2">// xxxxxxxx xxxx1101 (xx 0d)</span>
    <span class="s1">uint64x2_t paired32 =</span>
        <span class="s1">vreinterpretq_u64_u32(vsraq_n_u32(paired16, paired16, </span><span class="s3">14</span><span class="s1">));</span>

    <span class="s2">// Last, an even wider 64-bit shift + add to get our result in the low 8 bit</span>
    <span class="s2">// lanes. xx xx xx 0d xx xx xx 02</span>
    <span class="s2">//            \_________ |   paired64 = (uint8x8_t)(paired32 + (paired32 &gt;&gt;</span>
    <span class="s2">//            28))</span>
    <span class="s2">//                      \|</span>
    <span class="s2">// xx xx xx xx xx xx xx d2</span>
    <span class="s2">//</span>
    <span class="s2">// 00001101 00000010 (0d 02)</span>
    <span class="s2">//     \   \___ |  |</span>
    <span class="s2">//      '---.  \|  |</span>
    <span class="s2">// xxxxxxxx 11010010 (xx d2)</span>
    <span class="s1">uint8x16_t paired64 =</span>
        <span class="s1">vreinterpretq_u8_u64(vsraq_n_u64(paired32, paired32, </span><span class="s3">28</span><span class="s1">));</span>

    <span class="s2">// Extract the low 8 bits from each 64-bit lane with 2 8-bit extracts.</span>
    <span class="s2">// xx xx xx xx xx xx xx d2</span>
    <span class="s2">//                      ||  return paired64[0]</span>
    <span class="s2">//                      d2</span>
    <span class="s2">// Note: Little endian would return the correct value 4b (01001011) instead.</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_u8(paired64, </span><span class="s3">0</span><span class="s1">) | ((</span><span class="s0">int</span><span class="s1">) vgetq_lane_u8(paired64, </span><span class="s3">8</span><span class="s1">) &lt;&lt; </span><span class="s3">8</span><span class="s1">);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Copy the lower 64-bit integer in a to dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_movepi64_pi64</span>
<span class="s1">FORCE_INLINE __m64 _mm_movepi64_pi64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s64(vget_low_s64(vreinterpretq_s64_m128i(a)));</span>
<span class="s1">}</span>

<span class="s2">// Copy the 64-bit integer a to the lower element of dst, and zero the upper</span>
<span class="s2">// element.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0]</span>
<span class="s2">//   dst[127:64] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_movpi64_epi64</span>
<span class="s1">FORCE_INLINE __m128i _mm_movpi64_epi64(__m64 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(</span>
        <span class="s1">vcombine_s64(vreinterpret_s64_m64(a), vdup_n_s64(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s1">}</span>

<span class="s2">// NEON does not provide this method</span>
<span class="s2">// Creates a 4-bit mask from the most significant bits of the four</span>
<span class="s2">// single-precision, floating-point values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/4490ys29(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_movemask_ps(__m128 a)</span>
<span class="s1">{</span>
    <span class="s1">uint32x4_t input = vreinterpretq_u32_m128(a);</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">static const </span><span class="s1">int32x4_t shift = {</span><span class="s3">0</span><span class="s1">, </span><span class="s3">1</span><span class="s1">, </span><span class="s3">2</span><span class="s1">, </span><span class="s3">3</span><span class="s1">};</span>
    <span class="s1">uint32x4_t tmp = vshrq_n_u32(input, </span><span class="s3">31</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vaddvq_u32(vshlq_u32(tmp, shift));</span>
<span class="s0">#else</span>
    <span class="s2">// Uses the exact same method as _mm_movemask_epi8, see that for details.</span>
    <span class="s2">// Shift out everything but the sign bits with a 32-bit unsigned shift</span>
    <span class="s2">// right.</span>
    <span class="s1">uint64x2_t high_bits = vreinterpretq_u64_u32(vshrq_n_u32(input, </span><span class="s3">31</span><span class="s1">));</span>
    <span class="s2">// Merge the two pairs together with a 64-bit unsigned shift right + add.</span>
    <span class="s1">uint8x16_t paired =</span>
        <span class="s1">vreinterpretq_u8_u64(vsraq_n_u64(high_bits, high_bits, </span><span class="s3">31</span><span class="s1">));</span>
    <span class="s2">// Extract the result.</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_u8(paired, </span><span class="s3">0</span><span class="s1">) | (vgetq_lane_u8(paired, </span><span class="s3">8</span><span class="s1">) &lt;&lt; </span><span class="s3">2</span><span class="s1">);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Compute the bitwise NOT of a and then AND with a 128-bit vector containing</span>
<span class="s2">// all 1's, and return 1 if the result is zero, otherwise return 0.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_test_all_ones</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_test_all_ones(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">(uint64_t)(vgetq_lane_s64(a, </span><span class="s3">0</span><span class="s1">) &amp; vgetq_lane_s64(a, </span><span class="s3">1</span><span class="s1">)) ==</span>
           <span class="s1">~(uint64_t) </span><span class="s3">0</span><span class="s1">;</span>
<span class="s1">}</span>

<span class="s2">// Compute the bitwise AND of 128 bits (representing integer data) in a and</span>
<span class="s2">// mask, and return 1 if the result is zero, otherwise return 0.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_test_all_zeros</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_test_all_zeros(__m128i a, __m128i mask)</span>
<span class="s1">{</span>
    <span class="s1">int64x2_t a_and_mask =</span>
        <span class="s1">vandq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(mask));</span>
    <span class="s0">return </span><span class="s1">(vgetq_lane_s64(a_and_mask, </span><span class="s3">0</span><span class="s1">) | vgetq_lane_s64(a_and_mask, </span><span class="s3">1</span><span class="s1">)) ? </span><span class="s3">0</span>
                                                                           <span class="s1">: </span><span class="s3">1</span><span class="s1">;</span>
<span class="s1">}</span>

<span class="s2">/* Math operations */</span>

<span class="s2">// Subtracts the four single-precision, floating-point values of a and b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 - b0</span>
<span class="s2">//   r1 := a1 - b1</span>
<span class="s2">//   r2 := a2 - b2</span>
<span class="s2">//   r3 := a3 - b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/1zad2k61(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_sub_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vsubq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Subtract the lower single-precision (32-bit) floating-point element in b from</span>
<span class="s2">// the lower single-precision (32-bit) floating-point element in a, store the</span>
<span class="s2">// result in the lower element of dst, and copy the upper 3 packed elements from</span>
<span class="s2">// a to the upper elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := a[31:0] - b[31:0]</span>
<span class="s2">//   dst[127:32] := a[127:32]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_ss</span>
<span class="s1">FORCE_INLINE __m128 _mm_sub_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_sub_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Subtract 2 packed 64-bit integers in b from 2 packed 64-bit integers in a,</span>
<span class="s2">// and store the results in dst.</span>
<span class="s2">//    r0 := a0 - b0</span>
<span class="s2">//    r1 := a1 - b1</span>
<span class="s1">FORCE_INLINE __m128i _mm_sub_epi64(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(</span>
        <span class="s1">vsubq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Subtracts the 4 signed or unsigned 32-bit integers of b from the 4 signed or</span>
<span class="s2">// unsigned 32-bit integers of a.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 - b0</span>
<span class="s2">//   r1 := a1 - b1</span>
<span class="s2">//   r2 := a2 - b2</span>
<span class="s2">//   r3 := a3 - b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/fhh866h0(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_sub_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vsubq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_sub_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vsubq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_sub_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vsubq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Subtract 64-bit integer b from 64-bit integer a, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0] - b[63:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sub_si64</span>
<span class="s1">FORCE_INLINE __m64 _mm_sub_si64(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s64(</span>
        <span class="s1">vsub_s64(vreinterpret_s64_m64(a), vreinterpret_s64_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Subtracts the 8 unsigned 16-bit integers of bfrom the 8 unsigned 16-bit</span>
<span class="s2">// integers of a and saturates..</span>
<span class="s2">// https://technet.microsoft.com/en-us/subscriptions/index/f44y0s19(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_subs_epu16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(</span>
        <span class="s1">vqsubq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Subtracts the 16 unsigned 8-bit integers of b from the 16 unsigned 8-bit</span>
<span class="s2">// integers of a and saturates.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := UnsignedSaturate(a0 - b0)</span>
<span class="s2">//   r1 := UnsignedSaturate(a1 - b1)</span>
<span class="s2">//   ...</span>
<span class="s2">//   r15 := UnsignedSaturate(a15 - b15)</span>
<span class="s2">//</span>
<span class="s2">// https://technet.microsoft.com/en-us/subscriptions/yadkxc18(v=vs.90)</span>
<span class="s1">FORCE_INLINE __m128i _mm_subs_epu8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vqsubq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Subtracts the 16 signed 8-bit integers of b from the 16 signed 8-bit integers</span>
<span class="s2">// of a and saturates.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := SignedSaturate(a0 - b0)</span>
<span class="s2">//   r1 := SignedSaturate(a1 - b1)</span>
<span class="s2">//   ...</span>
<span class="s2">//   r15 := SignedSaturate(a15 - b15)</span>
<span class="s2">//</span>
<span class="s2">// https://technet.microsoft.com/en-us/subscriptions/by7kzks1(v=vs.90)</span>
<span class="s1">FORCE_INLINE __m128i _mm_subs_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vqsubq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Subtracts the 8 signed 16-bit integers of b from the 8 signed 16-bit integers</span>
<span class="s2">// of a and saturates.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := SignedSaturate(a0 - b0)</span>
<span class="s2">//   r1 := SignedSaturate(a1 - b1)</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := SignedSaturate(a7 - b7)</span>
<span class="s2">//</span>
<span class="s2">// https://technet.microsoft.com/en-us/subscriptions/3247z5b8(v=vs.90)</span>
<span class="s1">FORCE_INLINE __m128i _mm_subs_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vqsubq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_adds_epu16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(</span>
        <span class="s1">vqaddq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Negate packed 8-bit integers in a when the corresponding signed</span>
<span class="s2">// 8-bit integer in b is negative, and store the results in dst.</span>
<span class="s2">// Element in dst are zeroed out when the corresponding element</span>
<span class="s2">// in b is zero.</span>
<span class="s2">//</span>
<span class="s2">//   for i in 0..15</span>
<span class="s2">//     if b[i] &lt; 0</span>
<span class="s2">//       r[i] := -a[i]</span>
<span class="s2">//     else if b[i] == 0</span>
<span class="s2">//       r[i] := 0</span>
<span class="s2">//     else</span>
<span class="s2">//       r[i] := a[i]</span>
<span class="s2">//     fi</span>
<span class="s2">//   done</span>
<span class="s1">FORCE_INLINE __m128i _mm_sign_epi8(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
    <span class="s1">int8x16_t a = vreinterpretq_s8_m128i(_a);</span>
    <span class="s1">int8x16_t b = vreinterpretq_s8_m128i(_b);</span>

    <span class="s2">// signed shift right: faster than vclt</span>
    <span class="s2">// (b &lt; 0) ? 0xFF : 0</span>
    <span class="s1">uint8x16_t ltMask = vreinterpretq_u8_s8(vshrq_n_s8(b, </span><span class="s3">7</span><span class="s1">));</span>

    <span class="s2">// (b == 0) ? 0xFF : 0</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">int8x16_t zeroMask = vreinterpretq_s8_u8(vceqzq_s8(b));</span>
<span class="s0">#else</span>
    <span class="s1">int8x16_t zeroMask = vreinterpretq_s8_u8(vceqq_s8(b, vdupq_n_s8(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s0">#endif</span>

    <span class="s2">// bitwise select either a or nagative 'a' (vnegq_s8(a) return nagative 'a')</span>
    <span class="s2">// based on ltMask</span>
    <span class="s1">int8x16_t masked = vbslq_s8(ltMask, vnegq_s8(a), a);</span>
    <span class="s2">// res = masked &amp; (~zeroMask)</span>
    <span class="s1">int8x16_t res = vbicq_s8(masked, zeroMask);</span>

    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(res);</span>
<span class="s1">}</span>

<span class="s2">// Negate packed 16-bit integers in a when the corresponding signed</span>
<span class="s2">// 16-bit integer in b is negative, and store the results in dst.</span>
<span class="s2">// Element in dst are zeroed out when the corresponding element</span>
<span class="s2">// in b is zero.</span>
<span class="s2">//</span>
<span class="s2">//   for i in 0..7</span>
<span class="s2">//     if b[i] &lt; 0</span>
<span class="s2">//       r[i] := -a[i]</span>
<span class="s2">//     else if b[i] == 0</span>
<span class="s2">//       r[i] := 0</span>
<span class="s2">//     else</span>
<span class="s2">//       r[i] := a[i]</span>
<span class="s2">//     fi</span>
<span class="s2">//   done</span>
<span class="s1">FORCE_INLINE __m128i _mm_sign_epi16(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
    <span class="s1">int16x8_t a = vreinterpretq_s16_m128i(_a);</span>
    <span class="s1">int16x8_t b = vreinterpretq_s16_m128i(_b);</span>

    <span class="s2">// signed shift right: faster than vclt</span>
    <span class="s2">// (b &lt; 0) ? 0xFFFF : 0</span>
    <span class="s1">uint16x8_t ltMask = vreinterpretq_u16_s16(vshrq_n_s16(b, </span><span class="s3">15</span><span class="s1">));</span>
    <span class="s2">// (b == 0) ? 0xFFFF : 0</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">int16x8_t zeroMask = vreinterpretq_s16_u16(vceqzq_s16(b));</span>
<span class="s0">#else</span>
    <span class="s1">int16x8_t zeroMask = vreinterpretq_s16_u16(vceqq_s16(b, vdupq_n_s16(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s0">#endif</span>

    <span class="s2">// bitwise select either a or negative 'a' (vnegq_s16(a) equals to negative</span>
    <span class="s2">// 'a') based on ltMask</span>
    <span class="s1">int16x8_t masked = vbslq_s16(ltMask, vnegq_s16(a), a);</span>
    <span class="s2">// res = masked &amp; (~zeroMask)</span>
    <span class="s1">int16x8_t res = vbicq_s16(masked, zeroMask);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(res);</span>
<span class="s1">}</span>

<span class="s2">// Negate packed 32-bit integers in a when the corresponding signed</span>
<span class="s2">// 32-bit integer in b is negative, and store the results in dst.</span>
<span class="s2">// Element in dst are zeroed out when the corresponding element</span>
<span class="s2">// in b is zero.</span>
<span class="s2">//</span>
<span class="s2">//   for i in 0..3</span>
<span class="s2">//     if b[i] &lt; 0</span>
<span class="s2">//       r[i] := -a[i]</span>
<span class="s2">//     else if b[i] == 0</span>
<span class="s2">//       r[i] := 0</span>
<span class="s2">//     else</span>
<span class="s2">//       r[i] := a[i]</span>
<span class="s2">//     fi</span>
<span class="s2">//   done</span>
<span class="s1">FORCE_INLINE __m128i _mm_sign_epi32(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
    <span class="s1">int32x4_t a = vreinterpretq_s32_m128i(_a);</span>
    <span class="s1">int32x4_t b = vreinterpretq_s32_m128i(_b);</span>

    <span class="s2">// signed shift right: faster than vclt</span>
    <span class="s2">// (b &lt; 0) ? 0xFFFFFFFF : 0</span>
    <span class="s1">uint32x4_t ltMask = vreinterpretq_u32_s32(vshrq_n_s32(b, </span><span class="s3">31</span><span class="s1">));</span>

    <span class="s2">// (b == 0) ? 0xFFFFFFFF : 0</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">int32x4_t zeroMask = vreinterpretq_s32_u32(vceqzq_s32(b));</span>
<span class="s0">#else</span>
    <span class="s1">int32x4_t zeroMask = vreinterpretq_s32_u32(vceqq_s32(b, vdupq_n_s32(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s0">#endif</span>

    <span class="s2">// bitwise select either a or negative 'a' (vnegq_s32(a) equals to negative</span>
    <span class="s2">// 'a') based on ltMask</span>
    <span class="s1">int32x4_t masked = vbslq_s32(ltMask, vnegq_s32(a), a);</span>
    <span class="s2">// res = masked &amp; (~zeroMask)</span>
    <span class="s1">int32x4_t res = vbicq_s32(masked, zeroMask);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(res);</span>
<span class="s1">}</span>

<span class="s2">// Negate packed 16-bit integers in a when the corresponding signed 16-bit</span>
<span class="s2">// integer in b is negative, and store the results in dst. Element in dst are</span>
<span class="s2">// zeroed out when the corresponding element in b is zero.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      IF b[i+15:i] &lt; 0</span>
<span class="s2">//        dst[i+15:i] := -(a[i+15:i])</span>
<span class="s2">//      ELSE IF b[i+15:i] == 0</span>
<span class="s2">//        dst[i+15:i] := 0</span>
<span class="s2">//      ELSE</span>
<span class="s2">//        dst[i+15:i] := a[i+15:i]</span>
<span class="s2">//      FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sign_pi16</span>
<span class="s1">FORCE_INLINE __m64 _mm_sign_pi16(__m64 _a, __m64 _b)</span>
<span class="s1">{</span>
    <span class="s1">int16x4_t a = vreinterpret_s16_m64(_a);</span>
    <span class="s1">int16x4_t b = vreinterpret_s16_m64(_b);</span>

    <span class="s2">// signed shift right: faster than vclt</span>
    <span class="s2">// (b &lt; 0) ? 0xFFFF : 0</span>
    <span class="s1">uint16x4_t ltMask = vreinterpret_u16_s16(vshr_n_s16(b, </span><span class="s3">15</span><span class="s1">));</span>

    <span class="s2">// (b == 0) ? 0xFFFF : 0</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">int16x4_t zeroMask = vreinterpret_s16_u16(vceqz_s16(b));</span>
<span class="s0">#else</span>
    <span class="s1">int16x4_t zeroMask = vreinterpret_s16_u16(vceq_s16(b, vdup_n_s16(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s0">#endif</span>

    <span class="s2">// bitwise select either a or nagative 'a' (vneg_s16(a) return nagative 'a')</span>
    <span class="s2">// based on ltMask</span>
    <span class="s1">int16x4_t masked = vbsl_s16(ltMask, vneg_s16(a), a);</span>
    <span class="s2">// res = masked &amp; (~zeroMask)</span>
    <span class="s1">int16x4_t res = vbic_s16(masked, zeroMask);</span>

    <span class="s0">return </span><span class="s1">vreinterpret_m64_s16(res);</span>
<span class="s1">}</span>

<span class="s2">// Negate packed 32-bit integers in a when the corresponding signed 32-bit</span>
<span class="s2">// integer in b is negative, and store the results in dst. Element in dst are</span>
<span class="s2">// zeroed out when the corresponding element in b is zero.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//      i := j*32</span>
<span class="s2">//      IF b[i+31:i] &lt; 0</span>
<span class="s2">//        dst[i+31:i] := -(a[i+31:i])</span>
<span class="s2">//      ELSE IF b[i+31:i] == 0</span>
<span class="s2">//        dst[i+31:i] := 0</span>
<span class="s2">//      ELSE</span>
<span class="s2">//        dst[i+31:i] := a[i+31:i]</span>
<span class="s2">//      FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sign_pi32</span>
<span class="s1">FORCE_INLINE __m64 _mm_sign_pi32(__m64 _a, __m64 _b)</span>
<span class="s1">{</span>
    <span class="s1">int32x2_t a = vreinterpret_s32_m64(_a);</span>
    <span class="s1">int32x2_t b = vreinterpret_s32_m64(_b);</span>

    <span class="s2">// signed shift right: faster than vclt</span>
    <span class="s2">// (b &lt; 0) ? 0xFFFFFFFF : 0</span>
    <span class="s1">uint32x2_t ltMask = vreinterpret_u32_s32(vshr_n_s32(b, </span><span class="s3">31</span><span class="s1">));</span>

    <span class="s2">// (b == 0) ? 0xFFFFFFFF : 0</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">int32x2_t zeroMask = vreinterpret_s32_u32(vceqz_s32(b));</span>
<span class="s0">#else</span>
    <span class="s1">int32x2_t zeroMask = vreinterpret_s32_u32(vceq_s32(b, vdup_n_s32(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s0">#endif</span>

    <span class="s2">// bitwise select either a or nagative 'a' (vneg_s32(a) return nagative 'a')</span>
    <span class="s2">// based on ltMask</span>
    <span class="s1">int32x2_t masked = vbsl_s32(ltMask, vneg_s32(a), a);</span>
    <span class="s2">// res = masked &amp; (~zeroMask)</span>
    <span class="s1">int32x2_t res = vbic_s32(masked, zeroMask);</span>

    <span class="s0">return </span><span class="s1">vreinterpret_m64_s32(res);</span>
<span class="s1">}</span>

<span class="s2">// Negate packed 8-bit integers in a when the corresponding signed 8-bit integer</span>
<span class="s2">// in b is negative, and store the results in dst. Element in dst are zeroed out</span>
<span class="s2">// when the corresponding element in b is zero.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//      i := j*8</span>
<span class="s2">//      IF b[i+7:i] &lt; 0</span>
<span class="s2">//        dst[i+7:i] := -(a[i+7:i])</span>
<span class="s2">//      ELSE IF b[i+7:i] == 0</span>
<span class="s2">//        dst[i+7:i] := 0</span>
<span class="s2">//      ELSE</span>
<span class="s2">//        dst[i+7:i] := a[i+7:i]</span>
<span class="s2">//      FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sign_pi8</span>
<span class="s1">FORCE_INLINE __m64 _mm_sign_pi8(__m64 _a, __m64 _b)</span>
<span class="s1">{</span>
    <span class="s1">int8x8_t a = vreinterpret_s8_m64(_a);</span>
    <span class="s1">int8x8_t b = vreinterpret_s8_m64(_b);</span>

    <span class="s2">// signed shift right: faster than vclt</span>
    <span class="s2">// (b &lt; 0) ? 0xFF : 0</span>
    <span class="s1">uint8x8_t ltMask = vreinterpret_u8_s8(vshr_n_s8(b, </span><span class="s3">7</span><span class="s1">));</span>

    <span class="s2">// (b == 0) ? 0xFF : 0</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">int8x8_t zeroMask = vreinterpret_s8_u8(vceqz_s8(b));</span>
<span class="s0">#else</span>
    <span class="s1">int8x8_t zeroMask = vreinterpret_s8_u8(vceq_s8(b, vdup_n_s8(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s0">#endif</span>

    <span class="s2">// bitwise select either a or nagative 'a' (vneg_s8(a) return nagative 'a')</span>
    <span class="s2">// based on ltMask</span>
    <span class="s1">int8x8_t masked = vbsl_s8(ltMask, vneg_s8(a), a);</span>
    <span class="s2">// res = masked &amp; (~zeroMask)</span>
    <span class="s1">int8x8_t res = vbic_s8(masked, zeroMask);</span>

    <span class="s0">return </span><span class="s1">vreinterpret_m64_s8(res);</span>
<span class="s1">}</span>

<span class="s2">// Average packed unsigned 16-bit integers in a and b, and store the results in</span>
<span class="s2">// dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//     i := j*16</span>
<span class="s2">//     dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_avg_pu16</span>
<span class="s1">FORCE_INLINE __m64 _mm_avg_pu16(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_u16(</span>
        <span class="s1">vrhadd_u16(vreinterpret_u16_m64(a), vreinterpret_u16_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Average packed unsigned 8-bit integers in a and b, and store the results in</span>
<span class="s2">// dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//     i := j*8</span>
<span class="s2">//     dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_avg_pu8</span>
<span class="s1">FORCE_INLINE __m64 _mm_avg_pu8(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_u8(</span>
        <span class="s1">vrhadd_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Average packed unsigned 8-bit integers in a and b, and store the results in</span>
<span class="s2">// dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//     i := j*8</span>
<span class="s2">//     dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_m_pavgb</span>
<span class="s0">#define </span><span class="s1">_m_pavgb(a, b) _mm_avg_pu8(a, b)</span>

<span class="s2">// Average packed unsigned 16-bit integers in a and b, and store the results in</span>
<span class="s2">// dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//     i := j*16</span>
<span class="s2">//     dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_m_pavgw</span>
<span class="s0">#define </span><span class="s1">_m_pavgw(a, b) _mm_avg_pu16(a, b)</span>

<span class="s2">// Computes the average of the 16 unsigned 8-bit integers in a and the 16</span>
<span class="s2">// unsigned 8-bit integers in b and rounds.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 + b0) / 2</span>
<span class="s2">//   r1 := (a1 + b1) / 2</span>
<span class="s2">//   ...</span>
<span class="s2">//   r15 := (a15 + b15) / 2</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/8zwh554a(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_avg_epu8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vrhaddq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the average of the 8 unsigned 16-bit integers in a and the 8</span>
<span class="s2">// unsigned 16-bit integers in b and rounds.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 + b0) / 2</span>
<span class="s2">//   r1 := (a1 + b1) / 2</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := (a7 + b7) / 2</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/y13ca3c8(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_avg_epu16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">(__m128i) vrhaddq_u16(vreinterpretq_u16_m128i(a),</span>
                                 <span class="s1">vreinterpretq_u16_m128i(b));</span>
<span class="s1">}</span>

<span class="s2">// Adds the four single-precision, floating-point values of a and b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 + b0</span>
<span class="s2">//   r1 := a1 + b1</span>
<span class="s2">//   r2 := a2 + b2</span>
<span class="s2">//   r3 := a3 + b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/c9848chc(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_add_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vaddq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Add packed double-precision (64-bit) floating-point elements in a and b, and</span>
<span class="s2">// store the results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_add_pd(__m128d a, __m128d b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(</span>
        <span class="s1">vaddq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));</span>
<span class="s0">#else</span>
    <span class="s0">double </span><span class="s1">*da = (</span><span class="s0">double </span><span class="s1">*) &amp;a;</span>
    <span class="s0">double </span><span class="s1">*db = (</span><span class="s0">double </span><span class="s1">*) &amp;b;</span>
    <span class="s0">double </span><span class="s1">c[</span><span class="s3">2</span><span class="s1">];</span>
    <span class="s1">c[</span><span class="s3">0</span><span class="s1">] = da[</span><span class="s3">0</span><span class="s1">] + db[</span><span class="s3">0</span><span class="s1">];</span>
    <span class="s1">c[</span><span class="s3">1</span><span class="s1">] = da[</span><span class="s3">1</span><span class="s1">] + db[</span><span class="s3">1</span><span class="s1">];</span>
    <span class="s0">return </span><span class="s1">vld1q_f32((float32_t *) c);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Add 64-bit integers a and b, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0] + b[63:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_add_si64</span>
<span class="s1">FORCE_INLINE __m64 _mm_add_si64(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s64(</span>
        <span class="s1">vadd_s64(vreinterpret_s64_m64(a), vreinterpret_s64_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// adds the scalar single-precision floating point values of a and b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/be94x2y6(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_add_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32_t b0 = vgetq_lane_f32(vreinterpretq_f32_m128(b), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32x4_t value = vsetq_lane_f32(b0, vdupq_n_f32(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s2">// the upper values in the result must be the remnants of &lt;a&gt;.</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vaddq_f32(a, value));</span>
<span class="s1">}</span>

<span class="s2">// Adds the 4 signed or unsigned 64-bit integers in a to the 4 signed or</span>
<span class="s2">// unsigned 32-bit integers in b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/09xs4fkk(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_add_epi64(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(</span>
        <span class="s1">vaddq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Adds the 4 signed or unsigned 32-bit integers in a to the 4 signed or</span>
<span class="s2">// unsigned 32-bit integers in b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 + b0</span>
<span class="s2">//   r1 := a1 + b1</span>
<span class="s2">//   r2 := a2 + b2</span>
<span class="s2">//   r3 := a3 + b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/09xs4fkk(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_add_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vaddq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Adds the 8 signed or unsigned 16-bit integers in a to the 8 signed or</span>
<span class="s2">// unsigned 16-bit integers in b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/fceha5k4(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_add_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vaddq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Adds the 16 signed or unsigned 8-bit integers in a to the 16 signed or</span>
<span class="s2">// unsigned 8-bit integers in b.</span>
<span class="s2">// https://technet.microsoft.com/en-us/subscriptions/yc7tcyzs(v=vs.90)</span>
<span class="s1">FORCE_INLINE __m128i _mm_add_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vaddq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Adds the 8 signed 16-bit integers in a to the 8 signed 16-bit integers in b</span>
<span class="s2">// and saturates.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := SignedSaturate(a0 + b0)</span>
<span class="s2">//   r1 := SignedSaturate(a1 + b1)</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := SignedSaturate(a7 + b7)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/1a306ef8(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_adds_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vqaddq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Add packed signed 8-bit integers in a and b using saturation, and store the</span>
<span class="s2">// results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 15</span>
<span class="s2">//     i := j*8</span>
<span class="s2">//     dst[i+7:i] := Saturate8( a[i+7:i] + b[i+7:i] )</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_adds_epi8</span>
<span class="s1">FORCE_INLINE __m128i _mm_adds_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vqaddq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Adds the 16 unsigned 8-bit integers in a to the 16 unsigned 8-bit integers in</span>
<span class="s2">// b and saturates..</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/9hahyddy(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_adds_epu8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vqaddq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Multiplies the 8 signed or unsigned 16-bit integers from a by the 8 signed or</span>
<span class="s2">// unsigned 16-bit integers from b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 * b0)[15:0]</span>
<span class="s2">//   r1 := (a1 * b1)[15:0]</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := (a7 * b7)[15:0]</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/9ks1472s(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_mullo_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vmulq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Multiplies the 4 signed or unsigned 32-bit integers from a by the 4 signed or</span>
<span class="s2">// unsigned 32-bit integers from b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/bb531409(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_mullo_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vmulq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Multiply the packed unsigned 16-bit integers in a and b, producing</span>
<span class="s2">// intermediate 32-bit integers, and store the high 16 bits of the intermediate</span>
<span class="s2">// integers in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      tmp[31:0] := a[i+15:i] * b[i+15:i]</span>
<span class="s2">//      dst[i+15:i] := tmp[31:16]</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_m_pmulhuw</span>
<span class="s0">#define </span><span class="s1">_m_pmulhuw(a, b) _mm_mulhi_pu16(a, b)</span>

<span class="s2">// Multiplies the four single-precision, floating-point values of a and b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 * b0</span>
<span class="s2">//   r1 := a1 * b1</span>
<span class="s2">//   r2 := a2 * b2</span>
<span class="s2">//   r3 := a3 * b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/22kbk6t9(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_mul_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vmulq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Multiply packed double-precision (64-bit) floating-point elements in a and b,</span>
<span class="s2">// and store the results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mul_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_mul_pd(__m128d a, __m128d b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(</span>
        <span class="s1">vmulq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));</span>
<span class="s0">#else</span>
    <span class="s0">double </span><span class="s1">*da = (</span><span class="s0">double </span><span class="s1">*) &amp;a;</span>
    <span class="s0">double </span><span class="s1">*db = (</span><span class="s0">double </span><span class="s1">*) &amp;b;</span>
    <span class="s0">double </span><span class="s1">c[</span><span class="s3">2</span><span class="s1">];</span>
    <span class="s1">c[</span><span class="s3">0</span><span class="s1">] = da[</span><span class="s3">0</span><span class="s1">] * db[</span><span class="s3">0</span><span class="s1">];</span>
    <span class="s1">c[</span><span class="s3">1</span><span class="s1">] = da[</span><span class="s3">1</span><span class="s1">] * db[</span><span class="s3">1</span><span class="s1">];</span>
    <span class="s0">return </span><span class="s1">vld1q_f32((float32_t *) c);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Multiply the lower single-precision (32-bit) floating-point element in a and</span>
<span class="s2">// b, store the result in the lower element of dst, and copy the upper 3 packed</span>
<span class="s2">// elements from a to the upper elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := a[31:0] * b[31:0]</span>
<span class="s2">//   dst[127:32] := a[127:32]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mul_ss</span>
<span class="s1">FORCE_INLINE __m128 _mm_mul_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_mul_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Multiply the low unsigned 32-bit integers from each packed 64-bit element in</span>
<span class="s2">// a and b, and store the unsigned 64-bit results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   r0 :=  (a0 &amp; 0xFFFFFFFF) * (b0 &amp; 0xFFFFFFFF)</span>
<span class="s2">//   r1 :=  (a2 &amp; 0xFFFFFFFF) * (b2 &amp; 0xFFFFFFFF)</span>
<span class="s1">FORCE_INLINE __m128i _mm_mul_epu32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s2">// vmull_u32 upcasts instead of masking, so we downcast.</span>
    <span class="s1">uint32x2_t a_lo = vmovn_u64(vreinterpretq_u64_m128i(a));</span>
    <span class="s1">uint32x2_t b_lo = vmovn_u64(vreinterpretq_u64_m128i(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(vmull_u32(a_lo, b_lo));</span>
<span class="s1">}</span>

<span class="s2">// Multiply the low unsigned 32-bit integers from a and b, and store the</span>
<span class="s2">// unsigned 64-bit result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[31:0] * b[31:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mul_su32</span>
<span class="s1">FORCE_INLINE __m64 _mm_mul_su32(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_u64(vget_low_u64(</span>
        <span class="s1">vmull_u32(vreinterpret_u32_m64(a), vreinterpret_u32_m64(b))));</span>
<span class="s1">}</span>

<span class="s2">// Multiply the low signed 32-bit integers from each packed 64-bit element in</span>
<span class="s2">// a and b, and store the signed 64-bit results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   r0 :=  (int64_t)(int32_t)a0 * (int64_t)(int32_t)b0</span>
<span class="s2">//   r1 :=  (int64_t)(int32_t)a2 * (int64_t)(int32_t)b2</span>
<span class="s1">FORCE_INLINE __m128i _mm_mul_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s2">// vmull_s32 upcasts instead of masking, so we downcast.</span>
    <span class="s1">int32x2_t a_lo = vmovn_s64(vreinterpretq_s64_m128i(a));</span>
    <span class="s1">int32x2_t b_lo = vmovn_s64(vreinterpretq_s64_m128i(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vmull_s32(a_lo, b_lo));</span>
<span class="s1">}</span>

<span class="s2">// Multiplies the 8 signed 16-bit integers from a by the 8 signed 16-bit</span>
<span class="s2">// integers from b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 * b0) + (a1 * b1)</span>
<span class="s2">//   r1 := (a2 * b2) + (a3 * b3)</span>
<span class="s2">//   r2 := (a4 * b4) + (a5 * b5)</span>
<span class="s2">//   r3 := (a6 * b6) + (a7 * b7)</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/yht36sa6(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_madd_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">int32x4_t low = vmull_s16(vget_low_s16(vreinterpretq_s16_m128i(a)),</span>
                              <span class="s1">vget_low_s16(vreinterpretq_s16_m128i(b)));</span>
    <span class="s1">int32x4_t high = vmull_s16(vget_high_s16(vreinterpretq_s16_m128i(a)),</span>
                               <span class="s1">vget_high_s16(vreinterpretq_s16_m128i(b)));</span>

    <span class="s1">int32x2_t low_sum = vpadd_s32(vget_low_s32(low), vget_high_s32(low));</span>
    <span class="s1">int32x2_t high_sum = vpadd_s32(vget_low_s32(high), vget_high_s32(high));</span>

    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(low_sum, high_sum));</span>
<span class="s1">}</span>

<span class="s2">// Multiply packed signed 16-bit integers in a and b, producing intermediate</span>
<span class="s2">// signed 32-bit integers. Shift right by 15 bits while rounding up, and store</span>
<span class="s2">// the packed 16-bit integers in dst.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := Round(((int32_t)a0 * (int32_t)b0) &gt;&gt; 15)</span>
<span class="s2">//   r1 := Round(((int32_t)a1 * (int32_t)b1) &gt;&gt; 15)</span>
<span class="s2">//   r2 := Round(((int32_t)a2 * (int32_t)b2) &gt;&gt; 15)</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := Round(((int32_t)a7 * (int32_t)b7) &gt;&gt; 15)</span>
<span class="s1">FORCE_INLINE __m128i _mm_mulhrs_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s2">// Has issues due to saturation</span>
    <span class="s2">// return vreinterpretq_m128i_s16(vqrdmulhq_s16(a, b));</span>

    <span class="s2">// Multiply</span>
    <span class="s1">int32x4_t mul_lo = vmull_s16(vget_low_s16(vreinterpretq_s16_m128i(a)),</span>
                                 <span class="s1">vget_low_s16(vreinterpretq_s16_m128i(b)));</span>
    <span class="s1">int32x4_t mul_hi = vmull_s16(vget_high_s16(vreinterpretq_s16_m128i(a)),</span>
                                 <span class="s1">vget_high_s16(vreinterpretq_s16_m128i(b)));</span>

    <span class="s2">// Rounding narrowing shift right</span>
    <span class="s2">// narrow = (int16_t)((mul + 16384) &gt;&gt; 15);</span>
    <span class="s1">int16x4_t narrow_lo = vrshrn_n_s32(mul_lo, </span><span class="s3">15</span><span class="s1">);</span>
    <span class="s1">int16x4_t narrow_hi = vrshrn_n_s32(mul_hi, </span><span class="s3">15</span><span class="s1">);</span>

    <span class="s2">// Join together</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vcombine_s16(narrow_lo, narrow_hi));</span>
<span class="s1">}</span>

<span class="s2">// Vertically multiply each unsigned 8-bit integer from a with the corresponding</span>
<span class="s2">// signed 8-bit integer from b, producing intermediate signed 16-bit integers.</span>
<span class="s2">// Horizontally add adjacent pairs of intermediate signed 16-bit integers,</span>
<span class="s2">// and pack the saturated results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] +</span>
<span class="s2">//      a[i+7:i]*b[i+7:i] )</span>
<span class="s2">//   ENDFOR</span>
<span class="s1">FORCE_INLINE __m128i _mm_maddubs_epi16(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">uint8x16_t a = vreinterpretq_u8_m128i(_a);</span>
    <span class="s1">int8x16_t b = vreinterpretq_s8_m128i(_b);</span>
    <span class="s1">int16x8_t tl = vmulq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(a))),</span>
                             <span class="s1">vmovl_s8(vget_low_s8(b)));</span>
    <span class="s1">int16x8_t th = vmulq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(a))),</span>
                             <span class="s1">vmovl_s8(vget_high_s8(b)));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vqaddq_s16(vuzp1q_s16(tl, th), vuzp2q_s16(tl, th)));</span>
<span class="s0">#else</span>
    <span class="s2">// This would be much simpler if x86 would choose to zero extend OR sign</span>
    <span class="s2">// extend, not both. This could probably be optimized better.</span>
    <span class="s1">uint16x8_t a = vreinterpretq_u16_m128i(_a);</span>
    <span class="s1">int16x8_t b = vreinterpretq_s16_m128i(_b);</span>

    <span class="s2">// Zero extend a</span>
    <span class="s1">int16x8_t a_odd = vreinterpretq_s16_u16(vshrq_n_u16(a, </span><span class="s3">8</span><span class="s1">));</span>
    <span class="s1">int16x8_t a_even = vreinterpretq_s16_u16(vbicq_u16(a, vdupq_n_u16(</span><span class="s3">0xff00</span><span class="s1">)));</span>

    <span class="s2">// Sign extend by shifting left then shifting right.</span>
    <span class="s1">int16x8_t b_even = vshrq_n_s16(vshlq_n_s16(b, </span><span class="s3">8</span><span class="s1">), </span><span class="s3">8</span><span class="s1">);</span>
    <span class="s1">int16x8_t b_odd = vshrq_n_s16(b, </span><span class="s3">8</span><span class="s1">);</span>

    <span class="s2">// multiply</span>
    <span class="s1">int16x8_t prod1 = vmulq_s16(a_even, b_even);</span>
    <span class="s1">int16x8_t prod2 = vmulq_s16(a_odd, b_odd);</span>

    <span class="s2">// saturated add</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vqaddq_s16(prod1, prod2));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Computes the fused multiple add product of 32-bit floating point numbers.</span>
<span class="s2">//</span>
<span class="s2">// Return Value</span>
<span class="s2">// Multiplies A and B, and adds C to the temporary result before returning it.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_fmadd</span>
<span class="s1">FORCE_INLINE __m128 _mm_fmadd_ps(__m128 a, __m128 b, __m128 c)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vfmaq_f32(vreinterpretq_f32_m128(c),</span>
                                            <span class="s1">vreinterpretq_f32_m128(b),</span>
                                            <span class="s1">vreinterpretq_f32_m128(a)));</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">_mm_add_ps(_mm_mul_ps(a, b), c);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Alternatively add and subtract packed single-precision (32-bit)</span>
<span class="s2">// floating-point elements in a to/from packed elements in b, and store the</span>
<span class="s2">// results in dst.</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=addsub_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_addsub_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">__m128 mask = {-</span><span class="s3">1</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f, </span><span class="s3">1</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f, -</span><span class="s3">1</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f, </span><span class="s3">1</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f};</span>
    <span class="s0">return </span><span class="s1">_mm_fmadd_ps(b, mask, a);</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute differences of packed unsigned 8-bit integers in a and</span>
<span class="s2">// b, then horizontally sum each consecutive 8 differences to produce two</span>
<span class="s2">// unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low</span>
<span class="s2">// 16 bits of 64-bit elements in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sad_epu8</span>
<span class="s1">FORCE_INLINE __m128i _mm_sad_epu8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">uint16x8_t t = vpaddlq_u8(vabdq_u8((uint8x16_t) a, (uint8x16_t) b));</span>
    <span class="s1">uint16_t r0 = t[</span><span class="s3">0</span><span class="s1">] + t[</span><span class="s3">1</span><span class="s1">] + t[</span><span class="s3">2</span><span class="s1">] + t[</span><span class="s3">3</span><span class="s1">];</span>
    <span class="s1">uint16_t r4 = t[</span><span class="s3">4</span><span class="s1">] + t[</span><span class="s3">5</span><span class="s1">] + t[</span><span class="s3">6</span><span class="s1">] + t[</span><span class="s3">7</span><span class="s1">];</span>
    <span class="s1">uint16x8_t r = vsetq_lane_u16(r0, vdupq_n_u16(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">(__m128i) vsetq_lane_u16(r4, r, </span><span class="s3">4</span><span class="s1">);</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute differences of packed unsigned 8-bit integers in a and</span>
<span class="s2">// b, then horizontally sum each consecutive 8 differences to produce four</span>
<span class="s2">// unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low</span>
<span class="s2">// 16 bits of dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_sad_pu8</span>
<span class="s1">FORCE_INLINE __m64 _mm_sad_pu8(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s1">uint16x4_t t =</span>
        <span class="s1">vpaddl_u8(vabd_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));</span>
    <span class="s1">uint16_t r0 = t[</span><span class="s3">0</span><span class="s1">] + t[</span><span class="s3">1</span><span class="s1">] + t[</span><span class="s3">2</span><span class="s1">] + t[</span><span class="s3">3</span><span class="s1">];</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_u16(vset_lane_u16(r0, vdup_n_u16(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Compute the absolute differences of packed unsigned 8-bit integers in a and</span>
<span class="s2">// b, then horizontally sum each consecutive 8 differences to produce four</span>
<span class="s2">// unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low</span>
<span class="s2">// 16 bits of dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//      i := j*8</span>
<span class="s2">//      tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//   dst[15:0] := tmp[7:0] + tmp[15:8] + tmp[23:16] + tmp[31:24] + tmp[39:32] +</span>
<span class="s2">//   tmp[47:40] + tmp[55:48] + tmp[63:56] dst[63:16] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_m_psadbw</span>
<span class="s0">#define </span><span class="s1">_m_psadbw(a, b) _mm_sad_pu8(a, b)</span>

<span class="s2">// Divides the four single-precision, floating-point values of a and b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 / b0</span>
<span class="s2">//   r1 := a1 / b1</span>
<span class="s2">//   r2 := a2 / b2</span>
<span class="s2">//   r3 := a3 / b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/edaw8147(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_div_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vdivq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s0">#else</span>
    <span class="s1">float32x4_t recip0 = vrecpeq_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s1">float32x4_t recip1 =</span>
        <span class="s1">vmulq_f32(recip0, vrecpsq_f32(recip0, vreinterpretq_f32_m128(b)));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vmulq_f32(vreinterpretq_f32_m128(a), recip1));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Divides the scalar single-precision floating point value of a by b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/4y73xa49(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_div_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32_t value =</span>
        <span class="s1">vgetq_lane_f32(vreinterpretq_f32_m128(_mm_div_ps(a, b)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vsetq_lane_f32(value, vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Compute the approximate reciprocal of packed single-precision (32-bit)</span>
<span class="s2">// floating-point elements in a, and store the results in dst. The maximum</span>
<span class="s2">// relative error for this approximation is less than 1.5*2^-12.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_rcp_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_rcp_ps(__m128 in)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vdivq_f32(vdupq_n_f32(</span><span class="s3">1</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f), vreinterpretq_f32_m128(in)));</span>
<span class="s0">#else</span>
    <span class="s1">float32x4_t recip = vrecpeq_f32(vreinterpretq_f32_m128(in));</span>
    <span class="s1">recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(in)));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(recip);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Compute the approximate reciprocal of the lower single-precision (32-bit)</span>
<span class="s2">// floating-point element in a, store the result in the lower element of dst,</span>
<span class="s2">// and copy the upper 3 packed elements from a to the upper elements of dst. The</span>
<span class="s2">// maximum relative error for this approximation is less than 1.5*2^-12.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := (1.0 / a[31:0])</span>
<span class="s2">//   dst[127:32] := a[127:32]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_rcp_ss</span>
<span class="s1">FORCE_INLINE __m128 _mm_rcp_ss(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_rcp_ps(a));</span>
<span class="s1">}</span>

<span class="s2">// Computes the approximations of square roots of the four single-precision,</span>
<span class="s2">// floating-point values of a. First computes reciprocal square roots and then</span>
<span class="s2">// reciprocals of the four values.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := sqrt(a0)</span>
<span class="s2">//   r1 := sqrt(a1)</span>
<span class="s2">//   r2 := sqrt(a2)</span>
<span class="s2">//   r3 := sqrt(a3)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/8z67bwwk(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_sqrt_ps(__m128 in)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vsqrtq_f32(vreinterpretq_f32_m128(in)));</span>
<span class="s0">#else</span>
    <span class="s1">float32x4_t recipsq = vrsqrteq_f32(vreinterpretq_f32_m128(in));</span>
    <span class="s1">float32x4_t sq = vrecpeq_f32(recipsq);</span>
    <span class="s2">// ??? use step versions of both sqrt and recip for better accuracy?</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(sq);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Computes the approximation of the square root of the scalar single-precision</span>
<span class="s2">// floating point value of in.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/ahfsc22d(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_sqrt_ss(__m128 in)</span>
<span class="s1">{</span>
    <span class="s1">float32_t value =</span>
        <span class="s1">vgetq_lane_f32(vreinterpretq_f32_m128(_mm_sqrt_ps(in)), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vsetq_lane_f32(value, vreinterpretq_f32_m128(in), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Computes the approximations of the reciprocal square roots of the four</span>
<span class="s2">// single-precision floating point values of in.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/22hfsh53(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_rsqrt_ps(__m128 in)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vrsqrteq_f32(vreinterpretq_f32_m128(in)));</span>
<span class="s1">}</span>

<span class="s2">// Compute the approximate reciprocal square root of the lower single-precision</span>
<span class="s2">// (32-bit) floating-point element in a, store the result in the lower element</span>
<span class="s2">// of dst, and copy the upper 3 packed elements from a to the upper elements of</span>
<span class="s2">// dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_rsqrt_ss</span>
<span class="s1">FORCE_INLINE __m128 _mm_rsqrt_ss(__m128 in)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vsetq_lane_f32(vgetq_lane_f32(_mm_rsqrt_ps(in), </span><span class="s3">0</span><span class="s1">), in, </span><span class="s3">0</span><span class="s1">);</span>
<span class="s1">}</span>

<span class="s2">// Compare packed signed 16-bit integers in a and b, and store packed maximum</span>
<span class="s2">// values in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_pi16</span>
<span class="s1">FORCE_INLINE __m64 _mm_max_pi16(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s16(</span>
        <span class="s1">vmax_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed signed 16-bit integers in a and b, and store packed maximum</span>
<span class="s2">// values in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_pi16</span>
<span class="s0">#define </span><span class="s1">_m_pmaxsw(a, b) _mm_max_pi16(a, b)</span>

<span class="s2">// Computes the maximums of the four single-precision, floating-point values of</span>
<span class="s2">// a and b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/ff5d607a(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_max_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">SSE2NEON_PRECISE_MINMAX</span>
    <span class="s1">float32x4_t _a = vreinterpretq_f32_m128(a);</span>
    <span class="s1">float32x4_t _b = vreinterpretq_f32_m128(b);</span>
    <span class="s0">return </span><span class="s1">vbslq_f32(vcltq_f32(_b, _a), _a, _b);</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vmaxq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Compare packed unsigned 8-bit integers in a and b, and store packed maximum</span>
<span class="s2">// values in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//      i := j*8</span>
<span class="s2">//      dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_pu8</span>
<span class="s1">FORCE_INLINE __m64 _mm_max_pu8(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_u8(</span>
        <span class="s1">vmax_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed unsigned 8-bit integers in a and b, and store packed maximum</span>
<span class="s2">// values in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//      i := j*8</span>
<span class="s2">//      dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_pu8</span>
<span class="s0">#define </span><span class="s1">_m_pmaxub(a, b) _mm_max_pu8(a, b)</span>

<span class="s2">// Compare packed signed 16-bit integers in a and b, and store packed minimum</span>
<span class="s2">// values in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_pi16</span>
<span class="s1">FORCE_INLINE __m64 _mm_min_pi16(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s16(</span>
        <span class="s1">vmin_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed signed 16-bit integers in a and b, and store packed minimum</span>
<span class="s2">// values in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_pi16</span>
<span class="s0">#define </span><span class="s1">_m_pminsw(a, b) _mm_min_pi16(a, b)</span>

<span class="s2">// Computes the minima of the four single-precision, floating-point values of a</span>
<span class="s2">// and b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/wh13kadz(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_min_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">SSE2NEON_PRECISE_MINMAX</span>
    <span class="s1">float32x4_t _a = vreinterpretq_f32_m128(a);</span>
    <span class="s1">float32x4_t _b = vreinterpretq_f32_m128(b);</span>
    <span class="s0">return </span><span class="s1">vbslq_f32(vcltq_f32(_a, _b), _a, _b);</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vminq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Compare packed unsigned 8-bit integers in a and b, and store packed minimum</span>
<span class="s2">// values in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//      i := j*8</span>
<span class="s2">//      dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_pu8</span>
<span class="s1">FORCE_INLINE __m64 _mm_min_pu8(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_u8(</span>
        <span class="s1">vmin_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed unsigned 8-bit integers in a and b, and store packed minimum</span>
<span class="s2">// values in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//      i := j*8</span>
<span class="s2">//      dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_pu8</span>
<span class="s0">#define </span><span class="s1">_m_pminub(a, b) _mm_min_pu8(a, b)</span>

<span class="s2">// Computes the maximum of the two lower scalar single-precision floating point</span>
<span class="s2">// values of a and b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/s6db5esz(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_max_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32_t value = vgetq_lane_f32(_mm_max_ps(a, b), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vsetq_lane_f32(value, vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Computes the minimum of the two lower scalar single-precision floating point</span>
<span class="s2">// values of a and b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/0a9y7xaa(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_min_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">float32_t value = vgetq_lane_f32(_mm_min_ps(a, b), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vsetq_lane_f32(value, vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Computes the pairwise maxima of the 16 unsigned 8-bit integers from a and the</span>
<span class="s2">// 16 unsigned 8-bit integers from b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/st6634za(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_max_epu8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vmaxq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the pairwise minima of the 16 unsigned 8-bit integers from a and the</span>
<span class="s2">// 16 unsigned 8-bit integers from b.</span>
<span class="s2">// https://msdn.microsoft.com/ko-kr/library/17k8cf58(v=vs.100).aspxx</span>
<span class="s1">FORCE_INLINE __m128i _mm_min_epu8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vminq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the pairwise minima of the 8 signed 16-bit integers from a and the 8</span>
<span class="s2">// signed 16-bit integers from b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/6te997ew(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_min_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vminq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed signed 8-bit integers in a and b, and store packed maximum</span>
<span class="s2">// values in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epi8</span>
<span class="s1">FORCE_INLINE __m128i _mm_max_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vmaxq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed unsigned 16-bit integers in a and b, and store packed maximum</span>
<span class="s2">// values in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epu16</span>
<span class="s1">FORCE_INLINE __m128i _mm_max_epu16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(</span>
        <span class="s1">vmaxq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed signed 8-bit integers in a and b, and store packed minimum</span>
<span class="s2">// values in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_epi8</span>
<span class="s1">FORCE_INLINE __m128i _mm_min_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vminq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed unsigned 16-bit integers in a and b, and store packed minimum</span>
<span class="s2">// values in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_min_epu16</span>
<span class="s1">FORCE_INLINE __m128i _mm_min_epu16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(</span>
        <span class="s1">vminq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the pairwise maxima of the 8 signed 16-bit integers from a and the 8</span>
<span class="s2">// signed 16-bit integers from b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/LIBRary/3x060h7c(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_max_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vmaxq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// epi versions of min/max</span>
<span class="s2">// Computes the pariwise maximums of the four signed 32-bit integer values of a</span>
<span class="s2">// and b.</span>
<span class="s2">//</span>
<span class="s2">// A 128-bit parameter that can be defined with the following equations:</span>
<span class="s2">//   r0 := (a0 &gt; b0) ? a0 : b0</span>
<span class="s2">//   r1 := (a1 &gt; b1) ? a1 : b1</span>
<span class="s2">//   r2 := (a2 &gt; b2) ? a2 : b2</span>
<span class="s2">//   r3 := (a3 &gt; b3) ? a3 : b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/bb514055(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_max_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vmaxq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes the pariwise minima of the four signed 32-bit integer values of a</span>
<span class="s2">// and b.</span>
<span class="s2">//</span>
<span class="s2">// A 128-bit parameter that can be defined with the following equations:</span>
<span class="s2">//   r0 := (a0 &lt; b0) ? a0 : b0</span>
<span class="s2">//   r1 := (a1 &lt; b1) ? a1 : b1</span>
<span class="s2">//   r2 := (a2 &lt; b2) ? a2 : b2</span>
<span class="s2">//   r3 := (a3 &lt; b3) ? a3 : b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/bb531476(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_min_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vminq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed unsigned 32-bit integers in a and b, and store packed maximum</span>
<span class="s2">// values in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epu32</span>
<span class="s1">FORCE_INLINE __m128i _mm_max_epu32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(</span>
        <span class="s1">vmaxq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed unsigned 32-bit integers in a and b, and store packed minimum</span>
<span class="s2">// values in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_max_epu32</span>
<span class="s1">FORCE_INLINE __m128i _mm_min_epu32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(</span>
        <span class="s1">vminq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Multiply the packed unsigned 16-bit integers in a and b, producing</span>
<span class="s2">// intermediate 32-bit integers, and store the high 16 bits of the intermediate</span>
<span class="s2">// integers in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mulhi_pu16</span>
<span class="s1">FORCE_INLINE __m64 _mm_mulhi_pu16(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_u16(vshrn_n_u32(</span>
        <span class="s1">vmull_u16(vreinterpret_u16_m64(a), vreinterpret_u16_m64(b)), </span><span class="s3">16</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Multiplies the 8 signed 16-bit integers from a by the 8 signed 16-bit</span>
<span class="s2">// integers from b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 * b0)[31:16]</span>
<span class="s2">//   r1 := (a1 * b1)[31:16]</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := (a7 * b7)[31:16]</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/59hddw1d(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_mulhi_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s2">/* FIXME: issue with large values because of result saturation */</span>
    <span class="s2">// int16x8_t ret = vqdmulhq_s16(vreinterpretq_s16_m128i(a),</span>
    <span class="s2">// vreinterpretq_s16_m128i(b)); /* =2*a*b */ return</span>
    <span class="s2">// vreinterpretq_m128i_s16(vshrq_n_s16(ret, 1));</span>
    <span class="s1">int16x4_t a3210 = vget_low_s16(vreinterpretq_s16_m128i(a));</span>
    <span class="s1">int16x4_t b3210 = vget_low_s16(vreinterpretq_s16_m128i(b));</span>
    <span class="s1">int32x4_t ab3210 = vmull_s16(a3210, b3210); </span><span class="s2">/* 3333222211110000 */</span>
    <span class="s1">int16x4_t a7654 = vget_high_s16(vreinterpretq_s16_m128i(a));</span>
    <span class="s1">int16x4_t b7654 = vget_high_s16(vreinterpretq_s16_m128i(b));</span>
    <span class="s1">int32x4_t ab7654 = vmull_s16(a7654, b7654); </span><span class="s2">/* 7777666655554444 */</span>
    <span class="s1">uint16x8x2_t r =</span>
        <span class="s1">vuzpq_u16(vreinterpretq_u16_s32(ab3210), vreinterpretq_u16_s32(ab7654));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(r.val[</span><span class="s3">1</span><span class="s1">]);</span>
<span class="s1">}</span>

<span class="s2">// Multiply the packed unsigned 16-bit integers in a and b, producing</span>
<span class="s2">// intermediate 32-bit integers, and store the high 16 bits of the intermediate</span>
<span class="s2">// integers in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_mulhi_epu16</span>
<span class="s1">FORCE_INLINE __m128i _mm_mulhi_epu16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">uint16x4_t a3210 = vget_low_u16(vreinterpretq_u16_m128i(a));</span>
    <span class="s1">uint16x4_t b3210 = vget_low_u16(vreinterpretq_u16_m128i(b));</span>
    <span class="s1">uint32x4_t ab3210 = vmull_u16(a3210, b3210);</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">uint32x4_t ab7654 =</span>
        <span class="s1">vmull_high_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b));</span>
    <span class="s1">uint16x8_t r = vuzp2q_u16(vreinterpretq_u16_u32(ab3210),</span>
                              <span class="s1">vreinterpretq_u16_u32(ab7654));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(r);</span>
<span class="s0">#else</span>
    <span class="s1">uint16x4_t a7654 = vget_high_u16(vreinterpretq_u16_m128i(a));</span>
    <span class="s1">uint16x4_t b7654 = vget_high_u16(vreinterpretq_u16_m128i(b));</span>
    <span class="s1">uint32x4_t ab7654 = vmull_u16(a7654, b7654);</span>
    <span class="s1">uint16x8x2_t r =</span>
        <span class="s1">vuzpq_u16(vreinterpretq_u16_u32(ab3210), vreinterpretq_u16_u32(ab7654));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(r.val[</span><span class="s3">1</span><span class="s1">]);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Computes pairwise add of each argument as single-precision, floating-point</span>
<span class="s2">// values a and b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/yd9wecaa.aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_hadd_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vpaddq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s0">#else</span>
    <span class="s1">float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s1">float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vcombine_f32(vpadd_f32(a10, a32), vpadd_f32(b10, b32)));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Computes pairwise add of each argument as a 16-bit signed or unsigned integer</span>
<span class="s2">// values a and b.</span>
<span class="s1">FORCE_INLINE __m128i _mm_hadd_epi16(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
    <span class="s1">int16x8_t a = vreinterpretq_s16_m128i(_a);</span>
    <span class="s1">int16x8_t b = vreinterpretq_s16_m128i(_b);</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vpaddq_s16(a, b));</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vcombine_s16(vpadd_s16(vget_low_s16(a), vget_high_s16(a)),</span>
                     <span class="s1">vpadd_s16(vget_low_s16(b), vget_high_s16(b))));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Horizontally substract adjacent pairs of single-precision (32-bit)</span>
<span class="s2">// floating-point elements in a and b, and pack the results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_hsub_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_hsub_ps(__m128 _a, __m128 _b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vsubq_f32(</span>
        <span class="s1">vuzp1q_f32(vreinterpretq_f32_m128(_a), vreinterpretq_f32_m128(_b)),</span>
        <span class="s1">vuzp2q_f32(vreinterpretq_f32_m128(_a), vreinterpretq_f32_m128(_b))));</span>
<span class="s0">#else</span>
    <span class="s1">float32x4x2_t c =</span>
        <span class="s1">vuzpq_f32(vreinterpretq_f32_m128(_a), vreinterpretq_f32_m128(_b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vsubq_f32(c.val[</span><span class="s3">0</span><span class="s1">], c.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Horizontally add adjacent pairs of 16-bit integers in a and b, and pack the</span>
<span class="s2">// signed 16-bit results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_hadd_pi16</span>
<span class="s1">FORCE_INLINE __m64 _mm_hadd_pi16(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s16(</span>
        <span class="s1">vpadd_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Horizontally add adjacent pairs of 32-bit integers in a and b, and pack the</span>
<span class="s2">// signed 32-bit results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_hadd_pi32</span>
<span class="s1">FORCE_INLINE __m64 _mm_hadd_pi32(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s32(</span>
        <span class="s1">vpadd_s32(vreinterpret_s32_m64(a), vreinterpret_s32_m64(b)));</span>
<span class="s1">}</span>

<span class="s2">// Computes pairwise difference of each argument as a 16-bit signed or unsigned</span>
<span class="s2">// integer values a and b.</span>
<span class="s1">FORCE_INLINE __m128i _mm_hsub_epi16(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
    <span class="s1">int32x4_t a = vreinterpretq_s32_m128i(_a);</span>
    <span class="s1">int32x4_t b = vreinterpretq_s32_m128i(_b);</span>
    <span class="s2">// Interleave using vshrn/vmovn</span>
    <span class="s2">// [a0|a2|a4|a6|b0|b2|b4|b6]</span>
    <span class="s2">// [a1|a3|a5|a7|b1|b3|b5|b7]</span>
    <span class="s1">int16x8_t ab0246 = vcombine_s16(vmovn_s32(a), vmovn_s32(b));</span>
    <span class="s1">int16x8_t ab1357 = vcombine_s16(vshrn_n_s32(a, </span><span class="s3">16</span><span class="s1">), vshrn_n_s32(b, </span><span class="s3">16</span><span class="s1">));</span>
    <span class="s2">// Subtract</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vsubq_s16(ab0246, ab1357));</span>
<span class="s1">}</span>

<span class="s2">// Computes saturated pairwise sub of each argument as a 16-bit signed</span>
<span class="s2">// integer values a and b.</span>
<span class="s1">FORCE_INLINE __m128i _mm_hadds_epi16(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">int16x8_t a = vreinterpretq_s16_m128i(_a);</span>
    <span class="s1">int16x8_t b = vreinterpretq_s16_m128i(_b);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_s64_s16(</span>
        <span class="s1">vqaddq_s16(vuzp1q_s16(a, b), vuzp2q_s16(a, b)));</span>
<span class="s0">#else</span>
    <span class="s1">int32x4_t a = vreinterpretq_s32_m128i(_a);</span>
    <span class="s1">int32x4_t b = vreinterpretq_s32_m128i(_b);</span>
    <span class="s2">// Interleave using vshrn/vmovn</span>
    <span class="s2">// [a0|a2|a4|a6|b0|b2|b4|b6]</span>
    <span class="s2">// [a1|a3|a5|a7|b1|b3|b5|b7]</span>
    <span class="s1">int16x8_t ab0246 = vcombine_s16(vmovn_s32(a), vmovn_s32(b));</span>
    <span class="s1">int16x8_t ab1357 = vcombine_s16(vshrn_n_s32(a, </span><span class="s3">16</span><span class="s1">), vshrn_n_s32(b, </span><span class="s3">16</span><span class="s1">));</span>
    <span class="s2">// Saturated add</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vqaddq_s16(ab0246, ab1357));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Computes saturated pairwise difference of each argument as a 16-bit signed</span>
<span class="s2">// integer values a and b.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_hsubs_epi16</span>
<span class="s1">FORCE_INLINE __m128i _mm_hsubs_epi16(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">int16x8_t a = vreinterpretq_s16_m128i(_a);</span>
    <span class="s1">int16x8_t b = vreinterpretq_s16_m128i(_b);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_s64_s16(</span>
        <span class="s1">vqsubq_s16(vuzp1q_s16(a, b), vuzp2q_s16(a, b)));</span>
<span class="s0">#else</span>
    <span class="s1">int32x4_t a = vreinterpretq_s32_m128i(_a);</span>
    <span class="s1">int32x4_t b = vreinterpretq_s32_m128i(_b);</span>
    <span class="s2">// Interleave using vshrn/vmovn</span>
    <span class="s2">// [a0|a2|a4|a6|b0|b2|b4|b6]</span>
    <span class="s2">// [a1|a3|a5|a7|b1|b3|b5|b7]</span>
    <span class="s1">int16x8_t ab0246 = vcombine_s16(vmovn_s32(a), vmovn_s32(b));</span>
    <span class="s1">int16x8_t ab1357 = vcombine_s16(vshrn_n_s32(a, </span><span class="s3">16</span><span class="s1">), vshrn_n_s32(b, </span><span class="s3">16</span><span class="s1">));</span>
    <span class="s2">// Saturated subtract</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vqsubq_s16(ab0246, ab1357));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Computes pairwise add of each argument as a 32-bit signed or unsigned integer</span>
<span class="s2">// values a and b.</span>
<span class="s1">FORCE_INLINE __m128i _mm_hadd_epi32(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
    <span class="s1">int32x4_t a = vreinterpretq_s32_m128i(_a);</span>
    <span class="s1">int32x4_t b = vreinterpretq_s32_m128i(_b);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vcombine_s32(vpadd_s32(vget_low_s32(a), vget_high_s32(a)),</span>
                     <span class="s1">vpadd_s32(vget_low_s32(b), vget_high_s32(b))));</span>
<span class="s1">}</span>

<span class="s2">// Computes pairwise difference of each argument as a 32-bit signed or unsigned</span>
<span class="s2">// integer values a and b.</span>
<span class="s1">FORCE_INLINE __m128i _mm_hsub_epi32(__m128i _a, __m128i _b)</span>
<span class="s1">{</span>
    <span class="s1">int64x2_t a = vreinterpretq_s64_m128i(_a);</span>
    <span class="s1">int64x2_t b = vreinterpretq_s64_m128i(_b);</span>
    <span class="s2">// Interleave using vshrn/vmovn</span>
    <span class="s2">// [a0|a2|b0|b2]</span>
    <span class="s2">// [a1|a2|b1|b3]</span>
    <span class="s1">int32x4_t ab02 = vcombine_s32(vmovn_s64(a), vmovn_s64(b));</span>
    <span class="s1">int32x4_t ab13 = vcombine_s32(vshrn_n_s64(a, </span><span class="s3">32</span><span class="s1">), vshrn_n_s64(b, </span><span class="s3">32</span><span class="s1">));</span>
    <span class="s2">// Subtract</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vsubq_s32(ab02, ab13));</span>
<span class="s1">}</span>

<span class="s2">// Kahan summation for accurate summation of floating-point numbers.</span>
<span class="s2">// http://blog.zachbjornson.com/2019/08/11/fast-float-summation.html</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">sse2neon_kadd_f32(</span><span class="s0">float </span><span class="s1">*sum, </span><span class="s0">float </span><span class="s1">*c, </span><span class="s0">float </span><span class="s1">y)</span>
<span class="s1">{</span>
    <span class="s1">y -= *c;</span>
    <span class="s0">float </span><span class="s1">t = *sum + y;</span>
    <span class="s1">*c = (t - *sum) - y;</span>
    <span class="s1">*sum = t;</span>
<span class="s1">}</span>

<span class="s2">// Conditionally multiply the packed single-precision (32-bit) floating-point</span>
<span class="s2">// elements in a and b using the high 4 bits in imm8, sum the four products,</span>
<span class="s2">// and conditionally store the sum in dst using the low 4 bits of imm.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_dp_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_dp_ps(__m128 a, __m128 b, </span><span class="s0">const int </span><span class="s1">imm)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s2">/* shortcuts */</span>
    <span class="s0">if </span><span class="s1">(imm == </span><span class="s3">0xFF</span><span class="s1">) {</span>
        <span class="s0">return </span><span class="s1">_mm_set1_ps(vaddvq_f32(_mm_mul_ps(a, b)));</span>
    <span class="s1">}</span>
    <span class="s0">if </span><span class="s1">(imm == </span><span class="s3">0x7F</span><span class="s1">) {</span>
        <span class="s1">float32x4_t m = _mm_mul_ps(a, b);</span>
        <span class="s1">m[</span><span class="s3">3</span><span class="s1">] = </span><span class="s3">0</span><span class="s1">;</span>
        <span class="s0">return </span><span class="s1">_mm_set1_ps(vaddvq_f32(m));</span>
    <span class="s1">}</span>
<span class="s0">#endif</span>

    <span class="s0">float </span><span class="s1">s = </span><span class="s3">0</span><span class="s1">, c = </span><span class="s3">0</span><span class="s1">;</span>
    <span class="s1">float32x4_t f32a = vreinterpretq_f32_m128(a);</span>
    <span class="s1">float32x4_t f32b = vreinterpretq_f32_m128(b);</span>

    <span class="s2">/* To improve the accuracy of floating-point summation, Kahan algorithm 
     * is used for each operation. 
     */</span>
    <span class="s0">if </span><span class="s1">(imm &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">4</span><span class="s1">))</span>
        <span class="s1">sse2neon_kadd_f32(&amp;s, &amp;c, f32a[</span><span class="s3">0</span><span class="s1">] * f32b[</span><span class="s3">0</span><span class="s1">]);</span>
    <span class="s0">if </span><span class="s1">(imm &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">5</span><span class="s1">))</span>
        <span class="s1">sse2neon_kadd_f32(&amp;s, &amp;c, f32a[</span><span class="s3">1</span><span class="s1">] * f32b[</span><span class="s3">1</span><span class="s1">]);</span>
    <span class="s0">if </span><span class="s1">(imm &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">6</span><span class="s1">))</span>
        <span class="s1">sse2neon_kadd_f32(&amp;s, &amp;c, f32a[</span><span class="s3">2</span><span class="s1">] * f32b[</span><span class="s3">2</span><span class="s1">]);</span>
    <span class="s0">if </span><span class="s1">(imm &amp; (</span><span class="s3">1 </span><span class="s1">&lt;&lt; </span><span class="s3">7</span><span class="s1">))</span>
        <span class="s1">sse2neon_kadd_f32(&amp;s, &amp;c, f32a[</span><span class="s3">3</span><span class="s1">] * f32b[</span><span class="s3">3</span><span class="s1">]);</span>
    <span class="s1">s += c;</span>

    <span class="s1">float32x4_t res = {</span>
        <span class="s1">(imm &amp; </span><span class="s3">0x1</span><span class="s1">) ? s : </span><span class="s3">0</span><span class="s1">,</span>
        <span class="s1">(imm &amp; </span><span class="s3">0x2</span><span class="s1">) ? s : </span><span class="s3">0</span><span class="s1">,</span>
        <span class="s1">(imm &amp; </span><span class="s3">0x4</span><span class="s1">) ? s : </span><span class="s3">0</span><span class="s1">,</span>
        <span class="s1">(imm &amp; </span><span class="s3">0x8</span><span class="s1">) ? s : </span><span class="s3">0</span><span class="s1">,</span>
    <span class="s1">};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(res);</span>
<span class="s1">}</span>

<span class="s2">/* Compare operations */</span>

<span class="s2">// Compares for less than</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/f330yhc8(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmplt_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_u32(</span>
        <span class="s1">vcltq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares for less than</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/fy94wye7(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmplt_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_cmplt_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Compares for greater than.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 &gt; b0) ? 0xffffffff : 0x0</span>
<span class="s2">//   r1 := (a1 &gt; b1) ? 0xffffffff : 0x0</span>
<span class="s2">//   r2 := (a2 &gt; b2) ? 0xffffffff : 0x0</span>
<span class="s2">//   r3 := (a3 &gt; b3) ? 0xffffffff : 0x0</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/11dy102s(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpgt_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_u32(</span>
        <span class="s1">vcgtq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares for greater than.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/1xyyyy9e(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpgt_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_cmpgt_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Compares for greater than or equal.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/fs813y2t(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpge_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_u32(</span>
        <span class="s1">vcgeq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares for greater than or equal.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/kesh3ddc(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpge_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_cmpge_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Compares for less than or equal.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 &lt;= b0) ? 0xffffffff : 0x0</span>
<span class="s2">//   r1 := (a1 &lt;= b1) ? 0xffffffff : 0x0</span>
<span class="s2">//   r2 := (a2 &lt;= b2) ? 0xffffffff : 0x0</span>
<span class="s2">//   r3 := (a3 &lt;= b3) ? 0xffffffff : 0x0</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/1s75w83z(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmple_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_u32(</span>
        <span class="s1">vcleq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares for less than or equal.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/a7x0hbhw(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmple_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_cmple_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Compares for equality.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/36aectz5(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpeq_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_u32(</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares for equality.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/k423z28e(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpeq_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_cmpeq_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Compares for inequality.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/sf44thbx(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpneq_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_u32(vmvnq_u32(</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));</span>
<span class="s1">}</span>

<span class="s2">// Compares for inequality.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/ekya8fh4(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpneq_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_cmpneq_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Compares for not greater than or equal.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/wsexys62(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpnge_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_cmplt_ps(a, b);</span>
<span class="s1">}</span>

<span class="s2">// Compares for not greater than or equal.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/fk2y80s8(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpnge_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_cmplt_ss(a, b);</span>
<span class="s1">}</span>

<span class="s2">// Compares for not greater than.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/d0xh7w0s(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpngt_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_cmple_ps(a, b);</span>
<span class="s1">}</span>

<span class="s2">// Compares for not greater than.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/z7x9ydwh(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpngt_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_cmple_ss(a, b);</span>
<span class="s1">}</span>

<span class="s2">// Compares for not less than or equal.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/6a330kxw(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpnle_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_cmpgt_ps(a, b);</span>
<span class="s1">}</span>

<span class="s2">// Compares for not less than or equal.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/z7x9ydwh(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpnle_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_cmpgt_ss(a, b);</span>
<span class="s1">}</span>

<span class="s2">// Compares for not less than.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/4686bbdw(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpnlt_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_cmpge_ps(a, b);</span>
<span class="s1">}</span>

<span class="s2">// Compares for not less than.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/56b9z2wf(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpnlt_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_cmpge_ss(a, b);</span>
<span class="s1">}</span>

<span class="s2">// Compares the 16 signed or unsigned 8-bit integers in a and the 16 signed or</span>
<span class="s2">// unsigned 8-bit integers in b for equality.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/windows/desktop/bz5xk21a(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmpeq_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vceqq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares the 8 signed or unsigned 16-bit integers in a and the 8 signed or</span>
<span class="s2">// unsigned 16-bit integers in b for equality.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/2ay060te(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmpeq_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(</span>
        <span class="s1">vceqq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed 32-bit integers in a and b for equality, and store the results</span>
<span class="s2">// in dst</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmpeq_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(</span>
        <span class="s1">vceqq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compare packed 64-bit integers in a and b for equality, and store the results</span>
<span class="s2">// in dst</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmpeq_epi64(__m128i a, __m128i b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(</span>
        <span class="s1">vceqq_u64(vreinterpretq_u64_m128i(a), vreinterpretq_u64_m128i(b)));</span>
<span class="s0">#else</span>
    <span class="s2">// ARMv7 lacks vceqq_u64</span>
    <span class="s2">// (a == b) -&gt; (a_lo == b_lo) &amp;&amp; (a_hi == b_hi)</span>
    <span class="s1">uint32x4_t cmp =</span>
        <span class="s1">vceqq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b));</span>
    <span class="s1">uint32x4_t swapped = vrev64q_u32(cmp);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(vandq_u32(cmp, swapped));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Compares the 16 signed 8-bit integers in a and the 16 signed 8-bit integers</span>
<span class="s2">// in b for lesser than.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/windows/desktop/9s46csht(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmplt_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vcltq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares the 16 signed 8-bit integers in a and the 16 signed 8-bit integers</span>
<span class="s2">// in b for greater than.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 &gt; b0) ? 0xff : 0x0</span>
<span class="s2">//   r1 := (a1 &gt; b1) ? 0xff : 0x0</span>
<span class="s2">//   ...</span>
<span class="s2">//   r15 := (a15 &gt; b15) ? 0xff : 0x0</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/zh-tw/library/wf45zt2b(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmpgt_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vcgtq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares the 8 signed 16-bit integers in a and the 8 signed 16-bit integers</span>
<span class="s2">// in b for less than.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 &lt; b0) ? 0xffff : 0x0</span>
<span class="s2">//   r1 := (a1 &lt; b1) ? 0xffff : 0x0</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := (a7 &lt; b7) ? 0xffff : 0x0</span>
<span class="s2">//</span>
<span class="s2">// https://technet.microsoft.com/en-us/library/t863edb2(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmplt_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(</span>
        <span class="s1">vcltq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares the 8 signed 16-bit integers in a and the 8 signed 16-bit integers</span>
<span class="s2">// in b for greater than.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (a0 &gt; b0) ? 0xffff : 0x0</span>
<span class="s2">//   r1 := (a1 &gt; b1) ? 0xffff : 0x0</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := (a7 &gt; b7) ? 0xffff : 0x0</span>
<span class="s2">//</span>
<span class="s2">// https://technet.microsoft.com/en-us/library/xd43yfsa(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmpgt_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(</span>
        <span class="s1">vcgtq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s1">}</span>


<span class="s2">// Compares the 4 signed 32-bit integers in a and the 4 signed 32-bit integers</span>
<span class="s2">// in b for less than.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/4ak0bf5d(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmplt_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(</span>
        <span class="s1">vcltq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares the 4 signed 32-bit integers in a and the 4 signed 32-bit integers</span>
<span class="s2">// in b for greater than.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/1s9f2z0y(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmpgt_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(</span>
        <span class="s1">vcgtq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares the 2 signed 64-bit integers in a and the 2 signed 64-bit integers</span>
<span class="s2">// in b for greater than.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cmpgt_epi64(__m128i a, __m128i b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(</span>
        <span class="s1">vcgtq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));</span>
<span class="s0">#else</span>
    <span class="s2">// ARMv7 lacks vcgtq_s64.</span>
    <span class="s2">// This is based off of Clang's SSE2 polyfill:</span>
    <span class="s2">// (a &gt; b) -&gt; ((a_hi &gt; b_hi) || (a_lo &gt; b_lo &amp;&amp; a_hi == b_hi))</span>

    <span class="s2">// Mask the sign bit out since we need a signed AND an unsigned comparison</span>
    <span class="s2">// and it is ugly to try and split them.</span>
    <span class="s1">int32x4_t mask = vreinterpretq_s32_s64(vdupq_n_s64(</span><span class="s3">0x80000000</span><span class="s1">ull));</span>
    <span class="s1">int32x4_t a_mask = veorq_s32(vreinterpretq_s32_m128i(a), mask);</span>
    <span class="s1">int32x4_t b_mask = veorq_s32(vreinterpretq_s32_m128i(b), mask);</span>
    <span class="s2">// Check if a &gt; b</span>
    <span class="s1">int64x2_t greater = vreinterpretq_s64_u32(vcgtq_s32(a_mask, b_mask));</span>
    <span class="s2">// Copy upper mask to lower mask</span>
    <span class="s2">// a_hi &gt; b_hi</span>
    <span class="s1">int64x2_t gt_hi = vshrq_n_s64(greater, </span><span class="s3">63</span><span class="s1">);</span>
    <span class="s2">// Copy lower mask to upper mask</span>
    <span class="s2">// a_lo &gt; b_lo</span>
    <span class="s1">int64x2_t gt_lo = vsliq_n_s64(greater, greater, </span><span class="s3">32</span><span class="s1">);</span>
    <span class="s2">// Compare for equality</span>
    <span class="s1">int64x2_t equal = vreinterpretq_s64_u32(vceqq_s32(a_mask, b_mask));</span>
    <span class="s2">// Copy upper mask to lower mask</span>
    <span class="s2">// a_hi == b_hi</span>
    <span class="s1">int64x2_t eq_hi = vshrq_n_s64(equal, </span><span class="s3">63</span><span class="s1">);</span>
    <span class="s2">// a_hi &gt; b_hi || (a_lo &gt; b_lo &amp;&amp; a_hi == b_hi)</span>
    <span class="s1">int64x2_t ret = vorrq_s64(gt_hi, vandq_s64(gt_lo, eq_hi));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(ret);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Compares the four 32-bit floats in a and b to check if any values are NaN.</span>
<span class="s2">// Ordered compare between each value returns true for &quot;orderable&quot; and false for</span>
<span class="s2">// &quot;not orderable&quot; (NaN).</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/0h9w00fx(v=vs.100).aspx see</span>
<span class="s2">// also:</span>
<span class="s2">// http://stackoverflow.com/questions/8627331/what-does-ordered-unordered-comparison-mean</span>
<span class="s2">// http://stackoverflow.com/questions/29349621/neon-isnanval-intrinsics</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpord_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s2">// Note: NEON does not have ordered compare builtin</span>
    <span class="s2">// Need to compare a eq a and b eq b to check for NaN</span>
    <span class="s2">// Do AND of results to get final</span>
    <span class="s1">uint32x4_t ceqaa =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));</span>
    <span class="s1">uint32x4_t ceqbb =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_u32(vandq_u32(ceqaa, ceqbb));</span>
<span class="s1">}</span>

<span class="s2">// Compares for ordered.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/343t62da(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpord_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_cmpord_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Compares for unordered.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/khy6fk1t(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpunord_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">uint32x4_t f32a =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));</span>
    <span class="s1">uint32x4_t f32b =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_u32(vmvnq_u32(vandq_u32(f32a, f32b)));</span>
<span class="s1">}</span>

<span class="s2">// Compares for unordered.</span>
<span class="s2">// https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/2as2387b(v=vs.100)</span>
<span class="s1">FORCE_INLINE __m128 _mm_cmpunord_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_move_ss(a, _mm_cmpunord_ps(a, b));</span>
<span class="s1">}</span>

<span class="s2">// Compares the lower single-precision floating point scalar values of a and b</span>
<span class="s2">// using a less than operation. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/2kwe606b(v=vs.90).aspx Important</span>
<span class="s2">// note!! The documentation on MSDN is incorrect!  If either of the values is a</span>
<span class="s2">// NAN the docs say you will get a one, but in fact, it will return a zero!!</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_comilt_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s1">uint32x4_t a_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));</span>
    <span class="s1">uint32x4_t b_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));</span>
    <span class="s1">uint32x4_t a_and_b_not_nan = vandq_u32(a_not_nan, b_not_nan);</span>
    <span class="s1">uint32x4_t a_lt_b =</span>
        <span class="s1">vcltq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">(vgetq_lane_u32(vandq_u32(a_and_b_not_nan, a_lt_b), </span><span class="s3">0</span><span class="s1">) != </span><span class="s3">0</span><span class="s1">) ? </span><span class="s3">1 </span><span class="s1">: </span><span class="s3">0</span><span class="s1">;</span>
<span class="s1">}</span>

<span class="s2">// Compares the lower single-precision floating point scalar values of a and b</span>
<span class="s2">// using a greater than operation. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/b0738e0t(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_comigt_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s2">// return vgetq_lane_u32(vcgtq_f32(vreinterpretq_f32_m128(a),</span>
    <span class="s2">// vreinterpretq_f32_m128(b)), 0);</span>
    <span class="s1">uint32x4_t a_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));</span>
    <span class="s1">uint32x4_t b_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));</span>
    <span class="s1">uint32x4_t a_and_b_not_nan = vandq_u32(a_not_nan, b_not_nan);</span>
    <span class="s1">uint32x4_t a_gt_b =</span>
        <span class="s1">vcgtq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">(vgetq_lane_u32(vandq_u32(a_and_b_not_nan, a_gt_b), </span><span class="s3">0</span><span class="s1">) != </span><span class="s3">0</span><span class="s1">) ? </span><span class="s3">1 </span><span class="s1">: </span><span class="s3">0</span><span class="s1">;</span>
<span class="s1">}</span>

<span class="s2">// Compares the lower single-precision floating point scalar values of a and b</span>
<span class="s2">// using a less than or equal operation. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/1w4t7c57(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_comile_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s2">// return vgetq_lane_u32(vcleq_f32(vreinterpretq_f32_m128(a),</span>
    <span class="s2">// vreinterpretq_f32_m128(b)), 0);</span>
    <span class="s1">uint32x4_t a_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));</span>
    <span class="s1">uint32x4_t b_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));</span>
    <span class="s1">uint32x4_t a_and_b_not_nan = vandq_u32(a_not_nan, b_not_nan);</span>
    <span class="s1">uint32x4_t a_le_b =</span>
        <span class="s1">vcleq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">(vgetq_lane_u32(vandq_u32(a_and_b_not_nan, a_le_b), </span><span class="s3">0</span><span class="s1">) != </span><span class="s3">0</span><span class="s1">) ? </span><span class="s3">1 </span><span class="s1">: </span><span class="s3">0</span><span class="s1">;</span>
<span class="s1">}</span>

<span class="s2">// Compares the lower single-precision floating point scalar values of a and b</span>
<span class="s2">// using a greater than or equal operation. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/8t80des6(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_comige_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s2">// return vgetq_lane_u32(vcgeq_f32(vreinterpretq_f32_m128(a),</span>
    <span class="s2">// vreinterpretq_f32_m128(b)), 0);</span>
    <span class="s1">uint32x4_t a_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));</span>
    <span class="s1">uint32x4_t b_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));</span>
    <span class="s1">uint32x4_t a_and_b_not_nan = vandq_u32(a_not_nan, b_not_nan);</span>
    <span class="s1">uint32x4_t a_ge_b =</span>
        <span class="s1">vcgeq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">(vgetq_lane_u32(vandq_u32(a_and_b_not_nan, a_ge_b), </span><span class="s3">0</span><span class="s1">) != </span><span class="s3">0</span><span class="s1">) ? </span><span class="s3">1 </span><span class="s1">: </span><span class="s3">0</span><span class="s1">;</span>
<span class="s1">}</span>

<span class="s2">// Compares the lower single-precision floating point scalar values of a and b</span>
<span class="s2">// using an equality operation. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/93yx2h2b(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_comieq_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s2">// return vgetq_lane_u32(vceqq_f32(vreinterpretq_f32_m128(a),</span>
    <span class="s2">// vreinterpretq_f32_m128(b)), 0);</span>
    <span class="s1">uint32x4_t a_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));</span>
    <span class="s1">uint32x4_t b_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));</span>
    <span class="s1">uint32x4_t a_and_b_not_nan = vandq_u32(a_not_nan, b_not_nan);</span>
    <span class="s1">uint32x4_t a_eq_b =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));</span>
    <span class="s0">return </span><span class="s1">(vgetq_lane_u32(vandq_u32(a_and_b_not_nan, a_eq_b), </span><span class="s3">0</span><span class="s1">) != </span><span class="s3">0</span><span class="s1">) ? </span><span class="s3">1 </span><span class="s1">: </span><span class="s3">0</span><span class="s1">;</span>
<span class="s1">}</span>

<span class="s2">// Compares the lower single-precision floating point scalar values of a and b</span>
<span class="s2">// using an inequality operation. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bafh5e0a(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_comineq_ss(__m128 a, __m128 b)</span>
<span class="s1">{</span>
    <span class="s2">// return !vgetq_lane_u32(vceqq_f32(vreinterpretq_f32_m128(a),</span>
    <span class="s2">// vreinterpretq_f32_m128(b)), 0);</span>
    <span class="s1">uint32x4_t a_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));</span>
    <span class="s1">uint32x4_t b_not_nan =</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));</span>
    <span class="s1">uint32x4_t a_or_b_nan = vmvnq_u32(vandq_u32(a_not_nan, b_not_nan));</span>
    <span class="s1">uint32x4_t a_neq_b = vmvnq_u32(</span>
        <span class="s1">vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
    <span class="s0">return </span><span class="s1">(vgetq_lane_u32(vorrq_u32(a_or_b_nan, a_neq_b), </span><span class="s3">0</span><span class="s1">) != </span><span class="s3">0</span><span class="s1">) ? </span><span class="s3">1 </span><span class="s1">: </span><span class="s3">0</span><span class="s1">;</span>
<span class="s1">}</span>

<span class="s2">// according to the documentation, these intrinsics behave the same as the</span>
<span class="s2">// non-'u' versions.  We'll just alias them here.</span>
<span class="s0">#define </span><span class="s1">_mm_ucomilt_ss _mm_comilt_ss</span>
<span class="s0">#define </span><span class="s1">_mm_ucomile_ss _mm_comile_ss</span>
<span class="s0">#define </span><span class="s1">_mm_ucomigt_ss _mm_comigt_ss</span>
<span class="s0">#define </span><span class="s1">_mm_ucomige_ss _mm_comige_ss</span>
<span class="s0">#define </span><span class="s1">_mm_ucomieq_ss _mm_comieq_ss</span>
<span class="s0">#define </span><span class="s1">_mm_ucomineq_ss _mm_comineq_ss</span>

<span class="s2">/* Conversions */</span>

<span class="s2">// Convert packed signed 32-bit integers in b to packed single-precision</span>
<span class="s2">// (32-bit) floating-point elements, store the results in the lower 2 elements</span>
<span class="s2">// of dst, and copy the upper 2 packed elements from a to the upper elements of</span>
<span class="s2">// dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_Int32_To_FP32(b[31:0])</span>
<span class="s2">//   dst[63:32] := Convert_Int32_To_FP32(b[63:32])</span>
<span class="s2">//   dst[95:64] := a[95:64]</span>
<span class="s2">//   dst[127:96] := a[127:96]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvt_pi2ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvt_pi2ps(__m128 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vcombine_f32(vcvt_f32_s32(vreinterpret_s32_m64(b)),</span>
                     <span class="s1">vget_high_f32(vreinterpretq_f32_m128(a))));</span>
<span class="s1">}</span>

<span class="s2">// Convert the signed 32-bit integer b to a single-precision (32-bit)</span>
<span class="s2">// floating-point element, store the result in the lower element of dst, and</span>
<span class="s2">// copy the upper 3 packed elements from a to the upper elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_Int32_To_FP32(b[31:0])</span>
<span class="s2">//   dst[127:32] := a[127:32]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvt_si2ss</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvt_si2ss(__m128 a, </span><span class="s0">int </span><span class="s1">b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vsetq_lane_f32((</span><span class="s0">float</span><span class="s1">) b, vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Convert the signed 32-bit integer b to a single-precision (32-bit)</span>
<span class="s2">// floating-point element, store the result in the lower element of dst, and</span>
<span class="s2">// copy the upper 3 packed elements from a to the upper elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_Int32_To_FP32(b[31:0])</span>
<span class="s2">//   dst[127:32] := a[127:32]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtsi32_ss</span>
<span class="s0">#define </span><span class="s1">_mm_cvtsi32_ss(a, b) _mm_cvt_si2ss(a, b)</span>

<span class="s2">// Convert the signed 64-bit integer b to a single-precision (32-bit)</span>
<span class="s2">// floating-point element, store the result in the lower element of dst, and</span>
<span class="s2">// copy the upper 3 packed elements from a to the upper elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_Int64_To_FP32(b[63:0])</span>
<span class="s2">//   dst[127:32] := a[127:32]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtsi64_ss</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtsi64_ss(__m128 a, int64_t b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vsetq_lane_f32((</span><span class="s0">float</span><span class="s1">) b, vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Convert the lower single-precision (32-bit) floating-point element in a to a</span>
<span class="s2">// 32-bit integer, and store the result in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvt_ss2si</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_cvt_ss2si(__m128 a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_s32(vcvtnq_s32_f32(vreinterpretq_f32_m128(a)), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s0">#else</span>
    <span class="s1">float32_t data = vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">float32_t diff = data - floor(data);</span>
    <span class="s0">if </span><span class="s1">(diff &gt; </span><span class="s3">0.5</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">(int32_t) ceil(data);</span>
    <span class="s0">if </span><span class="s1">(diff == </span><span class="s3">0.5</span><span class="s1">) {</span>
        <span class="s1">int32_t f = (int32_t) floor(data);</span>
        <span class="s1">int32_t c = (int32_t) ceil(data);</span>
        <span class="s0">return </span><span class="s1">c &amp; </span><span class="s3">1 </span><span class="s1">? f : c;</span>
    <span class="s1">}</span>
    <span class="s0">return </span><span class="s1">(int32_t) floor(data);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Convert packed 16-bit integers in a to packed single-precision (32-bit)</span>
<span class="s2">// floating-point elements, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      m := j*32</span>
<span class="s2">//      dst[m+31:m] := Convert_Int16_To_FP32(a[i+15:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtpi16_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtpi16_ps(__m64 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vcvtq_f32_s32(vmovl_s16(vreinterpret_s16_m64(a))));</span>
<span class="s1">}</span>

<span class="s2">// Convert packed 32-bit integers in b to packed single-precision (32-bit)</span>
<span class="s2">// floating-point elements, store the results in the lower 2 elements of dst,</span>
<span class="s2">// and copy the upper 2 packed elements from a to the upper elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_Int32_To_FP32(b[31:0])</span>
<span class="s2">//   dst[63:32] := Convert_Int32_To_FP32(b[63:32])</span>
<span class="s2">//   dst[95:64] := a[95:64]</span>
<span class="s2">//   dst[127:96] := a[127:96]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtpi32_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtpi32_ps(__m128 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vcombine_f32(vcvt_f32_s32(vreinterpret_s32_m64(b)),</span>
                     <span class="s1">vget_high_f32(vreinterpretq_f32_m128(a))));</span>
<span class="s1">}</span>

<span class="s2">// Convert packed signed 32-bit integers in a to packed single-precision</span>
<span class="s2">// (32-bit) floating-point elements, store the results in the lower 2 elements</span>
<span class="s2">// of dst, then covert the packed signed 32-bit integers in b to</span>
<span class="s2">// single-precision (32-bit) floating-point element, and store the results in</span>
<span class="s2">// the upper 2 elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := Convert_Int32_To_FP32(a[31:0])</span>
<span class="s2">//   dst[63:32] := Convert_Int32_To_FP32(a[63:32])</span>
<span class="s2">//   dst[95:64] := Convert_Int32_To_FP32(b[31:0])</span>
<span class="s2">//   dst[127:96] := Convert_Int32_To_FP32(b[63:32])</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtpi32x2_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtpi32x2_ps(__m64 a, __m64 b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcvtq_f32_s32(</span>
        <span class="s1">vcombine_s32(vreinterpret_s32_m64(a), vreinterpret_s32_m64(b))));</span>
<span class="s1">}</span>

<span class="s2">// Convert the lower packed 8-bit integers in a to packed single-precision</span>
<span class="s2">// (32-bit) floating-point elements, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*8</span>
<span class="s2">//      m := j*32</span>
<span class="s2">//      dst[m+31:m] := Convert_Int8_To_FP32(a[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtpi8_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtpi8_ps(__m64 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcvtq_f32_s32(</span>
        <span class="s1">vmovl_s16(vget_low_s16(vmovl_s8(vreinterpret_s8_m64(a))))));</span>
<span class="s1">}</span>

<span class="s2">// Convert packed unsigned 16-bit integers in a to packed single-precision</span>
<span class="s2">// (32-bit) floating-point elements, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*16</span>
<span class="s2">//      m := j*32</span>
<span class="s2">//      dst[m+31:m] := Convert_UInt16_To_FP32(a[i+15:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtpu16_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtpu16_ps(__m64 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vcvtq_f32_u32(vmovl_u16(vreinterpret_u16_m64(a))));</span>
<span class="s1">}</span>

<span class="s2">// Convert the lower packed unsigned 8-bit integers in a to packed</span>
<span class="s2">// single-precision (32-bit) floating-point elements, and store the results in</span>
<span class="s2">// dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 3</span>
<span class="s2">//      i := j*8</span>
<span class="s2">//      m := j*32</span>
<span class="s2">//      dst[m+31:m] := Convert_UInt8_To_FP32(a[i+7:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtpu8_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtpu8_ps(__m64 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcvtq_f32_u32(</span>
        <span class="s1">vmovl_u16(vget_low_u16(vmovl_u8(vreinterpret_u8_m64(a))))));</span>
<span class="s1">}</span>

<span class="s2">// Converts the four single-precision, floating-point values of a to signed</span>
<span class="s2">// 32-bit integer values using truncate.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/1h005y6x(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvttps_epi32(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a)));</span>
<span class="s1">}</span>

<span class="s2">// Convert the lower double-precision (64-bit) floating-point element in a to a</span>
<span class="s2">// 64-bit integer with truncation, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvttsd_si64</span>
<span class="s1">FORCE_INLINE int64_t _mm_cvttsd_si64(__m128d a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_s64(vcvtq_s64_f64(vreinterpretq_f64_m128d(a)), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s0">#else</span>
    <span class="s0">double </span><span class="s1">ret = *((</span><span class="s0">double </span><span class="s1">*) &amp;a);</span>
    <span class="s0">return </span><span class="s1">(int64_t) ret;</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Convert the lower double-precision (64-bit) floating-point element in a to a</span>
<span class="s2">// 64-bit integer with truncation, and store the result in dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvttsd_si64x</span>
<span class="s0">#define </span><span class="s1">_mm_cvttsd_si64x(a) _mm_cvttsd_si64(a)</span>

<span class="s2">// Converts the four signed 32-bit integer values of a to single-precision,</span>
<span class="s2">// floating-point values</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/36bwxcx5(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtepi32_ps(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcvtq_f32_s32(vreinterpretq_s32_m128i(a)));</span>
<span class="s1">}</span>

<span class="s2">// Converts the four unsigned 8-bit integers in the lower 16 bits to four</span>
<span class="s2">// unsigned 32-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepu8_epi16(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);    </span><span class="s2">/* xxxx xxxx xxxx DCBA */</span>
    <span class="s1">uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16)); </span><span class="s2">/* 0x0x 0x0x 0D0C 0B0A */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(u16x8);</span>
<span class="s1">}</span>

<span class="s2">// Converts the four unsigned 8-bit integers in the lower 32 bits to four</span>
<span class="s2">// unsigned 32-bit integers.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bb531467%28v=vs.100%29.aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepu8_epi32(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);      </span><span class="s2">/* xxxx xxxx xxxx DCBA */</span>
    <span class="s1">uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16));   </span><span class="s2">/* 0x0x 0x0x 0D0C 0B0A */</span>
    <span class="s1">uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); </span><span class="s2">/* 000D 000C 000B 000A */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(u32x4);</span>
<span class="s1">}</span>

<span class="s2">// Converts the two unsigned 8-bit integers in the lower 16 bits to two</span>
<span class="s2">// unsigned 64-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepu8_epi64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);      </span><span class="s2">/* xxxx xxxx xxxx xxBA */</span>
    <span class="s1">uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16));   </span><span class="s2">/* 0x0x 0x0x 0x0x 0B0A */</span>
    <span class="s1">uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); </span><span class="s2">/* 000x 000x 000B 000A */</span>
    <span class="s1">uint64x2_t u64x2 = vmovl_u32(vget_low_u32(u32x4)); </span><span class="s2">/* 0000 000B 0000 000A */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(u64x2);</span>
<span class="s1">}</span>

<span class="s2">// Converts the four unsigned 8-bit integers in the lower 16 bits to four</span>
<span class="s2">// unsigned 32-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepi8_epi16(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int8x16_t s8x16 = vreinterpretq_s8_m128i(a);    </span><span class="s2">/* xxxx xxxx xxxx DCBA */</span>
    <span class="s1">int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16)); </span><span class="s2">/* 0x0x 0x0x 0D0C 0B0A */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(s16x8);</span>
<span class="s1">}</span>

<span class="s2">// Converts the four unsigned 8-bit integers in the lower 32 bits to four</span>
<span class="s2">// unsigned 32-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepi8_epi32(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int8x16_t s8x16 = vreinterpretq_s8_m128i(a);      </span><span class="s2">/* xxxx xxxx xxxx DCBA */</span>
    <span class="s1">int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16));   </span><span class="s2">/* 0x0x 0x0x 0D0C 0B0A */</span>
    <span class="s1">int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); </span><span class="s2">/* 000D 000C 000B 000A */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(s32x4);</span>
<span class="s1">}</span>

<span class="s2">// Converts the two signed 8-bit integers in the lower 32 bits to four</span>
<span class="s2">// signed 64-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepi8_epi64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int8x16_t s8x16 = vreinterpretq_s8_m128i(a);      </span><span class="s2">/* xxxx xxxx xxxx xxBA */</span>
    <span class="s1">int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16));   </span><span class="s2">/* 0x0x 0x0x 0x0x 0B0A */</span>
    <span class="s1">int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); </span><span class="s2">/* 000x 000x 000B 000A */</span>
    <span class="s1">int64x2_t s64x2 = vmovl_s32(vget_low_s32(s32x4)); </span><span class="s2">/* 0000 000B 0000 000A */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(s64x2);</span>
<span class="s1">}</span>

<span class="s2">// Converts the four signed 16-bit integers in the lower 64 bits to four signed</span>
<span class="s2">// 32-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepi16_epi32(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vmovl_s16(vget_low_s16(vreinterpretq_s16_m128i(a))));</span>
<span class="s1">}</span>

<span class="s2">// Converts the two signed 16-bit integers in the lower 32 bits two signed</span>
<span class="s2">// 32-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepi16_epi64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">int16x8_t s16x8 = vreinterpretq_s16_m128i(a);     </span><span class="s2">/* xxxx xxxx xxxx 0B0A */</span>
    <span class="s1">int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); </span><span class="s2">/* 000x 000x 000B 000A */</span>
    <span class="s1">int64x2_t s64x2 = vmovl_s32(vget_low_s32(s32x4)); </span><span class="s2">/* 0000 000B 0000 000A */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(s64x2);</span>
<span class="s1">}</span>

<span class="s2">// Converts the four unsigned 16-bit integers in the lower 64 bits to four</span>
<span class="s2">// unsigned 32-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepu16_epi32(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u32(</span>
        <span class="s1">vmovl_u16(vget_low_u16(vreinterpretq_u16_m128i(a))));</span>
<span class="s1">}</span>

<span class="s2">// Converts the two unsigned 16-bit integers in the lower 32 bits to two</span>
<span class="s2">// unsigned 64-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepu16_epi64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">uint16x8_t u16x8 = vreinterpretq_u16_m128i(a);     </span><span class="s2">/* xxxx xxxx xxxx 0B0A */</span>
    <span class="s1">uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); </span><span class="s2">/* 000x 000x 000B 000A */</span>
    <span class="s1">uint64x2_t u64x2 = vmovl_u32(vget_low_u32(u32x4)); </span><span class="s2">/* 0000 000B 0000 000A */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(u64x2);</span>
<span class="s1">}</span>

<span class="s2">// Converts the two unsigned 32-bit integers in the lower 64 bits to two</span>
<span class="s2">// unsigned 64-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepu32_epi64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(</span>
        <span class="s1">vmovl_u32(vget_low_u32(vreinterpretq_u32_m128i(a))));</span>
<span class="s1">}</span>

<span class="s2">// Converts the two signed 32-bit integers in the lower 64 bits to two signed</span>
<span class="s2">// 64-bit integers.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtepi32_epi64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(</span>
        <span class="s1">vmovl_s32(vget_low_s32(vreinterpretq_s32_m128i(a))));</span>
<span class="s1">}</span>

<span class="s2">// Converts the four single-precision, floating-point values of a to signed</span>
<span class="s2">// 32-bit integer values.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := (int) a0</span>
<span class="s2">//   r1 := (int) a1</span>
<span class="s2">//   r2 := (int) a2</span>
<span class="s2">//   r3 := (int) a3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/vstudio/xdc42k5e(v=vs.100).aspx</span>
<span class="s2">// *NOTE*. The default rounding mode on SSE is 'round to even', which ARMv7-A</span>
<span class="s2">// does not support! It is supported on ARMv8-A however.</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtps_epi32(__m128 a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcvtnq_s32_f32(a));</span>
<span class="s0">#else</span>
    <span class="s1">uint32x4_t signmask = vdupq_n_u32(</span><span class="s3">0x80000000</span><span class="s1">);</span>
    <span class="s1">float32x4_t half = vbslq_f32(signmask, vreinterpretq_f32_m128(a),</span>
                                 <span class="s1">vdupq_n_f32(</span><span class="s3">0</span><span class="s1">.</span><span class="s3">5</span><span class="s1">f)); </span><span class="s2">/* +/- 0.5 */</span>
    <span class="s1">int32x4_t r_normal = vcvtq_s32_f32(vaddq_f32(</span>
        <span class="s1">vreinterpretq_f32_m128(a), half)); </span><span class="s2">/* round to integer: [a + 0.5]*/</span>
    <span class="s1">int32x4_t r_trunc =</span>
        <span class="s1">vcvtq_s32_f32(vreinterpretq_f32_m128(a)); </span><span class="s2">/* truncate to integer: [a] */</span>
    <span class="s1">int32x4_t plusone = vreinterpretq_s32_u32(vshrq_n_u32(</span>
        <span class="s1">vreinterpretq_u32_s32(vnegq_s32(r_trunc)), </span><span class="s3">31</span><span class="s1">)); </span><span class="s2">/* 1 or 0 */</span>
    <span class="s1">int32x4_t r_even = vbicq_s32(vaddq_s32(r_trunc, plusone),</span>
                                 <span class="s1">vdupq_n_s32(</span><span class="s3">1</span><span class="s1">)); </span><span class="s2">/* ([a] + {0,1}) &amp; ~1 */</span>
    <span class="s1">float32x4_t delta = vsubq_f32(</span>
        <span class="s1">vreinterpretq_f32_m128(a),</span>
        <span class="s1">vcvtq_f32_s32(r_trunc)); </span><span class="s2">/* compute delta: delta = (a - [a]) */</span>
    <span class="s1">uint32x4_t is_delta_half = vceqq_f32(delta, half); </span><span class="s2">/* delta == +/- 0.5 */</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vbslq_s32(is_delta_half, r_even, r_normal));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Copy the lower 32-bit integer in a to dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := a[31:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtsi128_si32</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_cvtsi128_si32(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_s32(vreinterpretq_s32_m128i(a), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s1">}</span>

<span class="s2">// Copy the lower 64-bit integer in a to dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtsi128_si64</span>
<span class="s1">FORCE_INLINE int64_t _mm_cvtsi128_si64(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vgetq_lane_s64(vreinterpretq_s64_m128i(a), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s1">}</span>

<span class="s2">// Copy the lower 64-bit integer in a to dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtsi128_si64x</span>
<span class="s0">#define </span><span class="s1">_mm_cvtsi128_si64x(a) _mm_cvtsi128_si64(a)</span>

<span class="s2">// Moves 32-bit integer a to the least significant 32 bits of an __m128 object,</span>
<span class="s2">// zero extending the upper bits.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a</span>
<span class="s2">//   r1 := 0x0</span>
<span class="s2">//   r2 := 0x0</span>
<span class="s2">//   r3 := 0x0</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/ct3539ha%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtsi32_si128(</span><span class="s0">int </span><span class="s1">a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vsetq_lane_s32(a, vdupq_n_s32(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Moves 64-bit integer a to the least significant 64 bits of an __m128 object,</span>
<span class="s2">// zero extending the upper bits.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a</span>
<span class="s2">//   r1 := 0x0</span>
<span class="s1">FORCE_INLINE __m128i _mm_cvtsi64_si128(int64_t a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vsetq_lane_s64(a, vdupq_n_s64(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Cast vector of type __m128 to type __m128d. This intrinsic is only used for</span>
<span class="s2">// compilation and does not generate any instructions, thus it has zero latency.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_castps_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_castps_pd(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_s32(vreinterpretq_s32_m128(a));</span>
<span class="s1">}</span>

<span class="s2">// Applies a type cast to reinterpret four 32-bit floating point values passed</span>
<span class="s2">// in as a 128-bit parameter as packed 32-bit integers.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bb514099.aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_castps_si128(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vreinterpretq_s32_m128(a));</span>
<span class="s1">}</span>

<span class="s2">// Applies a type cast to reinterpret four 32-bit integers passed in as a</span>
<span class="s2">// 128-bit parameter as packed 32-bit floating point values.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bb514029.aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_castsi128_ps(__m128i a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_s32(vreinterpretq_s32_m128i(a));</span>
<span class="s1">}</span>

<span class="s2">// Loads 128-bit value. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/atzzad1h(v=vs.80).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_load_si128(</span><span class="s0">const </span><span class="s1">__m128i *p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vld1q_s32((</span><span class="s0">const </span><span class="s1">int32_t *) p));</span>
<span class="s1">}</span>

<span class="s2">// Load a double-precision (64-bit) floating-point element from memory into both</span>
<span class="s2">// elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//   dst[127:64] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_load1_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_load1_pd(</span><span class="s0">const double </span><span class="s1">*p)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(vld1q_dup_f64(p));</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_s64(vdupq_n_s64(*(</span><span class="s0">const </span><span class="s1">int64_t *) p));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Load a double-precision (64-bit) floating-point element from memory into the</span>
<span class="s2">// upper element of dst, and copy the lower element from a to dst. mem_addr does</span>
<span class="s2">// not need to be aligned on any particular boundary.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0]</span>
<span class="s2">//   dst[127:64] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadh_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_loadh_pd(__m128d a, </span><span class="s0">const double </span><span class="s1">*p)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(</span>
        <span class="s1">vcombine_f64(vget_low_f64(vreinterpretq_f64_m128d(a)), vld1_f64(p)));</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f32(vcombine_f32(</span>
        <span class="s1">vget_low_f32(vreinterpretq_f32_m128d(a)), vld1_f32((</span><span class="s0">const float </span><span class="s1">*) p)));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Load a double-precision (64-bit) floating-point element from memory into both</span>
<span class="s2">// elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//   dst[127:64] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_load_pd1</span>
<span class="s0">#define </span><span class="s1">_mm_load_pd1 _mm_load1_pd</span>

<span class="s2">// Load a double-precision (64-bit) floating-point element from memory into both</span>
<span class="s2">// elements of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//   dst[127:64] := MEM[mem_addr+63:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loaddup_pd</span>
<span class="s0">#define </span><span class="s1">_mm_loaddup_pd _mm_load1_pd</span>

<span class="s2">// Loads 128-bit value. :</span>
<span class="s2">// https://msdn.microsoft.com/zh-cn/library/f4k12ae8(v=vs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_loadu_si128(</span><span class="s0">const </span><span class="s1">__m128i *p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vld1q_s32((</span><span class="s0">const </span><span class="s1">int32_t *) p));</span>
<span class="s1">}</span>

<span class="s2">// Load unaligned 32-bit integer from memory into the first element of dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[31:0] := MEM[mem_addr+31:mem_addr]</span>
<span class="s2">//   dst[MAX:32] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_loadu_si32</span>
<span class="s1">FORCE_INLINE __m128i _mm_loadu_si32(</span><span class="s0">const void </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vsetq_lane_s32(*(</span><span class="s0">const </span><span class="s1">int32_t *) p, vdupq_n_s32(</span><span class="s3">0</span><span class="s1">), </span><span class="s3">0</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Convert packed double-precision (64-bit) floating-point elements in a to</span>
<span class="s2">// packed single-precision (32-bit) floating-point elements, and store the</span>
<span class="s2">// results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//     i := 32*j</span>
<span class="s2">//     k := 64*j</span>
<span class="s2">//     dst[i+31:i] := Convert_FP64_To_FP32(a[k+64:k])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//   dst[127:64] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtpd_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_cvtpd_ps(__m128d a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">float32x2_t tmp = vcvt_f32_f64(vreinterpretq_f64_m128d(a));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(tmp, vdup_n_f32(</span><span class="s3">0</span><span class="s1">)));</span>
<span class="s0">#else</span>
    <span class="s0">float </span><span class="s1">a0 = (</span><span class="s0">float</span><span class="s1">) ((</span><span class="s0">double </span><span class="s1">*) &amp;a)[</span><span class="s3">0</span><span class="s1">];</span>
    <span class="s0">float </span><span class="s1">a1 = (</span><span class="s0">float</span><span class="s1">) ((</span><span class="s0">double </span><span class="s1">*) &amp;a)[</span><span class="s3">1</span><span class="s1">];</span>
    <span class="s0">return </span><span class="s1">_mm_set_ps(</span><span class="s3">0</span><span class="s1">, </span><span class="s3">0</span><span class="s1">, a1, a0);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Copy the lower double-precision (64-bit) floating-point element of a to dst.</span>
<span class="s2">//</span>
<span class="s2">//   dst[63:0] := a[63:0]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtsd_f64</span>
<span class="s1">FORCE_INLINE </span><span class="s0">double </span><span class="s1">_mm_cvtsd_f64(__m128d a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">(</span><span class="s0">double</span><span class="s1">) vgetq_lane_f64(vreinterpretq_f64_m128d(a), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">((</span><span class="s0">double </span><span class="s1">*) &amp;a)[</span><span class="s3">0</span><span class="s1">];</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Convert packed single-precision (32-bit) floating-point elements in a to</span>
<span class="s2">// packed double-precision (64-bit) floating-point elements, and store the</span>
<span class="s2">// results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//     i := 64*j</span>
<span class="s2">//     k := 32*j</span>
<span class="s2">//     dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvtps_pd</span>
<span class="s1">FORCE_INLINE __m128d _mm_cvtps_pd(__m128 a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128d_f64(</span>
        <span class="s1">vcvt_f64_f32(vget_low_f32(vreinterpretq_f32_m128(a))));</span>
<span class="s0">#else</span>
    <span class="s0">double </span><span class="s1">a0 = (</span><span class="s0">double</span><span class="s1">) vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">double </span><span class="s1">a1 = (</span><span class="s0">double</span><span class="s1">) vgetq_lane_f32(vreinterpretq_f32_m128(a), </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">_mm_set_pd(a1, a0);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Cast vector of type __m128d to type __m128i. This intrinsic is only used for</span>
<span class="s2">// compilation and does not generate any instructions, thus it has zero latency.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_castpd_si128</span>
<span class="s1">FORCE_INLINE __m128i _mm_castpd_si128(__m128d a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vreinterpretq_s64_m128d(a));</span>
<span class="s1">}</span>

<span class="s2">// Blend packed single-precision (32-bit) floating-point elements from a and b</span>
<span class="s2">// using mask, and store the results in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_blendv_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_blendv_ps(__m128 a, __m128 b, __m128 mask)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vbslq_f32(vreinterpretq_u32_m128(mask),</span>
                                            <span class="s1">vreinterpretq_f32_m128(b),</span>
                                            <span class="s1">vreinterpretq_f32_m128(a)));</span>
<span class="s1">}</span>

<span class="s2">// Round the packed single-precision (32-bit) floating-point elements in a using</span>
<span class="s2">// the rounding parameter, and store the results as packed single-precision</span>
<span class="s2">// floating-point elements in dst.</span>
<span class="s2">// software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_round_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_round_ps(__m128 a, </span><span class="s0">int </span><span class="s1">rounding)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">switch </span><span class="s1">(rounding) {</span>
    <span class="s0">case </span><span class="s1">(_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC):</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vrndnq_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s0">case </span><span class="s1">(_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC):</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vrndmq_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s0">case </span><span class="s1">(_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC):</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vrndpq_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s0">case </span><span class="s1">(_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC):</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vrndq_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s0">default</span><span class="s1">:  </span><span class="s2">//_MM_FROUND_CUR_DIRECTION</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vrndiq_f32(vreinterpretq_f32_m128(a)));</span>
    <span class="s1">}</span>
<span class="s0">#else</span>
    <span class="s0">float </span><span class="s1">*v_float = (</span><span class="s0">float </span><span class="s1">*) &amp;a;</span>
    <span class="s1">__m128 zero, neg_inf, pos_inf;</span>

    <span class="s0">switch </span><span class="s1">(rounding) {</span>
    <span class="s0">case </span><span class="s1">(_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC):</span>
        <span class="s0">return </span><span class="s1">_mm_cvtepi32_ps(_mm_cvtps_epi32(a));</span>
    <span class="s0">case </span><span class="s1">(_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC):</span>
        <span class="s0">return </span><span class="s1">(__m128){floorf(v_float[</span><span class="s3">0</span><span class="s1">]), floorf(v_float[</span><span class="s3">1</span><span class="s1">]),</span>
                        <span class="s1">floorf(v_float[</span><span class="s3">2</span><span class="s1">]), floorf(v_float[</span><span class="s3">3</span><span class="s1">])};</span>
    <span class="s0">case </span><span class="s1">(_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC):</span>
        <span class="s0">return </span><span class="s1">(__m128){ceilf(v_float[</span><span class="s3">0</span><span class="s1">]), ceilf(v_float[</span><span class="s3">1</span><span class="s1">]), ceilf(v_float[</span><span class="s3">2</span><span class="s1">]),</span>
                        <span class="s1">ceilf(v_float[</span><span class="s3">3</span><span class="s1">])};</span>
    <span class="s0">case </span><span class="s1">(_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC):</span>
        <span class="s1">zero = _mm_set_ps(</span><span class="s3">0</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f, </span><span class="s3">0</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f, </span><span class="s3">0</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f, </span><span class="s3">0</span><span class="s1">.</span><span class="s3">0</span><span class="s1">f);</span>
        <span class="s1">neg_inf = _mm_set_ps(floorf(v_float[</span><span class="s3">0</span><span class="s1">]), floorf(v_float[</span><span class="s3">1</span><span class="s1">]),</span>
                             <span class="s1">floorf(v_float[</span><span class="s3">2</span><span class="s1">]), floorf(v_float[</span><span class="s3">3</span><span class="s1">]));</span>
        <span class="s1">pos_inf = _mm_set_ps(ceilf(v_float[</span><span class="s3">0</span><span class="s1">]), ceilf(v_float[</span><span class="s3">1</span><span class="s1">]),</span>
                             <span class="s1">ceilf(v_float[</span><span class="s3">2</span><span class="s1">]), ceilf(v_float[</span><span class="s3">3</span><span class="s1">]));</span>
        <span class="s0">return </span><span class="s1">_mm_blendv_ps(pos_inf, neg_inf, _mm_cmple_ps(a, zero));</span>
    <span class="s0">default</span><span class="s1">:  </span><span class="s2">//_MM_FROUND_CUR_DIRECTION</span>
        <span class="s0">return </span><span class="s1">(__m128){roundf(v_float[</span><span class="s3">0</span><span class="s1">]), roundf(v_float[</span><span class="s3">1</span><span class="s1">]),</span>
                        <span class="s1">roundf(v_float[</span><span class="s3">2</span><span class="s1">]), roundf(v_float[</span><span class="s3">3</span><span class="s1">])};</span>
    <span class="s1">}</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Convert packed single-precision (32-bit) floating-point elements in a to</span>
<span class="s2">// packed 32-bit integers, and store the results in dst.</span>
<span class="s2">//</span>
<span class="s2">//   FOR j := 0 to 1</span>
<span class="s2">//       i := 32*j</span>
<span class="s2">//       dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_cvt_ps2pi</span>
<span class="s1">FORCE_INLINE __m64 _mm_cvt_ps2pi(__m128 a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s32(</span>
        <span class="s1">vget_low_s32(vcvtnq_s32_f32(vreinterpretq_f32_m128(a))));</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpret_m64_s32(</span>
        <span class="s1">vcvt_s32_f32(vget_low_f32(vreinterpretq_f32_m128(</span>
            <span class="s1">_mm_round_ps(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)))));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Round the packed single-precision (32-bit) floating-point elements in a up to</span>
<span class="s2">// an integer value, and store the results as packed single-precision</span>
<span class="s2">// floating-point elements in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_ceil_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_ceil_ps(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_round_ps(a, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);</span>
<span class="s1">}</span>

<span class="s2">// Round the packed single-precision (32-bit) floating-point elements in a down</span>
<span class="s2">// to an integer value, and store the results as packed single-precision</span>
<span class="s2">// floating-point elements in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_floor_ps</span>
<span class="s1">FORCE_INLINE __m128 _mm_floor_ps(__m128 a)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_round_ps(a, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);</span>
<span class="s1">}</span>


<span class="s2">// Load 128-bits of integer data from unaligned memory into dst. This intrinsic</span>
<span class="s2">// may perform better than _mm_loadu_si128 when the data crosses a cache line</span>
<span class="s2">// boundary.</span>
<span class="s2">//</span>
<span class="s2">//   dst[127:0] := MEM[mem_addr+127:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_lddqu_si128</span>
<span class="s0">#define </span><span class="s1">_mm_lddqu_si128 _mm_loadu_si128</span>

<span class="s2">/* Miscellaneous Operations */</span>

<span class="s2">// Shifts the 8 signed 16-bit integers in a right by count bits while shifting</span>
<span class="s2">// in the sign bit.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 &gt;&gt; count</span>
<span class="s2">//   r1 := a1 &gt;&gt; count</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := a7 &gt;&gt; count</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/3c9997dk(v%3dvs.90).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_sra_epi16(__m128i a, __m128i count)</span>
<span class="s1">{</span>
    <span class="s1">int64_t c = (int64_t) vget_low_s64((int64x2_t) count);</span>
    <span class="s0">if </span><span class="s1">(c &gt; </span><span class="s3">15</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">_mm_cmplt_epi16(a, _mm_setzero_si128());</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vshlq_s16((int16x8_t) a, vdupq_n_s16(-c)));</span>
<span class="s1">}</span>

<span class="s2">// Shifts the 4 signed 32-bit integers in a right by count bits while shifting</span>
<span class="s2">// in the sign bit.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0 &gt;&gt; count</span>
<span class="s2">//   r1 := a1 &gt;&gt; count</span>
<span class="s2">//   r2 := a2 &gt;&gt; count</span>
<span class="s2">//   r3 := a3 &gt;&gt; count</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/ce40009e(v%3dvs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_sra_epi32(__m128i a, __m128i count)</span>
<span class="s1">{</span>
    <span class="s1">int64_t c = (int64_t) vget_low_s64((int64x2_t) count);</span>
    <span class="s0">if </span><span class="s1">(c &gt; </span><span class="s3">31</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">_mm_cmplt_epi32(a, _mm_setzero_si128());</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vshlq_s32((int32x4_t) a, vdupq_n_s32(-c)));</span>
<span class="s1">}</span>

<span class="s2">// Packs the 16 signed 16-bit integers from a and b into 8-bit integers and</span>
<span class="s2">// saturates.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/k4y4f7w5%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_packs_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vcombine_s8(vqmovn_s16(vreinterpretq_s16_m128i(a)),</span>
                    <span class="s1">vqmovn_s16(vreinterpretq_s16_m128i(b))));</span>
<span class="s1">}</span>

<span class="s2">// Packs the 16 signed 16 - bit integers from a and b into 8 - bit unsigned</span>
<span class="s2">// integers and saturates.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := UnsignedSaturate(a0)</span>
<span class="s2">//   r1 := UnsignedSaturate(a1)</span>
<span class="s2">//   ...</span>
<span class="s2">//   r7 := UnsignedSaturate(a7)</span>
<span class="s2">//   r8 := UnsignedSaturate(b0)</span>
<span class="s2">//   r9 := UnsignedSaturate(b1)</span>
<span class="s2">//   ...</span>
<span class="s2">//   r15 := UnsignedSaturate(b7)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/07ad1wx4(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_packus_epi16(</span><span class="s0">const </span><span class="s1">__m128i a, </span><span class="s0">const </span><span class="s1">__m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vcombine_u8(vqmovun_s16(vreinterpretq_s16_m128i(a)),</span>
                    <span class="s1">vqmovun_s16(vreinterpretq_s16_m128i(b))));</span>
<span class="s1">}</span>

<span class="s2">// Packs the 8 signed 32-bit integers from a and b into signed 16-bit integers</span>
<span class="s2">// and saturates.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := SignedSaturate(a0)</span>
<span class="s2">//   r1 := SignedSaturate(a1)</span>
<span class="s2">//   r2 := SignedSaturate(a2)</span>
<span class="s2">//   r3 := SignedSaturate(a3)</span>
<span class="s2">//   r4 := SignedSaturate(b0)</span>
<span class="s2">//   r5 := SignedSaturate(b1)</span>
<span class="s2">//   r6 := SignedSaturate(b2)</span>
<span class="s2">//   r7 := SignedSaturate(b3)</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/393t56f9%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_packs_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vcombine_s16(vqmovn_s32(vreinterpretq_s32_m128i(a)),</span>
                     <span class="s1">vqmovn_s32(vreinterpretq_s32_m128i(b))));</span>
<span class="s1">}</span>

<span class="s2">// Packs the 8 unsigned 32-bit integers from a and b into unsigned 16-bit</span>
<span class="s2">// integers and saturates.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := UnsignedSaturate(a0)</span>
<span class="s2">//   r1 := UnsignedSaturate(a1)</span>
<span class="s2">//   r2 := UnsignedSaturate(a2)</span>
<span class="s2">//   r3 := UnsignedSaturate(a3)</span>
<span class="s2">//   r4 := UnsignedSaturate(b0)</span>
<span class="s2">//   r5 := UnsignedSaturate(b1)</span>
<span class="s2">//   r6 := UnsignedSaturate(b2)</span>
<span class="s2">//   r7 := UnsignedSaturate(b3)</span>
<span class="s1">FORCE_INLINE __m128i _mm_packus_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u16(</span>
        <span class="s1">vcombine_u16(vqmovun_s32(vreinterpretq_s32_m128i(a)),</span>
                     <span class="s1">vqmovun_s32(vreinterpretq_s32_m128i(b))));</span>
<span class="s1">}</span>

<span class="s2">// Interleaves the lower 8 signed or unsigned 8-bit integers in a with the lower</span>
<span class="s2">// 8 signed or unsigned 8-bit integers in b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0</span>
<span class="s2">//   r1 := b0</span>
<span class="s2">//   r2 := a1</span>
<span class="s2">//   r3 := b1</span>
<span class="s2">//   ...</span>
<span class="s2">//   r14 := a7</span>
<span class="s2">//   r15 := b7</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/xf7k860c%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_unpacklo_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vzip1q_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s0">#else</span>
    <span class="s1">int8x8_t a1 = vreinterpret_s8_s16(vget_low_s16(vreinterpretq_s16_m128i(a)));</span>
    <span class="s1">int8x8_t b1 = vreinterpret_s8_s16(vget_low_s16(vreinterpretq_s16_m128i(b)));</span>
    <span class="s1">int8x8x2_t result = vzip_s8(a1, b1);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(vcombine_s8(result.val[</span><span class="s3">0</span><span class="s1">], result.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Interleaves the lower 4 signed or unsigned 16-bit integers in a with the</span>
<span class="s2">// lower 4 signed or unsigned 16-bit integers in b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0</span>
<span class="s2">//   r1 := b0</span>
<span class="s2">//   r2 := a1</span>
<span class="s2">//   r3 := b1</span>
<span class="s2">//   r4 := a2</span>
<span class="s2">//   r5 := b2</span>
<span class="s2">//   r6 := a3</span>
<span class="s2">//   r7 := b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/btxb17bw%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_unpacklo_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vzip1q_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s0">#else</span>
    <span class="s1">int16x4_t a1 = vget_low_s16(vreinterpretq_s16_m128i(a));</span>
    <span class="s1">int16x4_t b1 = vget_low_s16(vreinterpretq_s16_m128i(b));</span>
    <span class="s1">int16x4x2_t result = vzip_s16(a1, b1);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vcombine_s16(result.val[</span><span class="s3">0</span><span class="s1">], result.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Interleaves the lower 2 signed or unsigned 32 - bit integers in a with the</span>
<span class="s2">// lower 2 signed or unsigned 32 - bit integers in b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0</span>
<span class="s2">//   r1 := b0</span>
<span class="s2">//   r2 := a1</span>
<span class="s2">//   r3 := b1</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/x8atst9d(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_unpacklo_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vzip1q_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s0">#else</span>
    <span class="s1">int32x2_t a1 = vget_low_s32(vreinterpretq_s32_m128i(a));</span>
    <span class="s1">int32x2_t b1 = vget_low_s32(vreinterpretq_s32_m128i(b));</span>
    <span class="s1">int32x2x2_t result = vzip_s32(a1, b1);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(result.val[</span><span class="s3">0</span><span class="s1">], result.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_unpacklo_epi64(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">int64x1_t a_l = vget_low_s64(vreinterpretq_s64_m128i(a));</span>
    <span class="s1">int64x1_t b_l = vget_low_s64(vreinterpretq_s64_m128i(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vcombine_s64(a_l, b_l));</span>
<span class="s1">}</span>

<span class="s2">// Selects and interleaves the lower two single-precision, floating-point values</span>
<span class="s2">// from a and b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a0</span>
<span class="s2">//   r1 := b0</span>
<span class="s2">//   r2 := a1</span>
<span class="s2">//   r3 := b1</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/25st103b%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_unpacklo_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vzip1q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s0">#else</span>
    <span class="s1">float32x2_t a1 = vget_low_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32x2_t b1 = vget_low_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s1">float32x2x2_t result = vzip_f32(a1, b1);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(result.val[</span><span class="s3">0</span><span class="s1">], result.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Selects and interleaves the upper two single-precision, floating-point values</span>
<span class="s2">// from a and b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a2</span>
<span class="s2">//   r1 := b2</span>
<span class="s2">//   r2 := a3</span>
<span class="s2">//   r3 := b3</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/skccxx7d%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE __m128 _mm_unpackhi_ps(__m128 a, __m128 b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(</span>
        <span class="s1">vzip2q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));</span>
<span class="s0">#else</span>
    <span class="s1">float32x2_t a1 = vget_high_f32(vreinterpretq_f32_m128(a));</span>
    <span class="s1">float32x2_t b1 = vget_high_f32(vreinterpretq_f32_m128(b));</span>
    <span class="s1">float32x2x2_t result = vzip_f32(a1, b1);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128_f32(vcombine_f32(result.val[</span><span class="s3">0</span><span class="s1">], result.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Interleaves the upper 8 signed or unsigned 8-bit integers in a with the upper</span>
<span class="s2">// 8 signed or unsigned 8-bit integers in b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a8</span>
<span class="s2">//   r1 := b8</span>
<span class="s2">//   r2 := a9</span>
<span class="s2">//   r3 := b9</span>
<span class="s2">//   ...</span>
<span class="s2">//   r14 := a15</span>
<span class="s2">//   r15 := b15</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/t5h7783k(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_unpackhi_epi8(__m128i a, __m128i b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(</span>
        <span class="s1">vzip2q_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));</span>
<span class="s0">#else</span>
    <span class="s1">int8x8_t a1 =</span>
        <span class="s1">vreinterpret_s8_s16(vget_high_s16(vreinterpretq_s16_m128i(a)));</span>
    <span class="s1">int8x8_t b1 =</span>
        <span class="s1">vreinterpret_s8_s16(vget_high_s16(vreinterpretq_s16_m128i(b)));</span>
    <span class="s1">int8x8x2_t result = vzip_s8(a1, b1);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s8(vcombine_s8(result.val[</span><span class="s3">0</span><span class="s1">], result.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Interleaves the upper 4 signed or unsigned 16-bit integers in a with the</span>
<span class="s2">// upper 4 signed or unsigned 16-bit integers in b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a4</span>
<span class="s2">//   r1 := b4</span>
<span class="s2">//   r2 := a5</span>
<span class="s2">//   r3 := b5</span>
<span class="s2">//   r4 := a6</span>
<span class="s2">//   r5 := b6</span>
<span class="s2">//   r6 := a7</span>
<span class="s2">//   r7 := b7</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/03196cz7(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_unpackhi_epi16(__m128i a, __m128i b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(</span>
        <span class="s1">vzip2q_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));</span>
<span class="s0">#else</span>
    <span class="s1">int16x4_t a1 = vget_high_s16(vreinterpretq_s16_m128i(a));</span>
    <span class="s1">int16x4_t b1 = vget_high_s16(vreinterpretq_s16_m128i(b));</span>
    <span class="s1">int16x4x2_t result = vzip_s16(a1, b1);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s16(vcombine_s16(result.val[</span><span class="s3">0</span><span class="s1">], result.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Interleaves the upper 2 signed or unsigned 32-bit integers in a with the</span>
<span class="s2">// upper 2 signed or unsigned 32-bit integers in b.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/65sa7cbs(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_unpackhi_epi32(__m128i a, __m128i b)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(</span>
        <span class="s1">vzip2q_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));</span>
<span class="s0">#else</span>
    <span class="s1">int32x2_t a1 = vget_high_s32(vreinterpretq_s32_m128i(a));</span>
    <span class="s1">int32x2_t b1 = vget_high_s32(vreinterpretq_s32_m128i(b));</span>
    <span class="s1">int32x2x2_t result = vzip_s32(a1, b1);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s32(vcombine_s32(result.val[</span><span class="s3">0</span><span class="s1">], result.val[</span><span class="s3">1</span><span class="s1">]));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Interleaves the upper signed or unsigned 64-bit integer in a with the</span>
<span class="s2">// upper signed or unsigned 64-bit integer in b.</span>
<span class="s2">//</span>
<span class="s2">//   r0 := a1</span>
<span class="s2">//   r1 := b1</span>
<span class="s1">FORCE_INLINE __m128i _mm_unpackhi_epi64(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">int64x1_t a_h = vget_high_s64(vreinterpretq_s64_m128i(a));</span>
    <span class="s1">int64x1_t b_h = vget_high_s64(vreinterpretq_s64_m128i(b));</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vcombine_s64(a_h, b_h));</span>
<span class="s1">}</span>

<span class="s2">// Horizontally compute the minimum amongst the packed unsigned 16-bit integers</span>
<span class="s2">// in a, store the minimum and index in dst, and zero the remaining bits in dst.</span>
<span class="s2">//</span>
<span class="s2">//   index[2:0] := 0</span>
<span class="s2">//   min[15:0] := a[15:0]</span>
<span class="s2">//   FOR j := 0 to 7</span>
<span class="s2">//       i := j*16</span>
<span class="s2">//       IF a[i+15:i] &lt; min[15:0]</span>
<span class="s2">//           index[2:0] := j</span>
<span class="s2">//           min[15:0] := a[i+15:i]</span>
<span class="s2">//       FI</span>
<span class="s2">//   ENDFOR</span>
<span class="s2">//   dst[15:0] := min[15:0]</span>
<span class="s2">//   dst[18:16] := index[2:0]</span>
<span class="s2">//   dst[127:19] := 0</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_minpos_epu16</span>
<span class="s1">FORCE_INLINE __m128i _mm_minpos_epu16(__m128i a)</span>
<span class="s1">{</span>
    <span class="s1">__m128i dst;</span>
    <span class="s1">uint16_t min, idx = </span><span class="s3">0</span><span class="s1">;</span>
    <span class="s2">// Find the minimum value</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">min = vminvq_u16(vreinterpretq_u16_m128i(a));</span>
<span class="s0">#else</span>
    <span class="s1">__m64 tmp;</span>
    <span class="s1">tmp = vreinterpret_m64_u16(</span>
        <span class="s1">vmin_u16(vget_low_u16(vreinterpretq_u16_m128i(a)),</span>
                 <span class="s1">vget_high_u16(vreinterpretq_u16_m128i(a))));</span>
    <span class="s1">tmp = vreinterpret_m64_u16(</span>
        <span class="s1">vpmin_u16(vreinterpret_u16_m64(tmp), vreinterpret_u16_m64(tmp)));</span>
    <span class="s1">tmp = vreinterpret_m64_u16(</span>
        <span class="s1">vpmin_u16(vreinterpret_u16_m64(tmp), vreinterpret_u16_m64(tmp)));</span>
    <span class="s1">min = vget_lane_u16(vreinterpret_u16_m64(tmp), </span><span class="s3">0</span><span class="s1">);</span>
<span class="s0">#endif</span>
    <span class="s2">// Get the index of the minimum value</span>
    <span class="s0">int </span><span class="s1">i;</span>
    <span class="s0">for </span><span class="s1">(i = </span><span class="s3">0</span><span class="s1">; i &lt; </span><span class="s3">8</span><span class="s1">; i++) {</span>
        <span class="s0">if </span><span class="s1">(min == vgetq_lane_u16(vreinterpretq_u16_m128i(a), </span><span class="s3">0</span><span class="s1">)) {</span>
            <span class="s1">idx = (uint16_t) i;</span>
            <span class="s0">break</span><span class="s1">;</span>
        <span class="s1">}</span>
        <span class="s1">a = _mm_srli_si128(a, </span><span class="s3">2</span><span class="s1">);</span>
    <span class="s1">}</span>
    <span class="s2">// Generate result</span>
    <span class="s1">dst = _mm_setzero_si128();</span>
    <span class="s1">dst = vreinterpretq_m128i_u16(</span>
        <span class="s1">vsetq_lane_u16(min, vreinterpretq_u16_m128i(dst), </span><span class="s3">0</span><span class="s1">));</span>
    <span class="s1">dst = vreinterpretq_m128i_u16(</span>
        <span class="s1">vsetq_lane_u16(idx, vreinterpretq_u16_m128i(dst), </span><span class="s3">1</span><span class="s1">));</span>
    <span class="s0">return </span><span class="s1">dst;</span>
<span class="s1">}</span>

<span class="s2">// shift to right</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bb514041(v=vs.120).aspx</span>
<span class="s2">// http://blog.csdn.net/hemmingway/article/details/44828303</span>
<span class="s2">// Clang requires a macro here, as it is extremely picky about c being a</span>
<span class="s2">// literal.</span>
<span class="s0">#define </span><span class="s1">_mm_alignr_epi8(a, b, c) \</span>
    <span class="s1">((__m128i) vextq_s8((int8x16_t)(b), (int8x16_t)(a), (c)))</span>

<span class="s2">// Compute the bitwise AND of 128 bits (representing integer data) in a and b,</span>
<span class="s2">// and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the</span>
<span class="s2">// bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero,</span>
<span class="s2">// otherwise set CF to 0. Return the CF value.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testc_si128</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_testc_si128(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">int64x2_t s64 =</span>
        <span class="s1">vandq_s64(vreinterpretq_s64_s32(vmvnq_s32(vreinterpretq_s32_m128i(a))),</span>
                  <span class="s1">vreinterpretq_s64_m128i(b));</span>
    <span class="s0">return </span><span class="s1">!(vgetq_lane_s64(s64, </span><span class="s3">0</span><span class="s1">) | vgetq_lane_s64(s64, </span><span class="s3">1</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Compute the bitwise AND of 128 bits (representing integer data) in a and b,</span>
<span class="s2">// and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the</span>
<span class="s2">// bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero,</span>
<span class="s2">// otherwise set CF to 0. Return the ZF value.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_testz_si128</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_testz_si128(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s1">int64x2_t s64 =</span>
        <span class="s1">vandq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b));</span>
    <span class="s0">return </span><span class="s1">!(vgetq_lane_s64(s64, </span><span class="s3">0</span><span class="s1">) | vgetq_lane_s64(s64, </span><span class="s3">1</span><span class="s1">));</span>
<span class="s1">}</span>

<span class="s2">// Extracts the selected signed or unsigned 8-bit integer from a and zero</span>
<span class="s2">// extends.</span>
<span class="s2">// FORCE_INLINE int _mm_extract_epi8(__m128i a, __constrange(0,16) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_extract_epi8(a, imm) vgetq_lane_u8(vreinterpretq_u8_m128i(a), (imm))</span>

<span class="s2">// Inserts the least significant 8 bits of b into the selected 8-bit integer</span>
<span class="s2">// of a.</span>
<span class="s2">// FORCE_INLINE __m128i _mm_insert_epi8(__m128i a, int b,</span>
<span class="s2">//                                      __constrange(0,16) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_insert_epi8(a, b, imm)                                 \</span>
    <span class="s1">__extension__({                                                \</span>
        <span class="s1">vreinterpretq_m128i_s8(                                    \</span>
            <span class="s1">vsetq_lane_s8((b), vreinterpretq_s8_m128i(a), (imm))); \</span>
    <span class="s1">})</span>

<span class="s2">// Extracts the selected signed or unsigned 16-bit integer from a and zero</span>
<span class="s2">// extends.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/6dceta0c(v=vs.100).aspx</span>
<span class="s2">// FORCE_INLINE int _mm_extract_epi16(__m128i a, __constrange(0,8) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_extract_epi16(a, imm) \</span>
    <span class="s1">vgetq_lane_u16(vreinterpretq_u16_m128i(a), (imm))</span>

<span class="s2">// Inserts the least significant 16 bits of b into the selected 16-bit integer</span>
<span class="s2">// of a.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/kaze8hz1%28v=vs.100%29.aspx</span>
<span class="s2">// FORCE_INLINE __m128i _mm_insert_epi16(__m128i a, int b,</span>
<span class="s2">//                                       __constrange(0,8) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_insert_epi16(a, b, imm)                                  \</span>
    <span class="s1">__extension__({                                                  \</span>
        <span class="s1">vreinterpretq_m128i_s16(                                     \</span>
            <span class="s1">vsetq_lane_s16((b), vreinterpretq_s16_m128i(a), (imm))); \</span>
    <span class="s1">})</span>

<span class="s2">// Extracts the selected signed or unsigned 32-bit integer from a and zero</span>
<span class="s2">// extends.</span>
<span class="s2">// FORCE_INLINE int _mm_extract_epi32(__m128i a, __constrange(0,4) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_extract_epi32(a, imm) \</span>
    <span class="s1">vgetq_lane_s32(vreinterpretq_s32_m128i(a), (imm))</span>

<span class="s2">// Extracts the selected single-precision (32-bit) floating-point from a.</span>
<span class="s2">// FORCE_INLINE int _mm_extract_ps(__m128 a, __constrange(0,4) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_extract_ps(a, imm) vgetq_lane_s32(vreinterpretq_s32_m128(a), (imm))</span>

<span class="s2">// Inserts the least significant 32 bits of b into the selected 32-bit integer</span>
<span class="s2">// of a.</span>
<span class="s2">// FORCE_INLINE __m128i _mm_insert_epi32(__m128i a, int b,</span>
<span class="s2">//                                       __constrange(0,4) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_insert_epi32(a, b, imm)                                  \</span>
    <span class="s1">__extension__({                                                  \</span>
        <span class="s1">vreinterpretq_m128i_s32(                                     \</span>
            <span class="s1">vsetq_lane_s32((b), vreinterpretq_s32_m128i(a), (imm))); \</span>
    <span class="s1">})</span>

<span class="s2">// Extracts the selected signed or unsigned 64-bit integer from a and zero</span>
<span class="s2">// extends.</span>
<span class="s2">// FORCE_INLINE __int64 _mm_extract_epi64(__m128i a, __constrange(0,2) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_extract_epi64(a, imm) \</span>
    <span class="s1">vgetq_lane_s64(vreinterpretq_s64_m128i(a), (imm))</span>

<span class="s2">// Inserts the least significant 64 bits of b into the selected 64-bit integer</span>
<span class="s2">// of a.</span>
<span class="s2">// FORCE_INLINE __m128i _mm_insert_epi64(__m128i a, __int64 b,</span>
<span class="s2">//                                       __constrange(0,2) int imm)</span>
<span class="s0">#define </span><span class="s1">_mm_insert_epi64(a, b, imm)                                  \</span>
    <span class="s1">__extension__({                                                  \</span>
        <span class="s1">vreinterpretq_m128i_s64(                                     \</span>
            <span class="s1">vsetq_lane_s64((b), vreinterpretq_s64_m128i(a), (imm))); \</span>
    <span class="s1">})</span>

<span class="s2">// Count the number of bits set to 1 in unsigned 32-bit integer a, and</span>
<span class="s2">// return that count in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_popcnt_u32</span>
<span class="s1">FORCE_INLINE </span><span class="s0">int </span><span class="s1">_mm_popcnt_u32(</span><span class="s0">unsigned int </span><span class="s1">a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_popcount)</span>
    <span class="s0">return </span><span class="s1">__builtin_popcount(a);</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">(</span><span class="s0">int</span><span class="s1">) vaddlv_u8(vcnt_u8(vcreate_u8((uint64_t) a)));</span>
<span class="s0">#endif</span>
<span class="s0">#else</span>
    <span class="s1">uint32_t count = </span><span class="s3">0</span><span class="s1">;</span>
    <span class="s1">uint8x8_t input_val, count8x8_val;</span>
    <span class="s1">uint16x4_t count16x4_val;</span>
    <span class="s1">uint32x2_t count32x2_val;</span>

    <span class="s1">input_val = vld1_u8((uint8_t *) &amp;a);</span>
    <span class="s1">count8x8_val = vcnt_u8(input_val);</span>
    <span class="s1">count16x4_val = vpaddl_u8(count8x8_val);</span>
    <span class="s1">count32x2_val = vpaddl_u16(count16x4_val);</span>

    <span class="s1">vst1_u32(&amp;count, count32x2_val);</span>
    <span class="s0">return </span><span class="s1">count;</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Count the number of bits set to 1 in unsigned 64-bit integer a, and</span>
<span class="s2">// return that count in dst.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_popcnt_u64</span>
<span class="s1">FORCE_INLINE int64_t _mm_popcnt_u64(uint64_t a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_popcountll)</span>
    <span class="s0">return </span><span class="s1">__builtin_popcountll(a);</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">(int64_t) vaddlv_u8(vcnt_u8(vcreate_u8(a)));</span>
<span class="s0">#endif</span>
<span class="s0">#else</span>
    <span class="s1">uint64_t count = </span><span class="s3">0</span><span class="s1">;</span>
    <span class="s1">uint8x8_t input_val, count8x8_val;</span>
    <span class="s1">uint16x4_t count16x4_val;</span>
    <span class="s1">uint32x2_t count32x2_val;</span>
    <span class="s1">uint64x1_t count64x1_val;</span>

    <span class="s1">input_val = vld1_u8((uint8_t *) &amp;a);</span>
    <span class="s1">count8x8_val = vcnt_u8(input_val);</span>
    <span class="s1">count16x4_val = vpaddl_u8(count8x8_val);</span>
    <span class="s1">count32x2_val = vpaddl_u16(count16x4_val);</span>
    <span class="s1">count64x1_val = vpaddl_u32(count32x2_val);</span>
    <span class="s1">vst1_u64(&amp;count, count64x1_val);</span>
    <span class="s0">return </span><span class="s1">count;</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Macro: Transpose the 4x4 matrix formed by the 4 rows of single-precision</span>
<span class="s2">// (32-bit) floating-point elements in row0, row1, row2, and row3, and store the</span>
<span class="s2">// transposed matrix in these vectors (row0 now contains column 0, etc.).</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=MM_TRANSPOSE4_PS</span>
<span class="s0">#define </span><span class="s1">_MM_TRANSPOSE4_PS(row0, row1, row2, row3)         \</span>
    <span class="s0">do </span><span class="s1">{                                                  \</span>
        <span class="s1">float32x4x2_t ROW01 = vtrnq_f32(row0, row1);      \</span>
        <span class="s1">float32x4x2_t ROW23 = vtrnq_f32(row2, row3);      \</span>
        <span class="s1">row0 = vcombine_f32(vget_low_f32(ROW01.val[</span><span class="s3">0</span><span class="s1">]),   \</span>
                            <span class="s1">vget_low_f32(ROW23.val[</span><span class="s3">0</span><span class="s1">]));  \</span>
        <span class="s1">row1 = vcombine_f32(vget_low_f32(ROW01.val[</span><span class="s3">1</span><span class="s1">]),   \</span>
                            <span class="s1">vget_low_f32(ROW23.val[</span><span class="s3">1</span><span class="s1">]));  \</span>
        <span class="s1">row2 = vcombine_f32(vget_high_f32(ROW01.val[</span><span class="s3">0</span><span class="s1">]),  \</span>
                            <span class="s1">vget_high_f32(ROW23.val[</span><span class="s3">0</span><span class="s1">])); \</span>
        <span class="s1">row3 = vcombine_f32(vget_high_f32(ROW01.val[</span><span class="s3">1</span><span class="s1">]),  \</span>
                            <span class="s1">vget_high_f32(ROW23.val[</span><span class="s3">1</span><span class="s1">])); \</span>
    <span class="s1">} </span><span class="s0">while </span><span class="s1">(</span><span class="s3">0</span><span class="s1">)</span>

<span class="s2">/* Crypto Extensions */</span>

<span class="s0">#if </span><span class="s1">defined(__ARM_FEATURE_CRYPTO)</span>
<span class="s2">// Wraps vmull_p64</span>
<span class="s1">FORCE_INLINE uint64x2_t _sse2neon_vmull_p64(uint64x1_t _a, uint64x1_t _b)</span>
<span class="s1">{</span>
    <span class="s1">poly64_t a = vget_lane_p64(vreinterpret_p64_u64(_a), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">poly64_t b = vget_lane_p64(vreinterpret_p64_u64(_b), </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_u64_p128(vmull_p64(a, b));</span>
<span class="s1">}</span>
<span class="s0">#else  </span><span class="s2">// ARMv7 polyfill</span>
<span class="s2">// ARMv7/some A64 lacks vmull_p64, but it has vmull_p8.</span>
<span class="s2">//</span>
<span class="s2">// vmull_p8 calculates 8 8-bit-&gt;16-bit polynomial multiplies, but we need a</span>
<span class="s2">// 64-bit-&gt;128-bit polynomial multiply.</span>
<span class="s2">//</span>
<span class="s2">// It needs some work and is somewhat slow, but it is still faster than all</span>
<span class="s2">// known scalar methods.</span>
<span class="s2">//</span>
<span class="s2">// Algorithm adapted to C from</span>
<span class="s2">// https://www.workofard.com/2017/07/ghash-for-low-end-cores/, which is adapted</span>
<span class="s2">// from &quot;Fast Software Polynomial Multiplication on ARM Processors Using the</span>
<span class="s2">// NEON Engine&quot; by Danilo Camara, Conrado Gouvea, Julio Lopez and Ricardo Dahab</span>
<span class="s2">// (https://hal.inria.fr/hal-01506572)</span>
<span class="s0">static </span><span class="s1">uint64x2_t _sse2neon_vmull_p64(uint64x1_t _a, uint64x1_t _b)</span>
<span class="s1">{</span>
    <span class="s1">poly8x8_t a = vreinterpret_p8_u64(_a);</span>
    <span class="s1">poly8x8_t b = vreinterpret_p8_u64(_b);</span>

    <span class="s2">// Masks</span>
    <span class="s1">uint8x16_t k48_32 = vcombine_u8(vcreate_u8(</span><span class="s3">0x0000ffffffffffff</span><span class="s1">),</span>
                                    <span class="s1">vcreate_u8(</span><span class="s3">0x00000000ffffffff</span><span class="s1">));</span>
    <span class="s1">uint8x16_t k16_00 = vcombine_u8(vcreate_u8(</span><span class="s3">0x000000000000ffff</span><span class="s1">),</span>
                                    <span class="s1">vcreate_u8(</span><span class="s3">0x0000000000000000</span><span class="s1">));</span>

    <span class="s2">// Do the multiplies, rotating with vext to get all combinations</span>
    <span class="s1">uint8x16_t d = vreinterpretq_u8_p16(vmull_p8(a, b));  </span><span class="s2">// D = A0 * B0</span>
    <span class="s1">uint8x16_t e =</span>
        <span class="s1">vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, </span><span class="s3">1</span><span class="s1">)));  </span><span class="s2">// E = A0 * B1</span>
    <span class="s1">uint8x16_t f =</span>
        <span class="s1">vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, </span><span class="s3">1</span><span class="s1">), b));  </span><span class="s2">// F = A1 * B0</span>
    <span class="s1">uint8x16_t g =</span>
        <span class="s1">vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, </span><span class="s3">2</span><span class="s1">)));  </span><span class="s2">// G = A0 * B2</span>
    <span class="s1">uint8x16_t h =</span>
        <span class="s1">vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, </span><span class="s3">2</span><span class="s1">), b));  </span><span class="s2">// H = A2 * B0</span>
    <span class="s1">uint8x16_t i =</span>
        <span class="s1">vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, </span><span class="s3">3</span><span class="s1">)));  </span><span class="s2">// I = A0 * B3</span>
    <span class="s1">uint8x16_t j =</span>
        <span class="s1">vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, </span><span class="s3">3</span><span class="s1">), b));  </span><span class="s2">// J = A3 * B0</span>
    <span class="s1">uint8x16_t k =</span>
        <span class="s1">vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, </span><span class="s3">4</span><span class="s1">)));  </span><span class="s2">// L = A0 * B4</span>

    <span class="s2">// Add cross products</span>
    <span class="s1">uint8x16_t l = veorq_u8(e, f);  </span><span class="s2">// L = E + F</span>
    <span class="s1">uint8x16_t m = veorq_u8(g, h);  </span><span class="s2">// M = G + H</span>
    <span class="s1">uint8x16_t n = veorq_u8(i, j);  </span><span class="s2">// N = I + J</span>

    <span class="s2">// Interleave. Using vzip1 and vzip2 prevents Clang from emitting TBL</span>
    <span class="s2">// instructions.</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">uint8x16_t lm_p0 = vreinterpretq_u8_u64(</span>
        <span class="s1">vzip1q_u64(vreinterpretq_u64_u8(l), vreinterpretq_u64_u8(m)));</span>
    <span class="s1">uint8x16_t lm_p1 = vreinterpretq_u8_u64(</span>
        <span class="s1">vzip2q_u64(vreinterpretq_u64_u8(l), vreinterpretq_u64_u8(m)));</span>
    <span class="s1">uint8x16_t nk_p0 = vreinterpretq_u8_u64(</span>
        <span class="s1">vzip1q_u64(vreinterpretq_u64_u8(n), vreinterpretq_u64_u8(k)));</span>
    <span class="s1">uint8x16_t nk_p1 = vreinterpretq_u8_u64(</span>
        <span class="s1">vzip2q_u64(vreinterpretq_u64_u8(n), vreinterpretq_u64_u8(k)));</span>
<span class="s0">#else</span>
    <span class="s1">uint8x16_t lm_p0 = vcombine_u8(vget_low_u8(l), vget_low_u8(m));</span>
    <span class="s1">uint8x16_t lm_p1 = vcombine_u8(vget_high_u8(l), vget_high_u8(m));</span>
    <span class="s1">uint8x16_t nk_p0 = vcombine_u8(vget_low_u8(n), vget_low_u8(k));</span>
    <span class="s1">uint8x16_t nk_p1 = vcombine_u8(vget_high_u8(n), vget_high_u8(k));</span>
<span class="s0">#endif</span>
    <span class="s2">// t0 = (L) (P0 + P1) &lt;&lt; 8</span>
    <span class="s2">// t1 = (M) (P2 + P3) &lt;&lt; 16</span>
    <span class="s1">uint8x16_t t0t1_tmp = veorq_u8(lm_p0, lm_p1);</span>
    <span class="s1">uint8x16_t t0t1_h = vandq_u8(lm_p1, k48_32);</span>
    <span class="s1">uint8x16_t t0t1_l = veorq_u8(t0t1_tmp, t0t1_h);</span>

    <span class="s2">// t2 = (N) (P4 + P5) &lt;&lt; 24</span>
    <span class="s2">// t3 = (K) (P6 + P7) &lt;&lt; 32</span>
    <span class="s1">uint8x16_t t2t3_tmp = veorq_u8(nk_p0, nk_p1);</span>
    <span class="s1">uint8x16_t t2t3_h = vandq_u8(nk_p1, k16_00);</span>
    <span class="s1">uint8x16_t t2t3_l = veorq_u8(t2t3_tmp, t2t3_h);</span>

    <span class="s2">// De-interleave</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s1">uint8x16_t t0 = vreinterpretq_u8_u64(</span>
        <span class="s1">vuzp1q_u64(vreinterpretq_u64_u8(t0t1_l), vreinterpretq_u64_u8(t0t1_h)));</span>
    <span class="s1">uint8x16_t t1 = vreinterpretq_u8_u64(</span>
        <span class="s1">vuzp2q_u64(vreinterpretq_u64_u8(t0t1_l), vreinterpretq_u64_u8(t0t1_h)));</span>
    <span class="s1">uint8x16_t t2 = vreinterpretq_u8_u64(</span>
        <span class="s1">vuzp1q_u64(vreinterpretq_u64_u8(t2t3_l), vreinterpretq_u64_u8(t2t3_h)));</span>
    <span class="s1">uint8x16_t t3 = vreinterpretq_u8_u64(</span>
        <span class="s1">vuzp2q_u64(vreinterpretq_u64_u8(t2t3_l), vreinterpretq_u64_u8(t2t3_h)));</span>
<span class="s0">#else</span>
    <span class="s1">uint8x16_t t1 = vcombine_u8(vget_high_u8(t0t1_l), vget_high_u8(t0t1_h));</span>
    <span class="s1">uint8x16_t t0 = vcombine_u8(vget_low_u8(t0t1_l), vget_low_u8(t0t1_h));</span>
    <span class="s1">uint8x16_t t3 = vcombine_u8(vget_high_u8(t2t3_l), vget_high_u8(t2t3_h));</span>
    <span class="s1">uint8x16_t t2 = vcombine_u8(vget_low_u8(t2t3_l), vget_low_u8(t2t3_h));</span>
<span class="s0">#endif</span>
    <span class="s2">// Shift the cross products</span>
    <span class="s1">uint8x16_t t0_shift = vextq_u8(t0, t0, </span><span class="s3">15</span><span class="s1">);  </span><span class="s2">// t0 &lt;&lt; 8</span>
    <span class="s1">uint8x16_t t1_shift = vextq_u8(t1, t1, </span><span class="s3">14</span><span class="s1">);  </span><span class="s2">// t1 &lt;&lt; 16</span>
    <span class="s1">uint8x16_t t2_shift = vextq_u8(t2, t2, </span><span class="s3">13</span><span class="s1">);  </span><span class="s2">// t2 &lt;&lt; 24</span>
    <span class="s1">uint8x16_t t3_shift = vextq_u8(t3, t3, </span><span class="s3">12</span><span class="s1">);  </span><span class="s2">// t3 &lt;&lt; 32</span>

    <span class="s2">// Accumulate the products</span>
    <span class="s1">uint8x16_t cross1 = veorq_u8(t0_shift, t1_shift);</span>
    <span class="s1">uint8x16_t cross2 = veorq_u8(t2_shift, t3_shift);</span>
    <span class="s1">uint8x16_t mix = veorq_u8(d, cross1);</span>
    <span class="s1">uint8x16_t r = veorq_u8(mix, cross2);</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_u64_u8(r);</span>
<span class="s1">}</span>
<span class="s0">#endif  </span><span class="s2">// ARMv7 polyfill</span>

<span class="s1">FORCE_INLINE __m128i _mm_clmulepi64_si128(__m128i _a, __m128i _b, </span><span class="s0">const int </span><span class="s1">imm)</span>
<span class="s1">{</span>
    <span class="s1">uint64x2_t a = vreinterpretq_u64_m128i(_a);</span>
    <span class="s1">uint64x2_t b = vreinterpretq_u64_m128i(_b);</span>
    <span class="s0">switch </span><span class="s1">(imm &amp; </span><span class="s3">0x11</span><span class="s1">) {</span>
    <span class="s0">case </span><span class="s3">0x00</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(</span>
            <span class="s1">_sse2neon_vmull_p64(vget_low_u64(a), vget_low_u64(b)));</span>
    <span class="s0">case </span><span class="s3">0x01</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(</span>
            <span class="s1">_sse2neon_vmull_p64(vget_high_u64(a), vget_low_u64(b)));</span>
    <span class="s0">case </span><span class="s3">0x10</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(</span>
            <span class="s1">_sse2neon_vmull_p64(vget_low_u64(a), vget_high_u64(b)));</span>
    <span class="s0">case </span><span class="s3">0x11</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u64(</span>
            <span class="s1">_sse2neon_vmull_p64(vget_high_u64(a), vget_high_u64(b)));</span>
    <span class="s0">default</span><span class="s1">:</span>
        <span class="s1">abort();</span>
    <span class="s1">}</span>
<span class="s1">}</span>

<span class="s0">#if </span><span class="s1">!defined(__ARM_FEATURE_CRYPTO)</span>
<span class="s2">/* clang-format off */</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_DATA(w)                                           \</span>
    <span class="s1">{                                                                  \</span>
        <span class="s1">w(</span><span class="s3">0x63</span><span class="s1">), w(</span><span class="s3">0x7c</span><span class="s1">), w(</span><span class="s3">0x77</span><span class="s1">), w(</span><span class="s3">0x7b</span><span class="s1">), w(</span><span class="s3">0xf2</span><span class="s1">), w(</span><span class="s3">0x6b</span><span class="s1">), w(</span><span class="s3">0x6f</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xc5</span><span class="s1">), w(</span><span class="s3">0x30</span><span class="s1">), w(</span><span class="s3">0x01</span><span class="s1">), w(</span><span class="s3">0x67</span><span class="s1">), w(</span><span class="s3">0x2b</span><span class="s1">), w(</span><span class="s3">0xfe</span><span class="s1">), w(</span><span class="s3">0xd7</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xab</span><span class="s1">), w(</span><span class="s3">0x76</span><span class="s1">), w(</span><span class="s3">0xca</span><span class="s1">), w(</span><span class="s3">0x82</span><span class="s1">), w(</span><span class="s3">0xc9</span><span class="s1">), w(</span><span class="s3">0x7d</span><span class="s1">), w(</span><span class="s3">0xfa</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x59</span><span class="s1">), w(</span><span class="s3">0x47</span><span class="s1">), w(</span><span class="s3">0xf0</span><span class="s1">), w(</span><span class="s3">0xad</span><span class="s1">), w(</span><span class="s3">0xd4</span><span class="s1">), w(</span><span class="s3">0xa2</span><span class="s1">), w(</span><span class="s3">0xaf</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x9c</span><span class="s1">), w(</span><span class="s3">0xa4</span><span class="s1">), w(</span><span class="s3">0x72</span><span class="s1">), w(</span><span class="s3">0xc0</span><span class="s1">), w(</span><span class="s3">0xb7</span><span class="s1">), w(</span><span class="s3">0xfd</span><span class="s1">), w(</span><span class="s3">0x93</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x26</span><span class="s1">), w(</span><span class="s3">0x36</span><span class="s1">), w(</span><span class="s3">0x3f</span><span class="s1">), w(</span><span class="s3">0xf7</span><span class="s1">), w(</span><span class="s3">0xcc</span><span class="s1">), w(</span><span class="s3">0x34</span><span class="s1">), w(</span><span class="s3">0xa5</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xe5</span><span class="s1">), w(</span><span class="s3">0xf1</span><span class="s1">), w(</span><span class="s3">0x71</span><span class="s1">), w(</span><span class="s3">0xd8</span><span class="s1">), w(</span><span class="s3">0x31</span><span class="s1">), w(</span><span class="s3">0x15</span><span class="s1">), w(</span><span class="s3">0x04</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xc7</span><span class="s1">), w(</span><span class="s3">0x23</span><span class="s1">), w(</span><span class="s3">0xc3</span><span class="s1">), w(</span><span class="s3">0x18</span><span class="s1">), w(</span><span class="s3">0x96</span><span class="s1">), w(</span><span class="s3">0x05</span><span class="s1">), w(</span><span class="s3">0x9a</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x07</span><span class="s1">), w(</span><span class="s3">0x12</span><span class="s1">), w(</span><span class="s3">0x80</span><span class="s1">), w(</span><span class="s3">0xe2</span><span class="s1">), w(</span><span class="s3">0xeb</span><span class="s1">), w(</span><span class="s3">0x27</span><span class="s1">), w(</span><span class="s3">0xb2</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x75</span><span class="s1">), w(</span><span class="s3">0x09</span><span class="s1">), w(</span><span class="s3">0x83</span><span class="s1">), w(</span><span class="s3">0x2c</span><span class="s1">), w(</span><span class="s3">0x1a</span><span class="s1">), w(</span><span class="s3">0x1b</span><span class="s1">), w(</span><span class="s3">0x6e</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x5a</span><span class="s1">), w(</span><span class="s3">0xa0</span><span class="s1">), w(</span><span class="s3">0x52</span><span class="s1">), w(</span><span class="s3">0x3b</span><span class="s1">), w(</span><span class="s3">0xd6</span><span class="s1">), w(</span><span class="s3">0xb3</span><span class="s1">), w(</span><span class="s3">0x29</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xe3</span><span class="s1">), w(</span><span class="s3">0x2f</span><span class="s1">), w(</span><span class="s3">0x84</span><span class="s1">), w(</span><span class="s3">0x53</span><span class="s1">), w(</span><span class="s3">0xd1</span><span class="s1">), w(</span><span class="s3">0x00</span><span class="s1">), w(</span><span class="s3">0xed</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x20</span><span class="s1">), w(</span><span class="s3">0xfc</span><span class="s1">), w(</span><span class="s3">0xb1</span><span class="s1">), w(</span><span class="s3">0x5b</span><span class="s1">), w(</span><span class="s3">0x6a</span><span class="s1">), w(</span><span class="s3">0xcb</span><span class="s1">), w(</span><span class="s3">0xbe</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x39</span><span class="s1">), w(</span><span class="s3">0x4a</span><span class="s1">), w(</span><span class="s3">0x4c</span><span class="s1">), w(</span><span class="s3">0x58</span><span class="s1">), w(</span><span class="s3">0xcf</span><span class="s1">), w(</span><span class="s3">0xd0</span><span class="s1">), w(</span><span class="s3">0xef</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xaa</span><span class="s1">), w(</span><span class="s3">0xfb</span><span class="s1">), w(</span><span class="s3">0x43</span><span class="s1">), w(</span><span class="s3">0x4d</span><span class="s1">), w(</span><span class="s3">0x33</span><span class="s1">), w(</span><span class="s3">0x85</span><span class="s1">), w(</span><span class="s3">0x45</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xf9</span><span class="s1">), w(</span><span class="s3">0x02</span><span class="s1">), w(</span><span class="s3">0x7f</span><span class="s1">), w(</span><span class="s3">0x50</span><span class="s1">), w(</span><span class="s3">0x3c</span><span class="s1">), w(</span><span class="s3">0x9f</span><span class="s1">), w(</span><span class="s3">0xa8</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x51</span><span class="s1">), w(</span><span class="s3">0xa3</span><span class="s1">), w(</span><span class="s3">0x40</span><span class="s1">), w(</span><span class="s3">0x8f</span><span class="s1">), w(</span><span class="s3">0x92</span><span class="s1">), w(</span><span class="s3">0x9d</span><span class="s1">), w(</span><span class="s3">0x38</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xf5</span><span class="s1">), w(</span><span class="s3">0xbc</span><span class="s1">), w(</span><span class="s3">0xb6</span><span class="s1">), w(</span><span class="s3">0xda</span><span class="s1">), w(</span><span class="s3">0x21</span><span class="s1">), w(</span><span class="s3">0x10</span><span class="s1">), w(</span><span class="s3">0xff</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xf3</span><span class="s1">), w(</span><span class="s3">0xd2</span><span class="s1">), w(</span><span class="s3">0xcd</span><span class="s1">), w(</span><span class="s3">0x0c</span><span class="s1">), w(</span><span class="s3">0x13</span><span class="s1">), w(</span><span class="s3">0xec</span><span class="s1">), w(</span><span class="s3">0x5f</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x97</span><span class="s1">), w(</span><span class="s3">0x44</span><span class="s1">), w(</span><span class="s3">0x17</span><span class="s1">), w(</span><span class="s3">0xc4</span><span class="s1">), w(</span><span class="s3">0xa7</span><span class="s1">), w(</span><span class="s3">0x7e</span><span class="s1">), w(</span><span class="s3">0x3d</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x64</span><span class="s1">), w(</span><span class="s3">0x5d</span><span class="s1">), w(</span><span class="s3">0x19</span><span class="s1">), w(</span><span class="s3">0x73</span><span class="s1">), w(</span><span class="s3">0x60</span><span class="s1">), w(</span><span class="s3">0x81</span><span class="s1">), w(</span><span class="s3">0x4f</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xdc</span><span class="s1">), w(</span><span class="s3">0x22</span><span class="s1">), w(</span><span class="s3">0x2a</span><span class="s1">), w(</span><span class="s3">0x90</span><span class="s1">), w(</span><span class="s3">0x88</span><span class="s1">), w(</span><span class="s3">0x46</span><span class="s1">), w(</span><span class="s3">0xee</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xb8</span><span class="s1">), w(</span><span class="s3">0x14</span><span class="s1">), w(</span><span class="s3">0xde</span><span class="s1">), w(</span><span class="s3">0x5e</span><span class="s1">), w(</span><span class="s3">0x0b</span><span class="s1">), w(</span><span class="s3">0xdb</span><span class="s1">), w(</span><span class="s3">0xe0</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x32</span><span class="s1">), w(</span><span class="s3">0x3a</span><span class="s1">), w(</span><span class="s3">0x0a</span><span class="s1">), w(</span><span class="s3">0x49</span><span class="s1">), w(</span><span class="s3">0x06</span><span class="s1">), w(</span><span class="s3">0x24</span><span class="s1">), w(</span><span class="s3">0x5c</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xc2</span><span class="s1">), w(</span><span class="s3">0xd3</span><span class="s1">), w(</span><span class="s3">0xac</span><span class="s1">), w(</span><span class="s3">0x62</span><span class="s1">), w(</span><span class="s3">0x91</span><span class="s1">), w(</span><span class="s3">0x95</span><span class="s1">), w(</span><span class="s3">0xe4</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x79</span><span class="s1">), w(</span><span class="s3">0xe7</span><span class="s1">), w(</span><span class="s3">0xc8</span><span class="s1">), w(</span><span class="s3">0x37</span><span class="s1">), w(</span><span class="s3">0x6d</span><span class="s1">), w(</span><span class="s3">0x8d</span><span class="s1">), w(</span><span class="s3">0xd5</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x4e</span><span class="s1">), w(</span><span class="s3">0xa9</span><span class="s1">), w(</span><span class="s3">0x6c</span><span class="s1">), w(</span><span class="s3">0x56</span><span class="s1">), w(</span><span class="s3">0xf4</span><span class="s1">), w(</span><span class="s3">0xea</span><span class="s1">), w(</span><span class="s3">0x65</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x7a</span><span class="s1">), w(</span><span class="s3">0xae</span><span class="s1">), w(</span><span class="s3">0x08</span><span class="s1">), w(</span><span class="s3">0xba</span><span class="s1">), w(</span><span class="s3">0x78</span><span class="s1">), w(</span><span class="s3">0x25</span><span class="s1">), w(</span><span class="s3">0x2e</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x1c</span><span class="s1">), w(</span><span class="s3">0xa6</span><span class="s1">), w(</span><span class="s3">0xb4</span><span class="s1">), w(</span><span class="s3">0xc6</span><span class="s1">), w(</span><span class="s3">0xe8</span><span class="s1">), w(</span><span class="s3">0xdd</span><span class="s1">), w(</span><span class="s3">0x74</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x1f</span><span class="s1">), w(</span><span class="s3">0x4b</span><span class="s1">), w(</span><span class="s3">0xbd</span><span class="s1">), w(</span><span class="s3">0x8b</span><span class="s1">), w(</span><span class="s3">0x8a</span><span class="s1">), w(</span><span class="s3">0x70</span><span class="s1">), w(</span><span class="s3">0x3e</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xb5</span><span class="s1">), w(</span><span class="s3">0x66</span><span class="s1">), w(</span><span class="s3">0x48</span><span class="s1">), w(</span><span class="s3">0x03</span><span class="s1">), w(</span><span class="s3">0xf6</span><span class="s1">), w(</span><span class="s3">0x0e</span><span class="s1">), w(</span><span class="s3">0x61</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x35</span><span class="s1">), w(</span><span class="s3">0x57</span><span class="s1">), w(</span><span class="s3">0xb9</span><span class="s1">), w(</span><span class="s3">0x86</span><span class="s1">), w(</span><span class="s3">0xc1</span><span class="s1">), w(</span><span class="s3">0x1d</span><span class="s1">), w(</span><span class="s3">0x9e</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xe1</span><span class="s1">), w(</span><span class="s3">0xf8</span><span class="s1">), w(</span><span class="s3">0x98</span><span class="s1">), w(</span><span class="s3">0x11</span><span class="s1">), w(</span><span class="s3">0x69</span><span class="s1">), w(</span><span class="s3">0xd9</span><span class="s1">), w(</span><span class="s3">0x8e</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x94</span><span class="s1">), w(</span><span class="s3">0x9b</span><span class="s1">), w(</span><span class="s3">0x1e</span><span class="s1">), w(</span><span class="s3">0x87</span><span class="s1">), w(</span><span class="s3">0xe9</span><span class="s1">), w(</span><span class="s3">0xce</span><span class="s1">), w(</span><span class="s3">0x55</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0x28</span><span class="s1">), w(</span><span class="s3">0xdf</span><span class="s1">), w(</span><span class="s3">0x8c</span><span class="s1">), w(</span><span class="s3">0xa1</span><span class="s1">), w(</span><span class="s3">0x89</span><span class="s1">), w(</span><span class="s3">0x0d</span><span class="s1">), w(</span><span class="s3">0xbf</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xe6</span><span class="s1">), w(</span><span class="s3">0x42</span><span class="s1">), w(</span><span class="s3">0x68</span><span class="s1">), w(</span><span class="s3">0x41</span><span class="s1">), w(</span><span class="s3">0x99</span><span class="s1">), w(</span><span class="s3">0x2d</span><span class="s1">), w(</span><span class="s3">0x0f</span><span class="s1">), \</span>
        <span class="s1">w(</span><span class="s3">0xb0</span><span class="s1">), w(</span><span class="s3">0x54</span><span class="s1">), w(</span><span class="s3">0xbb</span><span class="s1">), w(</span><span class="s3">0x16</span><span class="s1">)                             \</span>
    <span class="s1">}</span>
<span class="s2">/* clang-format on */</span>

<span class="s2">/* X Macro trick. See https://en.wikipedia.org/wiki/X_Macro */</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_H0(x) (x)</span>
<span class="s0">static const </span><span class="s1">uint8_t SSE2NEON_sbox[</span><span class="s3">256</span><span class="s1">] = SSE2NEON_AES_DATA(SSE2NEON_AES_H0);</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_H0</span>

<span class="s2">// In the absence of crypto extensions, implement aesenc using regular neon</span>
<span class="s2">// intrinsics instead. See:</span>
<span class="s2">// https://www.workofard.com/2017/01/accelerated-aes-for-the-arm64-linux-kernel/</span>
<span class="s2">// https://www.workofard.com/2017/07/ghash-for-low-end-cores/ and</span>
<span class="s2">// https://github.com/ColinIanKing/linux-next-mirror/blob/b5f466091e130caaf0735976648f72bd5e09aa84/crypto/aegis128-neon-inner.c#L52</span>
<span class="s2">// for more information Reproduced with permission of the author.</span>
<span class="s1">FORCE_INLINE __m128i _mm_aesenc_si128(__m128i EncBlock, __m128i RoundKey)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__)</span>
    <span class="s0">static const </span><span class="s1">uint8_t shift_rows[] = {</span><span class="s3">0x0</span><span class="s1">, </span><span class="s3">0x5</span><span class="s1">, </span><span class="s3">0xa</span><span class="s1">, </span><span class="s3">0xf</span><span class="s1">, </span><span class="s3">0x4</span><span class="s1">, </span><span class="s3">0x9</span><span class="s1">,</span>
                                         <span class="s3">0xe</span><span class="s1">, </span><span class="s3">0x3</span><span class="s1">, </span><span class="s3">0x8</span><span class="s1">, </span><span class="s3">0xd</span><span class="s1">, </span><span class="s3">0x2</span><span class="s1">, </span><span class="s3">0x7</span><span class="s1">,</span>
                                         <span class="s3">0xc</span><span class="s1">, </span><span class="s3">0x1</span><span class="s1">, </span><span class="s3">0x6</span><span class="s1">, </span><span class="s3">0xb</span><span class="s1">};</span>
    <span class="s0">static const </span><span class="s1">uint8_t ror32by8[] = {</span><span class="s3">0x1</span><span class="s1">, </span><span class="s3">0x2</span><span class="s1">, </span><span class="s3">0x3</span><span class="s1">, </span><span class="s3">0x0</span><span class="s1">, </span><span class="s3">0x5</span><span class="s1">, </span><span class="s3">0x6</span><span class="s1">, </span><span class="s3">0x7</span><span class="s1">, </span><span class="s3">0x4</span><span class="s1">,</span>
                                       <span class="s3">0x9</span><span class="s1">, </span><span class="s3">0xa</span><span class="s1">, </span><span class="s3">0xb</span><span class="s1">, </span><span class="s3">0x8</span><span class="s1">, </span><span class="s3">0xd</span><span class="s1">, </span><span class="s3">0xe</span><span class="s1">, </span><span class="s3">0xf</span><span class="s1">, </span><span class="s3">0xc</span><span class="s1">};</span>

    <span class="s1">uint8x16_t v;</span>
    <span class="s1">uint8x16_t w = vreinterpretq_u8_m128i(EncBlock);</span>

    <span class="s2">// shift rows</span>
    <span class="s1">w = vqtbl1q_u8(w, vld1q_u8(shift_rows));</span>

    <span class="s2">// sub bytes</span>
    <span class="s1">v = vqtbl4q_u8(vld1q_u8_x4(SSE2NEON_sbox), w);</span>
    <span class="s1">v = vqtbx4q_u8(v, vld1q_u8_x4(SSE2NEON_sbox + </span><span class="s3">0x40</span><span class="s1">), w - </span><span class="s3">0x40</span><span class="s1">);</span>
    <span class="s1">v = vqtbx4q_u8(v, vld1q_u8_x4(SSE2NEON_sbox + </span><span class="s3">0x80</span><span class="s1">), w - </span><span class="s3">0x80</span><span class="s1">);</span>
    <span class="s1">v = vqtbx4q_u8(v, vld1q_u8_x4(SSE2NEON_sbox + </span><span class="s3">0xc0</span><span class="s1">), w - </span><span class="s3">0xc0</span><span class="s1">);</span>

    <span class="s2">// mix columns</span>
    <span class="s1">w = (v &lt;&lt; </span><span class="s3">1</span><span class="s1">) ^ (uint8x16_t)(((int8x16_t) v &gt;&gt; </span><span class="s3">7</span><span class="s1">) &amp; </span><span class="s3">0x1b</span><span class="s1">);</span>
    <span class="s1">w ^= (uint8x16_t) vrev32q_u16((uint16x8_t) v);</span>
    <span class="s1">w ^= vqtbl1q_u8(v ^ w, vld1q_u8(ror32by8));</span>

    <span class="s2">//  add round key</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(w) ^ RoundKey;</span>

<span class="s0">#else </span><span class="s2">/* ARMv7-A NEON implementation */</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_B2W(b0, b1, b2, b3)                                       \</span>
    <span class="s1">(((uint32_t)(b3) &lt;&lt; </span><span class="s3">24</span><span class="s1">) | ((uint32_t)(b2) &lt;&lt; </span><span class="s3">16</span><span class="s1">) | ((uint32_t)(b1) &lt;&lt; </span><span class="s3">8</span><span class="s1">) | \</span>
     <span class="s1">(b0))</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_F2(x) ((x &lt;&lt; </span><span class="s3">1</span><span class="s1">) ^ (((x &gt;&gt; </span><span class="s3">7</span><span class="s1">) &amp; </span><span class="s3">1</span><span class="s1">) * </span><span class="s3">0x011b </span><span class="s2">/* WPOLY */</span><span class="s1">))</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_F3(x) (SSE2NEON_AES_F2(x) ^ x)</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_U0(p) \</span>
    <span class="s1">SSE2NEON_AES_B2W(SSE2NEON_AES_F2(p), p, p, SSE2NEON_AES_F3(p))</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_U1(p) \</span>
    <span class="s1">SSE2NEON_AES_B2W(SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p), p, p)</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_U2(p) \</span>
    <span class="s1">SSE2NEON_AES_B2W(p, SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p), p)</span>
<span class="s0">#define </span><span class="s1">SSE2NEON_AES_U3(p) \</span>
    <span class="s1">SSE2NEON_AES_B2W(p, p, SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p))</span>
    <span class="s0">static const </span><span class="s1">uint32_t ALIGN_STRUCT(</span><span class="s3">16</span><span class="s1">) aes_table[</span><span class="s3">4</span><span class="s1">][</span><span class="s3">256</span><span class="s1">] = {</span>
        <span class="s1">SSE2NEON_AES_DATA(SSE2NEON_AES_U0),</span>
        <span class="s1">SSE2NEON_AES_DATA(SSE2NEON_AES_U1),</span>
        <span class="s1">SSE2NEON_AES_DATA(SSE2NEON_AES_U2),</span>
        <span class="s1">SSE2NEON_AES_DATA(SSE2NEON_AES_U3),</span>
    <span class="s1">};</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_B2W</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_F2</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_F3</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_U0</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_U1</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_U2</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_U3</span>

    <span class="s1">uint32_t x0 = _mm_cvtsi128_si32(EncBlock);</span>
    <span class="s1">uint32_t x1 = _mm_cvtsi128_si32(_mm_shuffle_epi32(EncBlock, </span><span class="s3">0x55</span><span class="s1">));</span>
    <span class="s1">uint32_t x2 = _mm_cvtsi128_si32(_mm_shuffle_epi32(EncBlock, </span><span class="s3">0xAA</span><span class="s1">));</span>
    <span class="s1">uint32_t x3 = _mm_cvtsi128_si32(_mm_shuffle_epi32(EncBlock, </span><span class="s3">0xFF</span><span class="s1">));</span>

    <span class="s1">__m128i out = _mm_set_epi32(</span>
        <span class="s1">(aes_table[</span><span class="s3">0</span><span class="s1">][x3 &amp; </span><span class="s3">0xff</span><span class="s1">] ^ aes_table[</span><span class="s3">1</span><span class="s1">][(x0 &gt;&gt; </span><span class="s3">8</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">] ^</span>
         <span class="s1">aes_table[</span><span class="s3">2</span><span class="s1">][(x1 &gt;&gt; </span><span class="s3">16</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">] ^ aes_table[</span><span class="s3">3</span><span class="s1">][x2 &gt;&gt; </span><span class="s3">24</span><span class="s1">]),</span>
        <span class="s1">(aes_table[</span><span class="s3">0</span><span class="s1">][x2 &amp; </span><span class="s3">0xff</span><span class="s1">] ^ aes_table[</span><span class="s3">1</span><span class="s1">][(x3 &gt;&gt; </span><span class="s3">8</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">] ^</span>
         <span class="s1">aes_table[</span><span class="s3">2</span><span class="s1">][(x0 &gt;&gt; </span><span class="s3">16</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">] ^ aes_table[</span><span class="s3">3</span><span class="s1">][x1 &gt;&gt; </span><span class="s3">24</span><span class="s1">]),</span>
        <span class="s1">(aes_table[</span><span class="s3">0</span><span class="s1">][x1 &amp; </span><span class="s3">0xff</span><span class="s1">] ^ aes_table[</span><span class="s3">1</span><span class="s1">][(x2 &gt;&gt; </span><span class="s3">8</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">] ^</span>
         <span class="s1">aes_table[</span><span class="s3">2</span><span class="s1">][(x3 &gt;&gt; </span><span class="s3">16</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">] ^ aes_table[</span><span class="s3">3</span><span class="s1">][x0 &gt;&gt; </span><span class="s3">24</span><span class="s1">]),</span>
        <span class="s1">(aes_table[</span><span class="s3">0</span><span class="s1">][x0 &amp; </span><span class="s3">0xff</span><span class="s1">] ^ aes_table[</span><span class="s3">1</span><span class="s1">][(x1 &gt;&gt; </span><span class="s3">8</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">] ^</span>
         <span class="s1">aes_table[</span><span class="s3">2</span><span class="s1">][(x2 &gt;&gt; </span><span class="s3">16</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">] ^ aes_table[</span><span class="s3">3</span><span class="s1">][x3 &gt;&gt; </span><span class="s3">24</span><span class="s1">]));</span>

    <span class="s0">return </span><span class="s1">_mm_xor_si128(out, RoundKey);</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_aesenclast_si128(__m128i a, __m128i RoundKey)</span>
<span class="s1">{</span>
    <span class="s2">/* FIXME: optimized for NEON */</span>
    <span class="s1">uint8_t v[</span><span class="s3">4</span><span class="s1">][</span><span class="s3">4</span><span class="s1">] = {</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s1">] = {SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">0</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">5</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">10</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">15</span><span class="s1">)]},</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s1">] = {SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">4</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">9</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">14</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">3</span><span class="s1">)]},</span>
        <span class="s1">[</span><span class="s3">2</span><span class="s1">] = {SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">8</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">13</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">2</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">7</span><span class="s1">)]},</span>
        <span class="s1">[</span><span class="s3">3</span><span class="s1">] = {SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">12</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">1</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">6</span><span class="s1">)],</span>
               <span class="s1">SSE2NEON_sbox[vreinterpretq_nth_u8_m128i(a, </span><span class="s3">11</span><span class="s1">)]},</span>
    <span class="s1">};</span>
    <span class="s0">for </span><span class="s1">(</span><span class="s0">int </span><span class="s1">i = </span><span class="s3">0</span><span class="s1">; i &lt; </span><span class="s3">16</span><span class="s1">; i++)</span>
        <span class="s1">vreinterpretq_nth_u8_m128i(a, i) =</span>
            <span class="s1">v[i / </span><span class="s3">4</span><span class="s1">][i % </span><span class="s3">4</span><span class="s1">] ^ vreinterpretq_nth_u8_m128i(RoundKey, i);</span>
    <span class="s0">return </span><span class="s1">a;</span>
<span class="s1">}</span>

<span class="s2">// Emits the Advanced Encryption Standard (AES) instruction aeskeygenassist.</span>
<span class="s2">// This instruction generates a round key for AES encryption. See</span>
<span class="s2">// https://kazakov.life/2017/11/01/cryptocurrency-mining-on-ios-devices/</span>
<span class="s2">// for details.</span>
<span class="s2">//</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/cc714138(v=vs.120).aspx</span>
<span class="s1">FORCE_INLINE __m128i _mm_aeskeygenassist_si128(__m128i key, </span><span class="s0">const int </span><span class="s1">rcon)</span>
<span class="s1">{</span>
    <span class="s1">uint32_t X1 = _mm_cvtsi128_si32(_mm_shuffle_epi32(key, </span><span class="s3">0x55</span><span class="s1">));</span>
    <span class="s1">uint32_t X3 = _mm_cvtsi128_si32(_mm_shuffle_epi32(key, </span><span class="s3">0xFF</span><span class="s1">));</span>
    <span class="s0">for </span><span class="s1">(</span><span class="s0">int </span><span class="s1">i = </span><span class="s3">0</span><span class="s1">; i &lt; </span><span class="s3">4</span><span class="s1">; ++i) {</span>
        <span class="s1">((uint8_t *) &amp;X1)[i] = SSE2NEON_sbox[((uint8_t *) &amp;X1)[i]];</span>
        <span class="s1">((uint8_t *) &amp;X3)[i] = SSE2NEON_sbox[((uint8_t *) &amp;X3)[i]];</span>
    <span class="s1">}</span>
    <span class="s0">return </span><span class="s1">_mm_set_epi32(((X3 &gt;&gt; </span><span class="s3">8</span><span class="s1">) | (X3 &lt;&lt; </span><span class="s3">24</span><span class="s1">)) ^ rcon, X3,</span>
                         <span class="s1">((X1 &gt;&gt; </span><span class="s3">8</span><span class="s1">) | (X1 &lt;&lt; </span><span class="s3">24</span><span class="s1">)) ^ rcon, X1);</span>
<span class="s1">}</span>
<span class="s0">#undef </span><span class="s1">SSE2NEON_AES_DATA</span>

<span class="s0">#else </span><span class="s2">/* __ARM_FEATURE_CRYPTO */</span>
<span class="s2">// Implements equivalent of 'aesenc' by combining AESE (with an empty key) and</span>
<span class="s2">// AESMC and then manually applying the real key as an xor operation. This</span>
<span class="s2">// unfortunately means an additional xor op; the compiler should be able to</span>
<span class="s2">// optimize this away for repeated calls however. See</span>
<span class="s2">// https://blog.michaelbrase.com/2018/05/08/emulating-x86-aes-intrinsics-on-armv8-a</span>
<span class="s2">// for more details.</span>
<span class="s1">FORCE_INLINE __m128i _mm_aesenc_si128(__m128i a, __m128i b)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(</span>
        <span class="s1">vaesmcq_u8(vaeseq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(</span><span class="s3">0</span><span class="s1">))) ^</span>
        <span class="s1">vreinterpretq_u8_m128i(b));</span>
<span class="s1">}</span>

<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_aesenclast_si128</span>
<span class="s1">FORCE_INLINE __m128i _mm_aesenclast_si128(__m128i a, __m128i RoundKey)</span>
<span class="s1">{</span>
    <span class="s0">return </span><span class="s1">_mm_xor_si128(vreinterpretq_m128i_u8(vaeseq_u8(</span>
                             <span class="s1">vreinterpretq_u8_m128i(a), vdupq_n_u8(</span><span class="s3">0</span><span class="s1">))),</span>
                         <span class="s1">RoundKey);</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE __m128i _mm_aeskeygenassist_si128(__m128i a, </span><span class="s0">const int </span><span class="s1">rcon)</span>
<span class="s1">{</span>
    <span class="s2">// AESE does ShiftRows and SubBytes on A</span>
    <span class="s1">uint8x16_t u8 = vaeseq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(</span><span class="s3">0</span><span class="s1">));</span>

    <span class="s1">uint8x16_t dest = {</span>
        <span class="s2">// Undo ShiftRows step from AESE and extract X1 and X3</span>
        <span class="s1">u8[</span><span class="s3">0x4</span><span class="s1">], u8[</span><span class="s3">0x1</span><span class="s1">], u8[</span><span class="s3">0xE</span><span class="s1">], u8[</span><span class="s3">0xB</span><span class="s1">],  </span><span class="s2">// SubBytes(X1)</span>
        <span class="s1">u8[</span><span class="s3">0x1</span><span class="s1">], u8[</span><span class="s3">0xE</span><span class="s1">], u8[</span><span class="s3">0xB</span><span class="s1">], u8[</span><span class="s3">0x4</span><span class="s1">],  </span><span class="s2">// ROT(SubBytes(X1))</span>
        <span class="s1">u8[</span><span class="s3">0xC</span><span class="s1">], u8[</span><span class="s3">0x9</span><span class="s1">], u8[</span><span class="s3">0x6</span><span class="s1">], u8[</span><span class="s3">0x3</span><span class="s1">],  </span><span class="s2">// SubBytes(X3)</span>
        <span class="s1">u8[</span><span class="s3">0x9</span><span class="s1">], u8[</span><span class="s3">0x6</span><span class="s1">], u8[</span><span class="s3">0x3</span><span class="s1">], u8[</span><span class="s3">0xC</span><span class="s1">],  </span><span class="s2">// ROT(SubBytes(X3))</span>
    <span class="s1">};</span>
    <span class="s1">uint32x4_t r = {</span><span class="s3">0</span><span class="s1">, (</span><span class="s0">unsigned</span><span class="s1">) rcon, </span><span class="s3">0</span><span class="s1">, (</span><span class="s0">unsigned</span><span class="s1">) rcon};</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_u8(dest) ^ vreinterpretq_m128i_u32(r);</span>
<span class="s1">}</span>
<span class="s0">#endif</span>

<span class="s2">/* Streaming Extensions */</span>

<span class="s2">// Guarantees that every preceding store is globally visible before any</span>
<span class="s2">// subsequent store.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/5h2w73d1%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_sfence(</span><span class="s0">void</span><span class="s1">)</span>
<span class="s1">{</span>
    <span class="s1">__sync_synchronize();</span>
<span class="s1">}</span>

<span class="s2">// Store 128-bits (composed of 4 packed single-precision (32-bit) floating-</span>
<span class="s2">// point elements) from a into memory using a non-temporal memory hint.</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_stream_ps</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_stream_ps(</span><span class="s0">float </span><span class="s1">*p, __m128 a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_nontemporal_store)</span>
    <span class="s1">__builtin_nontemporal_store(a, (float32x4_t *) p);</span>
<span class="s0">#else</span>
    <span class="s1">vst1q_f32(p, vreinterpretq_f32_m128(a));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Stores the data in a to the address p without polluting the caches.  If the</span>
<span class="s2">// cache line containing address p is already in the cache, the cache will be</span>
<span class="s2">// updated.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/ba08y07y%28v=vs.90%29.aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_stream_si128(__m128i *p, __m128i a)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_nontemporal_store)</span>
    <span class="s1">__builtin_nontemporal_store(a, p);</span>
<span class="s0">#else</span>
    <span class="s1">vst1q_s64((int64_t *) p, vreinterpretq_s64_m128i(a));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Load 128-bits of integer data from memory into dst using a non-temporal</span>
<span class="s2">// memory hint. mem_addr must be aligned on a 16-byte boundary or a</span>
<span class="s2">// general-protection exception may be generated.</span>
<span class="s2">//</span>
<span class="s2">//   dst[127:0] := MEM[mem_addr+127:mem_addr]</span>
<span class="s2">//</span>
<span class="s2">// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_stream_load_si128</span>
<span class="s1">FORCE_INLINE __m128i _mm_stream_load_si128(__m128i *p)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">__has_builtin(__builtin_nontemporal_store)</span>
    <span class="s0">return </span><span class="s1">__builtin_nontemporal_load(p);</span>
<span class="s0">#else</span>
    <span class="s0">return </span><span class="s1">vreinterpretq_m128i_s64(vld1q_s64((int64_t *) p));</span>
<span class="s0">#endif</span>
<span class="s1">}</span>

<span class="s2">// Cache line containing p is flushed and invalidated from all caches in the</span>
<span class="s2">// coherency domain. :</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/ba08y07y(v=vs.100).aspx</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_clflush(</span><span class="s0">void const </span><span class="s1">*p)</span>
<span class="s1">{</span>
    <span class="s1">(</span><span class="s0">void</span><span class="s1">) p;</span>
    <span class="s2">// no corollary for Neon?</span>
<span class="s1">}</span>

<span class="s2">// Allocate aligned blocks of memory.</span>
<span class="s2">// https://software.intel.com/en-us/</span>
<span class="s2">//         cpp-compiler-developer-guide-and-reference-allocating-and-freeing-aligned-memory-blocks</span>
<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">*_mm_malloc(size_t size, size_t align)</span>
<span class="s1">{</span>
    <span class="s0">void </span><span class="s1">*ptr;</span>
    <span class="s0">if </span><span class="s1">(align == </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">malloc(size);</span>
    <span class="s0">if </span><span class="s1">(align == </span><span class="s3">2 </span><span class="s1">|| (</span><span class="s0">sizeof</span><span class="s1">(</span><span class="s0">void </span><span class="s1">*) == </span><span class="s3">8 </span><span class="s1">&amp;&amp; align == </span><span class="s3">4</span><span class="s1">))</span>
        <span class="s1">align = </span><span class="s0">sizeof</span><span class="s1">(</span><span class="s0">void </span><span class="s1">*);</span>
    <span class="s0">if </span><span class="s1">(!posix_memalign(&amp;ptr, align, size))</span>
        <span class="s0">return </span><span class="s1">ptr;</span>
    <span class="s0">return </span><span class="s1">NULL;</span>
<span class="s1">}</span>

<span class="s1">FORCE_INLINE </span><span class="s0">void </span><span class="s1">_mm_free(</span><span class="s0">void </span><span class="s1">*addr)</span>
<span class="s1">{</span>
    <span class="s1">free(addr);</span>
<span class="s1">}</span>

<span class="s2">// Starting with the initial value in crc, accumulates a CRC32 value for</span>
<span class="s2">// unsigned 8-bit integer v.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bb514036(v=vs.100)</span>
<span class="s1">FORCE_INLINE uint32_t _mm_crc32_u8(uint32_t crc, uint8_t v)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__) &amp;&amp; defined(__ARM_FEATURE_CRC32)</span>
    <span class="s1">__asm__ __volatile__(</span><span class="s4">&quot;crc32cb %w[c], %w[c], %w[v]</span><span class="s7">\n\t</span><span class="s4">&quot;</span>
                         <span class="s1">: [c] </span><span class="s4">&quot;+r&quot;</span><span class="s1">(crc)</span>
                         <span class="s1">: [v] </span><span class="s4">&quot;r&quot;</span><span class="s1">(v));</span>
<span class="s0">#else</span>
    <span class="s1">crc ^= v;</span>
    <span class="s0">for </span><span class="s1">(</span><span class="s0">int </span><span class="s1">bit = </span><span class="s3">0</span><span class="s1">; bit &lt; </span><span class="s3">8</span><span class="s1">; bit++) {</span>
        <span class="s0">if </span><span class="s1">(crc &amp; </span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">crc = (crc &gt;&gt; </span><span class="s3">1</span><span class="s1">) ^ UINT32_C(</span><span class="s3">0x82f63b78</span><span class="s1">);</span>
        <span class="s0">else</span>
            <span class="s1">crc = (crc &gt;&gt; </span><span class="s3">1</span><span class="s1">);</span>
    <span class="s1">}</span>
<span class="s0">#endif</span>
    <span class="s0">return </span><span class="s1">crc;</span>
<span class="s1">}</span>

<span class="s2">// Starting with the initial value in crc, accumulates a CRC32 value for</span>
<span class="s2">// unsigned 16-bit integer v.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bb531411(v=vs.100)</span>
<span class="s1">FORCE_INLINE uint32_t _mm_crc32_u16(uint32_t crc, uint16_t v)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__) &amp;&amp; defined(__ARM_FEATURE_CRC32)</span>
    <span class="s1">__asm__ __volatile__(</span><span class="s4">&quot;crc32ch %w[c], %w[c], %w[v]</span><span class="s7">\n\t</span><span class="s4">&quot;</span>
                         <span class="s1">: [c] </span><span class="s4">&quot;+r&quot;</span><span class="s1">(crc)</span>
                         <span class="s1">: [v] </span><span class="s4">&quot;r&quot;</span><span class="s1">(v));</span>
<span class="s0">#else</span>
    <span class="s1">crc = _mm_crc32_u8(crc, v &amp; </span><span class="s3">0xff</span><span class="s1">);</span>
    <span class="s1">crc = _mm_crc32_u8(crc, (v &gt;&gt; </span><span class="s3">8</span><span class="s1">) &amp; </span><span class="s3">0xff</span><span class="s1">);</span>
<span class="s0">#endif</span>
    <span class="s0">return </span><span class="s1">crc;</span>
<span class="s1">}</span>

<span class="s2">// Starting with the initial value in crc, accumulates a CRC32 value for</span>
<span class="s2">// unsigned 32-bit integer v.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bb531394(v=vs.100)</span>
<span class="s1">FORCE_INLINE uint32_t _mm_crc32_u32(uint32_t crc, uint32_t v)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__) &amp;&amp; defined(__ARM_FEATURE_CRC32)</span>
    <span class="s1">__asm__ __volatile__(</span><span class="s4">&quot;crc32cw %w[c], %w[c], %w[v]</span><span class="s7">\n\t</span><span class="s4">&quot;</span>
                         <span class="s1">: [c] </span><span class="s4">&quot;+r&quot;</span><span class="s1">(crc)</span>
                         <span class="s1">: [v] </span><span class="s4">&quot;r&quot;</span><span class="s1">(v));</span>
<span class="s0">#else</span>
    <span class="s1">crc = _mm_crc32_u16(crc, v &amp; </span><span class="s3">0xffff</span><span class="s1">);</span>
    <span class="s1">crc = _mm_crc32_u16(crc, (v &gt;&gt; </span><span class="s3">16</span><span class="s1">) &amp; </span><span class="s3">0xffff</span><span class="s1">);</span>
<span class="s0">#endif</span>
    <span class="s0">return </span><span class="s1">crc;</span>
<span class="s1">}</span>

<span class="s2">// Starting with the initial value in crc, accumulates a CRC32 value for</span>
<span class="s2">// unsigned 64-bit integer v.</span>
<span class="s2">// https://msdn.microsoft.com/en-us/library/bb514033(v=vs.100)</span>
<span class="s1">FORCE_INLINE uint64_t _mm_crc32_u64(uint64_t crc, uint64_t v)</span>
<span class="s1">{</span>
<span class="s0">#if </span><span class="s1">defined(__aarch64__) &amp;&amp; defined(__ARM_FEATURE_CRC32)</span>
    <span class="s1">__asm__ __volatile__(</span><span class="s4">&quot;crc32cx %w[c], %w[c], %x[v]</span><span class="s7">\n\t</span><span class="s4">&quot;</span>
                         <span class="s1">: [c] </span><span class="s4">&quot;+r&quot;</span><span class="s1">(crc)</span>
                         <span class="s1">: [v] </span><span class="s4">&quot;r&quot;</span><span class="s1">(v));</span>
<span class="s0">#else</span>
    <span class="s1">crc = _mm_crc32_u32((uint32_t)(crc), v &amp; </span><span class="s3">0xffffffff</span><span class="s1">);</span>
    <span class="s1">crc = _mm_crc32_u32((uint32_t)(crc), (v &gt;&gt; </span><span class="s3">32</span><span class="s1">) &amp; </span><span class="s3">0xffffffff</span><span class="s1">);</span>
<span class="s0">#endif</span>
    <span class="s0">return </span><span class="s1">crc;</span>
<span class="s1">}</span>

<span class="s0">#if </span><span class="s1">defined(__GNUC__) || defined(__clang__)</span>
<span class="s1">#pragma pop_macro(</span><span class="s4">&quot;ALIGN_STRUCT&quot;</span><span class="s1">)</span>
<span class="s1">#pragma pop_macro(</span><span class="s4">&quot;FORCE_INLINE&quot;</span><span class="s1">)</span>
<span class="s0">#endif</span>

<span class="s0">#if </span><span class="s1">defined(__GNUC__)</span>
<span class="s1">#pragma GCC pop_options</span>
<span class="s0">#endif</span>

<span class="s0">#endif</span>
</pre>
</body>
</html>